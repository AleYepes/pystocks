{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ib_async import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.optimize import minimize\n",
    "import pandas_datareader.data as web\n",
    "import re\n",
    "import ast\n",
    "\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep functions\n",
    "def evaluate_literal(val):\n",
    "    try:\n",
    "        return ast.literal_eval(val)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return val\n",
    "    \n",
    "def load(path):\n",
    "    df = pd.read_csv(path)\n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].apply(evaluate_literal)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kind = 'midpoint'\n",
    "kind = 'trades'\n",
    "# kind = 'indices'\n",
    "\n",
    "if kind == 'midpoint':\n",
    "    root = 'data/daily-midpoint/'\n",
    "elif kind == 'trades':\n",
    "    root = 'data/daily-trades/'\n",
    "elif kind == 'indices':\n",
    "    root = 'data/indices/'\n",
    "\n",
    "data_path = root + 'series/'\n",
    "verified_path = root + 'verified_files.csv'\n",
    "\n",
    "if kind in ['trades', 'indices']:\n",
    "    price_col = 'average'\n",
    "else:\n",
    "    price_col = 'close'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify files\n",
    "fund_df = load('data/fundamentals.csv')\n",
    "\n",
    "try:\n",
    "    verified_df = pd.read_csv(verified_path)\n",
    "except FileNotFoundError:\n",
    "    util.startLoop()\n",
    "    ib = IB()\n",
    "    ib.connect('127.0.0.1', 7497, clientId=2)\n",
    "\n",
    "    file_list = os.listdir(data_path)\n",
    "    verified_files = []\n",
    "\n",
    "    for file_name in tqdm(file_list, total=len(file_list), desc=\"Verifying files\"):\n",
    "        if not file_name.endswith('.csv'):\n",
    "            continue\n",
    "        try:\n",
    "            symbol, exchange, currency = file_name.replace('.csv', '').split('-')\n",
    "            symbol_data = fund_df[(fund_df['symbol'] == symbol) & (fund_df['currency'] == currency)]\n",
    "            if symbol_data.empty:\n",
    "                continue\n",
    "\n",
    "            contract_details = ib.reqContractDetails(Stock(symbol, exchange, currency))\n",
    "            if not contract_details:\n",
    "                continue\n",
    "            isin = contract_details[0].secIdList[0].value\n",
    "\n",
    "            if symbol_data['isin'].iloc[0] != isin:\n",
    "                continue\n",
    "\n",
    "            instrument_name = symbol_data['longName'].iloc[0].replace('-', '').replace('+', '')\n",
    "            leveraged = any(\n",
    "                re.fullmatch(r'\\d+X', word) and int(word[:-1]) > 1 or word.lower().startswith(('lv', 'lev'))\n",
    "                for word in instrument_name.split()\n",
    "            )\n",
    "            if leveraged:\n",
    "                continue\n",
    "\n",
    "            verified_files.append({'symbol': symbol, 'currency': currency})\n",
    "        except ValueError as e:\n",
    "            print(f\"Invalid filename format {file_name}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_name}: {e}\")\n",
    "\n",
    "    verified_df = pd.DataFrame(verified_files)\n",
    "    verified_df.to_csv(verified_path, index=False)\n",
    "\n",
    "    ib.disconnect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge historical series with fundamentals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_series_types(df, price_col):\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.sort_values('date').reset_index(drop=True)\n",
    "    for col in ['volume', price_col]:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    return df\n",
    "\n",
    "def validate_raw_prices(df, price_col):\n",
    "    invalid_price_mask = df[price_col] <= 0\n",
    "    inconsistent_mask = pd.Series(False, index=df.index)\n",
    "    if 'low' in df.columns and 'high' in df.columns:\n",
    "        inconsistent_mask = (df['low'] > df['high'])\n",
    "\n",
    "    local_error_mask = invalid_price_mask | inconsistent_mask\n",
    "    df = df[~local_error_mask].copy()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def handle_stale_periods(df, price_col, max_stale_days=5):\n",
    "    stale_groups = (df[price_col].diff() != 0).cumsum()\n",
    "    if stale_groups.empty:\n",
    "        return df\n",
    "    \n",
    "    period_lengths = df.groupby(stale_groups)[price_col].transform('size')\n",
    "    long_stale_mask = period_lengths > max_stale_days\n",
    "    \n",
    "    is_intermediate_stale_row = (stale_groups.duplicated(keep='first') & \n",
    "                             stale_groups.duplicated(keep='last'))\n",
    "    \n",
    "    rows_to_drop_mask = long_stale_mask & is_intermediate_stale_row\n",
    "    df = df[~rows_to_drop_mask].copy()\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load historical series\n",
    "if 'copied' not in globals() or input('reload csvs? (y/n)').lower() == 'y':\n",
    "    latest = (datetime.now() - timedelta(days=365 * 6))\n",
    "    first_date = (datetime.now())\n",
    "    meta = []\n",
    "    file_list = os.listdir(data_path)\n",
    "    for file in tqdm(file_list, total=len(file_list)):\n",
    "        if not file.endswith('.csv'):\n",
    "            continue\n",
    "        \n",
    "        parts = os.path.splitext(file)[0].split('-')\n",
    "        symbol, exchange, currency = parts[0], parts[1], parts[2]\n",
    "        if not ((verified_df['symbol'] == symbol) & (verified_df['currency'] == currency)).any():\n",
    "            continue\n",
    "        \n",
    "        # Load and clean raw series\n",
    "        try:\n",
    "            df = load(data_path + file)\n",
    "            df = ensure_series_types(df, price_col)\n",
    "            df = validate_raw_prices(df, price_col)\n",
    "            df = handle_stale_periods(df, price_col)\n",
    "            # df = adjust_for_splits(df, price_col)\n",
    "\n",
    "            df['pct_change'] = df[price_col].pct_change()\n",
    "            if df['date'].max() > latest:\n",
    "                latest = df['date'].max()\n",
    "            if df['date'].min() < first_date:\n",
    "                first_date = df['date'].min()\n",
    "\n",
    "            meta.append({\n",
    "                'symbol': symbol,\n",
    "                'currency': currency,\n",
    "                'exchange_api': exchange,\n",
    "                'df': df[['date', price_col, 'volume', 'pct_change']],\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR {file}: {e}\")\n",
    "            \n",
    "    meta = pd.DataFrame(meta)\n",
    "    copied = meta.copy()\n",
    "    copied['df'] = copied['df'].apply(lambda x: x.copy()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESET\n",
    "meta = copied.copy()\n",
    "meta['df'] = copied['df'].apply(lambda x: x.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_and_nullify_global_outliers(meta_df, price_col, z_threshold=120.0, window=5):\n",
    "    all_pct_changes = pd.concat(\n",
    "        [row['df']['pct_change'] for _, row in meta_df.iterrows()],\n",
    "        ignore_index=True\n",
    "    ).dropna()\n",
    "    all_pct_changes = all_pct_changes[~np.isinf(all_pct_changes) & (all_pct_changes != 0)]\n",
    "\n",
    "    global_median_return = all_pct_changes.median()\n",
    "    global_mad = (all_pct_changes - global_median_return).abs().median()\n",
    "\n",
    "    outlier_series = {}\n",
    "    # for idx, row in tqdm(meta_df.iterrows(), total=len(meta_df)):\n",
    "    for idx, row in meta_df.iterrows():\n",
    "        df = row['df']\n",
    "        df = df.reset_index(drop=True)\n",
    "        if df['pct_change'].isnull().all():\n",
    "            continue\n",
    "        cols_to_null = [price_col, 'volume', 'high', 'low', 'pct_change']\n",
    "        cols_to_null = [c for c in cols_to_null if c in df.columns]\n",
    "\n",
    "        absolute_modified_z = (df['pct_change'] - global_median_return).abs() / global_mad\n",
    "        outlier_mask = absolute_modified_z > z_threshold\n",
    "        \n",
    "        if outlier_mask.any():\n",
    "            data_dict = absolute_modified_z[outlier_mask].describe()\n",
    "\n",
    "            candidate_indices = df.index[outlier_mask]\n",
    "            for df_idx in candidate_indices:\n",
    "                price_to_check_idx = df_idx - 1\n",
    "                price_to_check = df.loc[price_to_check_idx, price_col]\n",
    "                local_window_start = max(0, price_to_check_idx - window)\n",
    "                local_window = df.loc[local_window_start : price_to_check_idx - 1, price_col].dropna()\n",
    "                local_mean = local_window.mean()\n",
    "                local_std = local_window.std()\n",
    "                if local_std != 0: \n",
    "                    price_z_score = abs(price_to_check - local_mean) / local_std\n",
    "                    if price_z_score > z_threshold / 10:\n",
    "                        df.loc[price_to_check_idx, cols_to_null] = np.nan\n",
    "\n",
    "                price_to_check = df.loc[df_idx, price_col]\n",
    "                local_window_end = min(df_idx + window, df.index[outlier_mask].max())\n",
    "                local_window = df.loc[df_idx + 1: local_window_end, price_col].dropna()\n",
    "                local_mean = local_window.mean()\n",
    "                local_std = local_window.std()\n",
    "                if local_std != 0:\n",
    "                    price_z_score = abs(price_to_check - local_mean) / local_std\n",
    "                    if price_z_score > z_threshold / 10:\n",
    "                        df.loc[df_idx, cols_to_null] = np.nan\n",
    "\n",
    "\n",
    "            data_dict['new_length'] = len(df)\n",
    "            outlier_series[row['symbol']] = data_dict\n",
    "            \n",
    "            df['pct_change'] = df[price_col].pct_change(fill_method=None)\n",
    "            \n",
    "            meta_df.at[idx, 'df'] = df\n",
    "\n",
    "    # return outlier_series\n",
    "\n",
    "z_threshold = 50\n",
    "window = 5\n",
    "# modified_series_info = detect_and_nullify_global_outliers(meta, price_col=price_col, z_threshold=z_threshold, window=window)\n",
    "detect_and_nullify_global_outliers(meta, price_col=price_col, z_threshold=z_threshold, window=window)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check global outliers ### NATURALLY PLOTS A FEW STRAGLERS\n",
    "# def global_return_filter(meta_df, z_threshold=120.0):\n",
    "#     all_pct_changes = pd.concat(\n",
    "#         [row['df']['pct_change'] for _, row in meta_df.iterrows()],\n",
    "#         ignore_index=True\n",
    "#     )\n",
    "#     all_pct_changes.dropna(inplace=True)\n",
    "#     all_pct_changes = all_pct_changes[~np.isinf(all_pct_changes) & (all_pct_changes != 0)]\n",
    "\n",
    "#     global_median_return = all_pct_changes.median()\n",
    "#     global_mad = (all_pct_changes - global_median_return).abs().median()\n",
    "\n",
    "#     outlier_series = {}\n",
    "#     for _, row in tqdm(meta_df.iterrows(), total=len(meta_df)):\n",
    "#         df = row['df']        \n",
    "#         absolute_modified_z = (df['pct_change'] - global_median_return).abs() / global_mad\n",
    "#         if absolute_modified_z.max() > z_threshold:\n",
    "#             outlier_series[row['symbol']] = absolute_modified_z.describe()\n",
    "\n",
    "#     return outlier_series, global_mad, global_median_return\n",
    "\n",
    "# z_threshold = 50\n",
    "# globally_defective_symbols, global_mad, global_median_return = global_return_filter(meta, z_threshold=z_threshold)\n",
    "# globally_defective_symbols = pd.DataFrame(globally_defective_symbols)\n",
    "\n",
    "# meta_indexed = meta.set_index('symbol')\n",
    "# for symbol in globally_defective_symbols.T.sort_values(by='max', ascending=True).index.tolist():\n",
    "#     df = meta_indexed.loc[symbol, 'df'].copy()\n",
    "#     df = df.reset_index(drop=True)\n",
    "\n",
    "#     absolute_modified_z = (df['pct_change'] - global_median_return).abs() / global_mad\n",
    "#     outlier_mask = absolute_modified_z > z_threshold\n",
    "\n",
    "#     corrected_outlier_mask = pd.Series(False, index=df.index)\n",
    "#     for df_idx in df.index[outlier_mask]:\n",
    "#         # Check data points before\n",
    "#         price_to_check_idx = df_idx - 1\n",
    "#         price_to_check = df.at[price_to_check_idx, price_col]\n",
    "#         local_window_start = max(0, price_to_check_idx - window)\n",
    "#         local_window = df.loc[local_window_start : price_to_check_idx - 1, price_col].dropna()\n",
    "#         local_mean = local_window.mean()\n",
    "#         local_std = local_window.std()\n",
    "#         if local_std != 0:\n",
    "#             price_z_score = abs(price_to_check - local_mean) / local_std\n",
    "#             if price_z_score > z_threshold / 10:\n",
    "#                 corrected_outlier_mask.at[price_to_check_idx] = True\n",
    "\n",
    "#         # Check data points after\n",
    "#         price_to_check = df.at[df_idx, price_col]\n",
    "#         local_window_end = min(df_idx + window, df.index[outlier_mask].max())\n",
    "#         local_window = df.loc[df_idx + 1: local_window_end, price_col].dropna()\n",
    "#         local_mean = local_window.mean()\n",
    "#         local_std = local_window.std()\n",
    "#         if local_std != 0:\n",
    "#             price_z_score = abs(price_to_check - local_mean) / local_std\n",
    "#             if price_z_score > z_threshold / 10:\n",
    "#                 corrected_outlier_mask.at[df_idx] = True\n",
    "\n",
    "#     if corrected_outlier_mask.any():\n",
    "#         # Plotting\n",
    "#         plt.figure(figsize=(10, 6))\n",
    "#         plt.plot(df['date'], df[price_col], marker='o', label='Normal')\n",
    "#         plt.scatter(df.loc[corrected_outlier_mask, 'date'],\n",
    "#                     df.loc[corrected_outlier_mask, price_col],\n",
    "#                     color='red', label='Outlier', zorder=5)\n",
    "\n",
    "#         plt.title(f\"Symbol: {symbol}\")\n",
    "#         plt.xlabel(\"Date\")\n",
    "#         plt.ylabel(price_col)\n",
    "#         plt.legend()\n",
    "#         plt.tight_layout()\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Delete duplicates ETF files\n",
    "# duplicates = meta[meta.duplicated(subset=['symbol', 'currency'], keep=False)].copy()\n",
    "# duplicates['not_smart'] = duplicates['exchange_api'] != 'SMART'\n",
    "# duplicates['length'] = duplicates['df'].apply(len)\n",
    "\n",
    "# sorted_duplicates = duplicates.sort_values(\n",
    "#     by=['symbol', 'currency', 'length', 'not_smart'],\n",
    "#     ascending=[True, True, False, False]\n",
    "# )\n",
    "\n",
    "# rows_to_keep = sorted_duplicates.groupby(['symbol', 'currency']).head(1)\n",
    "# rows_to_delete = duplicates[~duplicates.index.isin(rows_to_keep.index)]\n",
    "# for idx, row in rows_to_delete.iterrows():\n",
    "#     file_name = f\"{row['symbol']}-{row['exchange_api']}-{row['currency']}.csv\"\n",
    "#     file_path = os.path.join(data_path, file_name)\n",
    "#     if os.path.exists(file_path):\n",
    "#         os.remove(file_path)\n",
    "#         print(f\"Deleted {file_path}\")\n",
    "#     else:\n",
    "#         print(f\"File not found: {file_path}\")\n",
    "\n",
    "# del duplicates, sorted_duplicates, rows_to_keep, rows_to_delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate series gap stats\n",
    "while True:\n",
    "    year_range = int(input('Year range (>= 2):'))\n",
    "    if year_range >= 2:\n",
    "        break\n",
    "\n",
    "oldest = latest - pd.Timedelta(days=365 * year_range)\n",
    "business_days = pd.date_range(start=oldest, end=latest, freq='B')\n",
    "\n",
    "# Calculate statistics for each DataFrame in meta\n",
    "for idx, row in tqdm(meta.iterrows(), total=len(meta)):\n",
    "    df = row['df']\n",
    "    merged = pd.merge(pd.DataFrame({'date': business_days}), df, on='date', how='left')\n",
    "    \n",
    "    # Calculate gaps\n",
    "    present = merged[price_col].notna()\n",
    "    present_idx = np.flatnonzero(present)\n",
    "    gaps = []\n",
    "    length = len(merged)\n",
    "\n",
    "    if present_idx.size > 0:\n",
    "        if present_idx[0] > 0:\n",
    "            gaps.append(present_idx[0])\n",
    "        if present_idx.size > 1:\n",
    "            internal_gaps = np.diff(present_idx) - 1\n",
    "            gaps.extend(gap for gap in internal_gaps if gap > 0)\n",
    "        if present_idx[-1] < length - 1:\n",
    "            gaps.append(length - 1 - present_idx[-1])\n",
    "    else:\n",
    "        gaps = [length]\n",
    "\n",
    "    gaps = np.array(gaps, dtype=int)\n",
    "    gaps = gaps[gaps > 0]\n",
    "    max_gap = float(gaps.max()) if gaps.size > 0 else 0.0\n",
    "    std_gap = float(gaps.std()) if gaps.size > 0 else 0.0\n",
    "    missing = length - present.sum()\n",
    "    pct_missing = missing / length\n",
    "\n",
    "    # Update meta with statistics\n",
    "    meta.at[idx, 'df'] = merged\n",
    "    meta.at[idx, 'max_gap'] = max_gap\n",
    "    meta.at[idx, 'missing'] = missing\n",
    "    meta.at[idx, 'pct_missing'] = pct_missing\n",
    "\n",
    "print(f'Latest: {latest}')\n",
    "print(f'Oldest: {oldest}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove series with large day gaps\n",
    "meta['max_gap_log'] = np.log1p(meta['max_gap'])\n",
    "\n",
    "condition = ((meta['max_gap_log'] < meta['max_gap_log'].mean()) & \n",
    "             (meta['pct_missing'] < meta['pct_missing'].mean()))\n",
    "\n",
    "# HARD-CODED 3y window mean stats\n",
    "max_gap_log = 3.0415511502218044\n",
    "max_pct_missing = 0.2915700460994459\n",
    "condition = ((meta['max_gap_log'] < max_gap_log) & \n",
    "             (meta['pct_missing'] < max_pct_missing))\n",
    "\n",
    "filtered = meta[condition].copy()\n",
    "\n",
    "print(f'{len(filtered)} ETFs included')\n",
    "print(f'{len(meta) - len(filtered)} dropped')\n",
    "del meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate/extrapolate price column and merge with fund\n",
    "for idx, row in tqdm(filtered.iterrows(), total=len(filtered)):\n",
    "    df = row['df']\n",
    "    df[price_col] = df[price_col].interpolate(method='akima', limit_direction='both')\n",
    "    if df[price_col].isna().any():\n",
    "        df[price_col] = df[price_col].ffill()\n",
    "        df[price_col] = df[price_col].bfill()\n",
    "    \n",
    "    df['pct_change'] = df[price_col].pct_change()\n",
    "    filtered.at[idx, 'df'] = df.set_index('date')\n",
    "\n",
    "filtered = pd.merge(filtered, fund_df, on=['symbol', 'currency'], how='inner').drop(['max_gap', 'missing', 'pct_missing', 'max_gap_log'], axis=1)\n",
    "\n",
    "del fund_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Spectral preprocessing\n",
    "# from statsmodels.tsa.stattools import adfuller\n",
    "# from scipy.signal import welch\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def test_stationarity(series, signif=0.05, regression='c'):\n",
    "#     try:\n",
    "#         series = series.dropna()\n",
    "#         adf_res = adfuller(series, regression=regression, autolag='AIC')\n",
    "#         stat, pval, used_lags, nobs, crit_vals, icbest = adf_res\n",
    "#         return {\n",
    "#             'test_statistic': stat,\n",
    "#             'p_value': pval,\n",
    "#             'used_lags': used_lags,\n",
    "#             'nobs': nobs,\n",
    "#             'critical_values': crit_vals,\n",
    "#             'is_stationary': (pval < signif),\n",
    "#             'icbest': icbest,\n",
    "#         }\n",
    "#     except Exception as e:\n",
    "#         print(f'{e}')\n",
    "#         return {\n",
    "#             'test_statistic': np.nan,\n",
    "#             'p_value': np.nan,\n",
    "#             'used_lags': np.nan,\n",
    "#             'nobs': np.nan,\n",
    "#             'critical_values': np.nan,\n",
    "#             'is_stationary': np.nan,\n",
    "#             'icbest': np.nan,\n",
    "#         }\n",
    "\n",
    "# def demean(series):\n",
    "#     mu = series.mean()\n",
    "#     return series - mu\n",
    "\n",
    "# def apply_window(series, window_type='hanning'):\n",
    "#     x = series.values if hasattr(series, 'values') else np.asarray(series)\n",
    "#     N = len(x)\n",
    "    \n",
    "#     window_map = {\n",
    "#         'hanning': np.hanning,\n",
    "#         'hamming': np.hamming,\n",
    "#         'bartlett':np.bartlett,\n",
    "#         'blackman':np.blackman,\n",
    "#     }\n",
    "    \n",
    "#     w = window_map[window_type](N)\n",
    "#     return x * w\n",
    "\n",
    "# def compute_welch_psd(series, fs=1.0, window='hann', nperseg=256, noverlap=None):\n",
    "#     if noverlap is None:\n",
    "#         noverlap = nperseg // 2\n",
    "#     freqs, psd = welch(\n",
    "#         series.dropna().values,\n",
    "#         fs=fs,\n",
    "#         window=window,\n",
    "#         nperseg=nperseg,\n",
    "#         noverlap=noverlap,\n",
    "#         detrend=False,\n",
    "#         scaling='density'\n",
    "#     )\n",
    "#     return freqs, psd\n",
    "\n",
    "\n",
    "# def analyze_spectrum_and_extract_features(series, fs=1.0, nperseg=256):\n",
    "#     series = series.dropna()\n",
    "#     if len(series) < nperseg:\n",
    "#         raise\n",
    "\n",
    "#     stationarity_info = test_stationarity(series)\n",
    "#     if not stationarity_info['is_stationary']:\n",
    "#         return {'is_stationary': False}\n",
    "\n",
    "#     demeaned_series = demean(series)\n",
    "#     try:\n",
    "#         freqs, psd = welch(\n",
    "#             demeaned_series,\n",
    "#             fs=fs,\n",
    "#             window='hann',\n",
    "#             nperseg=nperseg,\n",
    "#             noverlap=nperseg // 2,\n",
    "#             detrend=False,\n",
    "#             scaling='density'\n",
    "#         )\n",
    "#     except ValueError as e:\n",
    "#         print(e)\n",
    "#         print('welch')\n",
    "#         return None\n",
    "\n",
    "#     if len(freqs) < 2:\n",
    "#         print('small freqs')\n",
    "#         return None\n",
    "        \n",
    "#     non_zero_freq_mask = freqs > 0\n",
    "#     freqs = freqs[non_zero_freq_mask]\n",
    "#     psd = psd[non_zero_freq_mask]\n",
    "\n",
    "#     if len(psd) == 0:\n",
    "#         print('small psd')\n",
    "#         return None\n",
    "\n",
    "#     # Dominant cycle: Frequency with the highest power\n",
    "#     dominant_freq_idx = np.argmax(psd)\n",
    "#     dominant_freq = freqs[dominant_freq_idx]\n",
    "#     dominant_freq_power = psd[dominant_freq_idx]\n",
    "#     dominant_period_days = 1 / dominant_freq if dominant_freq != 0 else np.inf # Convert frequency to period in days\n",
    "\n",
    "#     # A high ratio suggests a strong, clear cycle. A low ratio suggests noise.\n",
    "#     spectral_concentration = dominant_freq_power / np.mean(psd)\n",
    "\n",
    "#     return {\n",
    "#         'is_stationary': True,\n",
    "#         'dominant_freq': dominant_freq,\n",
    "#         'dominant_period_days': dominant_period_days,\n",
    "#         'dominant_freq_power': dominant_freq_power,\n",
    "#         'spectral_concentration': spectral_concentration,\n",
    "#     }\n",
    "\n",
    "# spectral_features = []\n",
    "# for idx, row in tqdm(filtered.iterrows(), total=len(filtered)):\n",
    "#     df = row['df']\n",
    "#     returns = df['pct_change'].dropna()\n",
    "    \n",
    "#     features = analyze_spectrum_and_extract_features(returns, nperseg=256)\n",
    "#     if features:\n",
    "#         features['conId'] = row['conId']\n",
    "#         spectral_features.append(features)\n",
    "\n",
    "# spectral_df = pd.DataFrame(spectral_features)\n",
    "# if not spectral_df.empty:\n",
    "#     filtered = pd.merge(filtered, spectral_df, on=['conId'], how='left')\n",
    "\n",
    "# # You can now use these new columns for further filtering.\n",
    "# # For example, you might want to investigate or exclude series that are:\n",
    "# # - Non-stationary (is_stationary == False)\n",
    "# # - Have a very low spectral concentration (indicating they are mostly noise)\n",
    "# # - Have a dominant period that seems suspicious (e.g., exactly 7 days for a business-day series)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot asset class portfolios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Risk-free series calculation\n",
    "import pandas_datareader.data as web\n",
    "\n",
    "# 3-month bill/interest rate tickers (FRED/OECD) for each country\n",
    "tickers = {\n",
    "    'US': 'DTB3',\n",
    "    'Canada': 'IR3TIB01CAM156N',\n",
    "    'Germany': 'IR3TIB01DEM156N',\n",
    "    'UK': 'IR3TIB01GBM156N',\n",
    "    'France': 'IR3TIB01FRA156N',\n",
    "}\n",
    "\n",
    "# Fetch each series and convert from percentage to decimal\n",
    "bonds = {}\n",
    "failed = []\n",
    "for country, ticker in tickers.items():\n",
    "    try:\n",
    "        series = web.DataReader(ticker, 'fred', oldest, latest)\n",
    "        bonds[country] = series / 100.0\n",
    "    except Exception:\n",
    "        try:\n",
    "            series = web.DataReader(ticker, 'oecd', oldest, latest)\n",
    "            bonds[country] = series / 100.0\n",
    "        except Exception as oecd_err:\n",
    "            failed.append(country)\n",
    "\n",
    "# Combine into a single DataFrame\n",
    "df_bonds = pd.concat(bonds, axis=1)\n",
    "df_bonds.columns = [c for c in tickers if c not in failed]\n",
    "df_bonds = df_bonds.interpolate(method='akima').bfill().ffill()\n",
    "\n",
    "risk_free_df = df_bonds.mean(axis=1).rename('nominal_rate')\n",
    "risk_free_df = risk_free_df.reindex(business_days, copy=False)\n",
    "\n",
    "risk_free_df = pd.DataFrame(risk_free_df)\n",
    "risk_free_df['daily_nominal_rate'] = risk_free_df['nominal_rate'] / 252\n",
    "\n",
    "print(f'Short-term bonds used from: {df_bonds.columns.to_list()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add pct_change cols to dfs and create pct_changes\n",
    "numerical_cols = [col for col in filtered.columns if filtered[col].dtype in [np.int64, np.float64] and col not in ['conId']]\n",
    "\n",
    "pct_changes = pd.concat(\n",
    "        [row['df']['pct_change'].rename(row['conId']) \n",
    "        for _, row in filtered.iterrows()], axis=1\n",
    "    )\n",
    "\n",
    "# Remove uninformative cols for market portfolios \n",
    "uninformative_cols = [col for col in numerical_cols if filtered[col].nunique(dropna=True) <= 1]\n",
    "filtered = filtered.drop(columns=uninformative_cols)\n",
    "filtered = filtered.dropna(axis=1, how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add rate of change fundamentals\n",
    "def calculate_slope(value1, value2, time1, time2):\n",
    "    return (value1 - value2) / (time1 - time2)\n",
    "\n",
    "\n",
    "rate_fundamentals = [('EPSGrowth-1yr', 'EPS_growth_3yr', 'EPS_growth_5yr'),\n",
    "                     ('ReturnonAssets1Yr', 'ReturnonAssets3Yr'),\n",
    "                     ('ReturnonCapital', 'ReturnonCapital3Yr'),\n",
    "                     ('ReturnonEquity1Yr', 'ReturnonEquity3Yr'),\n",
    "                     ('ReturnonInvestment1Yr', 'ReturnonInvestment3Yr')]\n",
    "\n",
    "for cols in rate_fundamentals:\n",
    "    base_name = cols[0].replace('-1yr', '').replace('1Yr', '')\n",
    "    slope_col = f'fundamentals_{base_name}_slope'\n",
    "    \n",
    "    if len(cols) == 3:\n",
    "        col_1yr, col_3yr, col_5yr = cols\n",
    "\n",
    "        filtered[slope_col] = calculate_slope(\n",
    "            filtered[f'fundamentals_{col_1yr}'],\n",
    "            filtered[f'fundamentals_{col_5yr}'],\n",
    "            1, 5\n",
    "        )\n",
    "\n",
    "        if 'EPS' in base_name:\n",
    "            slope_1yr_3yr = calculate_slope(\n",
    "                filtered[f'fundamentals_{col_1yr}'],\n",
    "                filtered[f'fundamentals_{col_3yr}'],\n",
    "                1, 3\n",
    "            )\n",
    "            slope_3yr_5yr = calculate_slope(\n",
    "                filtered[f'fundamentals_{col_3yr}'],\n",
    "                filtered[f'fundamentals_{col_5yr}'],\n",
    "                3, 5\n",
    "            )\n",
    "            \n",
    "            second_deriv_col = f'fundamentals_{base_name}_second_deriv'\n",
    "            filtered[second_deriv_col] = calculate_slope(\n",
    "                slope_1yr_3yr,\n",
    "                slope_3yr_5yr,\n",
    "                1, 3\n",
    "            )\n",
    "    elif len(cols) == 2:\n",
    "        col_1yr, col_3yr = cols\n",
    "        filtered[slope_col] = calculate_slope(\n",
    "            filtered[f'fundamentals_{col_1yr}'],\n",
    "            filtered[f'fundamentals_{col_3yr}'],\n",
    "            1, 3\n",
    "        )\n",
    "\n",
    "# Add new cols to numericals\n",
    "numerical_cols = [col for col in filtered.columns if filtered[col].dtype in [np.int64, np.float64] and col not in ['conId']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return stats and split training and tests sets\n",
    "def get_return_stats(df, training_cutoff, momentum_cutoffs, risk_free_df):\n",
    "    training_df = df[df.index < training_cutoff]\n",
    "    training_rf = risk_free_df[risk_free_df.index < training_cutoff]\n",
    "\n",
    "    excess_returns = training_df['pct_change'] - training_rf['daily_nominal_rate']\n",
    "    sharpe = excess_returns.mean() / excess_returns.std()\n",
    "    # avg_volume = training_df['volume'].mean()\n",
    "\n",
    "    momentum_3mo = training_df[training_df.index >= momentum_cutoffs['3mo']]['pct_change'].mean()\n",
    "    momentum_6mo = training_df[training_df.index >= momentum_cutoffs['6mo']]['pct_change'].mean()\n",
    "    momentum_1y  = training_df[training_df.index >= momentum_cutoffs['1y']]['pct_change'].mean()\n",
    "\n",
    "    return pd.Series(\n",
    "        [momentum_3mo, momentum_6mo, momentum_1y, sharpe],#, avg_volume],\n",
    "        index=['momentum_3mo', 'momentum_6mo', 'momentum_1y', 'stats_sharpe']#, 'stats_avg_volume']\n",
    "    )\n",
    "\n",
    "final_20_pct = (latest-oldest).days//5\n",
    "training_cutoff = latest - pd.Timedelta(days=final_20_pct)\n",
    "momentum_cutoffs = {\n",
    "    '1y':  training_cutoff - pd.Timedelta(days=365),\n",
    "    '6mo': training_cutoff - pd.Timedelta(days=365 // 2),\n",
    "    '3mo': training_cutoff - pd.Timedelta(days=365 // 4),\n",
    "}\n",
    "\n",
    "# Apply to each row\n",
    "# filtered[['momentum_3mo', 'momentum_6mo', 'momentum_1y', 'stats_sharpe', 'stats_avg_volume']] = filtered['df'].apply(lambda df: get_return_stats(df, training_cutoff, momentum_cutoffs, risk_free_df))\n",
    "filtered[['momentum_3mo', 'momentum_6mo', 'momentum_1y', 'stats_sharpe']] = filtered['df'].apply(lambda df: get_return_stats(df, training_cutoff, momentum_cutoffs, risk_free_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all asset type indices/portfolios\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "holding_cols = [col for col in filtered.columns if col.startswith('holding_') and col != 'holding_types_variety'] + ['total']\n",
    "portfolio_dfs = {}\n",
    "\n",
    "for holding_col in holding_cols:\n",
    "    name = holding_col.split('_')[-1]\n",
    "    if holding_col == 'total':\n",
    "        weight = filtered['profile_cap_usd']\n",
    "    else:\n",
    "        weight = (filtered['profile_cap_usd'] * filtered[holding_col])\n",
    " \n",
    "    total_market_cap = (weight).sum()\n",
    "    filtered['weight'] = weight / total_market_cap\n",
    "    \n",
    "    weights = filtered.set_index('conId')['weight']\n",
    "    portfolio_return = pct_changes.dot(weights)\n",
    "    initial_price = 1\n",
    "    portfolio_price = initial_price * (1 + portfolio_return.fillna(0)).cumprod()\n",
    "\n",
    "    portfolio_df = pd.DataFrame({\n",
    "        'date': portfolio_price.index,\n",
    "        price_col: portfolio_price.values,\n",
    "        'pct_change': portfolio_return.values\n",
    "    }).set_index('date')\n",
    "\n",
    "    portfolio_dfs[name] = portfolio_df\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.title(f'{name.capitalize()} portfolio  -  ${format(total_market_cap, ',.0f')}')\n",
    "    plt.plot(portfolio_df.index, portfolio_df[price_col], marker='o')\n",
    "    plt.show()\n",
    "\n",
    "filtered.drop('weight', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Manual plot\n",
    "# symbol_test = 'SHV'\n",
    "# x = filtered[filtered['symbol'] == symbol_test].df.iloc[0].index\n",
    "# y = filtered[filtered['symbol'] == symbol_test].df.iloc[0]['average']#.pct_change()\n",
    "\n",
    "# y = risk_free_df['daily_nominal_rate']\n",
    "# x = risk_free_df.index\n",
    "# # y = df_bonds['UK']\n",
    "# # x = df_bonds.index\n",
    "# # y = portfolio_dfs['equity']['pct_change'] - risk_free_df['daily_nominal_rate']\n",
    "# # x = portfolio_dfs['equity'].index\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(x, y, marker='o')\n",
    "# # plt.xlim(market_portfolio_df['date'].min(), market_portfolio_df['date'].max())\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoid dummy trap\n",
    "empty_subcategories = {\n",
    "'holding_types': ['other'],\n",
    "'countries': ['Unidentified'], \n",
    "'currencies': ['<NoCurrency>'],\n",
    "'industries': ['NonClassifiedEquity', 'NotClassified-NonEquity'],\n",
    "'top10': ['OtherAssets', 'AccountsPayable','AccountsReceivable','AccountsReceivable&Pay','AdministrationFees','CustodyFees','ManagementFees','OtherAssetsandLiabilities','OtherAssetslessLiabilities', 'OtherFees','OtherLiabilities','Tax','Tax--ManagementFees'],\n",
    "'debtors': ['OTHER'],\n",
    "'maturity': ['%MaturityOther'],\n",
    "'debt_type': ['%QualityNotAvailable', '%QualityNotRated'],\n",
    "'manual': ['asset_other']\n",
    "}\n",
    "\n",
    "dummy_trap_cols = []\n",
    "for k, lst in empty_subcategories.items():\n",
    "    for i in lst:\n",
    "        if k == 'manual':\n",
    "            dummy_trap_cols.append(i)\n",
    "        else:\n",
    "            dummy_trap_cols.append(f'{k}_{i}')\n",
    "    \n",
    "filtered = filtered.drop(columns=dummy_trap_cols, axis=1, errors='ignore')\n",
    "numerical_cols = [col for col in filtered.columns if filtered[col].dtype in [np.int64, np.float64] and col not in ['conId']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select asset types to work on\n",
    "asset_conditions = {\n",
    "    'equity': (filtered['asset_equity'] == 1),\n",
    "    'cash': (filtered['asset_cash'] == 1),\n",
    "    'bond': (filtered['asset_bond'] == 1),\n",
    "    'other': (filtered['asset_equity'] == 0) & (filtered['asset_cash'] == 0) & (filtered['asset_bond'] == 0),\n",
    "}\n",
    "\n",
    "exclude_assets = ['bond']\n",
    "asset_classes = list(asset_conditions.keys())\n",
    "\n",
    "include_assets = [asset for asset in asset_classes if asset not in exclude_assets]\n",
    "combined_condition = pd.Series(False, index=filtered.index)\n",
    "for asset in include_assets:\n",
    "    combined_condition |= asset_conditions[asset]\n",
    "\n",
    "filtered_df = filtered[combined_condition]\n",
    "numerical_cols = [col for col in filtered_df.columns if filtered_df[col].dtype in [np.int64, np.float64] and col not in ['conId']]\n",
    "\n",
    "single_value_columns = [col for col in filtered_df.columns if col in numerical_cols and filtered_df[col].nunique() == 1]\n",
    "asset_cols = [col for col in filtered_df if col.startswith('asset')]\n",
    "filtered_df = filtered_df.drop(columns=single_value_columns + asset_cols)\n",
    "numerical_cols = [col for col in filtered_df.columns if filtered_df[col].dtype in [np.int64, np.float64] and col not in ['conId']]\n",
    "if exclude_assets:\n",
    "    print(\"Excluding assets:\", ', '.join(exclude_assets))\n",
    "else:\n",
    "    print(\"No assets excluded.\")\n",
    "\n",
    "pct_changes = pct_changes[filtered_df['conId']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select asset types to work on\n",
    "from fuzzywuzzy import fuzz\n",
    "import pandas as pd\n",
    "\n",
    "asset_conditions = {\n",
    "    'equity': (filtered['asset_equity'] == 1),\n",
    "    'cash': (filtered['asset_cash'] == 1),\n",
    "    'bond': (filtered['asset_bond'] == 1),\n",
    "    'other': (filtered['asset_equity'] == 0) & (filtered['asset_cash'] == 0) & (filtered['asset_bond'] == 0),\n",
    "}\n",
    "\n",
    "asset_to_exclude = input(\"Assets to EXCLUDE (equity, bond, cash, other): \").lower().replace(',', ' ').split()\n",
    "\n",
    "asset_classes = list(asset_conditions.keys())\n",
    "exclude_assets = set()\n",
    "for word in asset_to_exclude:\n",
    "    scores = [(asset, fuzz.ratio(word, asset.lower())) for asset in asset_classes]\n",
    "    best_asset, best_score = max(scores, key=lambda x: x[1])\n",
    "    if best_score >= 70:\n",
    "        exclude_assets.add(best_asset)\n",
    "\n",
    "include_assets = [asset for asset in asset_classes if asset not in exclude_assets]\n",
    "combined_condition = pd.Series(False, index=filtered.index)\n",
    "for asset in include_assets:\n",
    "    combined_condition |= asset_conditions[asset]\n",
    "\n",
    "filtered_df = filtered[combined_condition]\n",
    "numerical_cols = [col for col in filtered_df.columns if filtered_df[col].dtype in [np.int64, np.float64] and col not in ['conId']]\n",
    "\n",
    "single_value_columns = [col for col in filtered_df.columns if col in numerical_cols and filtered_df[col].nunique() == 1]\n",
    "asset_cols = [col for col in filtered_df if col.startswith('asset')]\n",
    "filtered_df = filtered_df.drop(columns=single_value_columns + asset_cols)\n",
    "numerical_cols = [col for col in filtered_df.columns if filtered_df[col].dtype in [np.int64, np.float64] and col not in ['conId']]\n",
    "if exclude_assets:\n",
    "    print(\"Excluding assets:\", ', '.join(exclude_assets))\n",
    "else:\n",
    "    print(\"No assets excluded.\")\n",
    "\n",
    "pct_changes = pct_changes[filtered_df['conId']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Country compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename to standard country strings\n",
    "import country_converter as coco\n",
    "\n",
    "cc = coco.CountryConverter()\n",
    "country_cols = [col for col in filtered_df.columns if col.startswith('countries') and not col.endswith('variety')]\n",
    "\n",
    "standard_names = set()\n",
    "rename_map = {}\n",
    "for col in country_cols:\n",
    "    if col == 'countries_Unidentified':\n",
    "        continue\n",
    "\n",
    "    raw_name = col.replace('countries_', '')\n",
    "    raw_name = ''.join(raw_name.split(' '))\n",
    "    raw_name = ''.join([' ' + char if char.isupper() and i > 0 else char for i, char in enumerate(raw_name)]).strip()\n",
    "\n",
    "    standard_name = cc.convert(names=raw_name, to='ISO3', not_found=None)\n",
    "    standard_names.add(standard_name)\n",
    "    if standard_name:\n",
    "        rename_map[col] = f'countries_{standard_name}'\n",
    "    else:\n",
    "        print(f\"Could not standardize: '{raw_name}' (from column '{col}')\")\n",
    "\n",
    "# Apply renaming\n",
    "filtered_df.rename(columns=rename_map, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSCI classifications dict\n",
    "msci_map = {\n",
    "    # Developed Markets\n",
    "    \"Canada\": \"Developed\",\n",
    "    \"USA\": \"Developed\",\n",
    "    \"Austria\": \"Developed\",\n",
    "    \"Belgium\": \"Developed\",\n",
    "    \"Denmark\": \"Developed\",\n",
    "    \"Finland\": \"Developed\",\n",
    "    \"France\": \"Developed\",\n",
    "    \"Germany\": \"Developed\",\n",
    "    \"Ireland\": \"Developed\",\n",
    "    \"Israel\": \"Developed\",\n",
    "    \"Italy\": \"Developed\",\n",
    "    \"Netherlands\": \"Developed\",\n",
    "    \"Norway\": \"Developed\",\n",
    "    \"Portugal\": \"Developed\",\n",
    "    \"Spain\": \"Developed\",\n",
    "    \"Sweden\": \"Developed\",\n",
    "    \"Switzerland\": \"Developed\",\n",
    "    \"UK\": \"Developed\",\n",
    "    \"Australia\": \"Developed\",\n",
    "    \"Hong Kong\": \"Developed\",\n",
    "    \"Japan\": \"Developed\",\n",
    "    \"New Zealand\": \"Developed\",\n",
    "    \"Singapore\": \"Developed\",\n",
    "    \"Luxembourg\": \"Developed\",\n",
    "    \"Slovakia\": \"Developed\",\n",
    "    \"Cyprus\": \"Developed\",\n",
    "\n",
    "    # Emerging Markets\n",
    "    \"Brazil\": \"Emerging\",\n",
    "    \"Chile\": \"Emerging\",\n",
    "    \"Colombia\": \"Emerging\",\n",
    "    \"Mexico\": \"Emerging\",\n",
    "    \"Peru\": \"Emerging\",\n",
    "    \"Czech Republic\": \"Emerging\",\n",
    "    \"Egypt\": \"Emerging\",\n",
    "    \"Greece\": \"Emerging\",\n",
    "    \"Hungary\": \"Emerging\",\n",
    "    \"Kuwait\": \"Emerging\",\n",
    "    \"Poland\": \"Emerging\",\n",
    "    \"Qatar\": \"Emerging\",\n",
    "    \"Saudi Arabia\": \"Emerging\",\n",
    "    \"South Africa\": \"Emerging\",\n",
    "    \"Turkey\": \"Emerging\",\n",
    "    \"United Arab Emirates\": \"Emerging\",\n",
    "    \"China\": \"Emerging\",\n",
    "    \"India\": \"Emerging\",\n",
    "    \"Indonesia\": \"Emerging\",\n",
    "    \"Korea\": \"Emerging\",\n",
    "    \"Malaysia\": \"Emerging\",\n",
    "    \"Philippines\": \"Emerging\",\n",
    "    \"Taiwan\": \"Emerging\",\n",
    "    \"Thailand\": \"Emerging\",\n",
    "    \"Bahamas\": \"Emerging\",\n",
    "    \"Costa Rica\": \"Emerging\",\n",
    "    \"Dominican Republic\": \"Emerging\",\n",
    "    \"Mongolia\": \"Emerging\",\n",
    "    \"Uruguay\": \"Emerging\",\n",
    "    \"Barbados\": \"Emerging\",\n",
    "\n",
    "    # Frontier Markets\n",
    "    \"Bahrain\": \"Frontier\",\n",
    "    \"Benin\": \"Frontier\",\n",
    "    \"Burkina Faso\": \"Frontier\",\n",
    "    \"Croatia\": \"Frontier\",\n",
    "    \"Estonia\": \"Frontier\",\n",
    "    \"Guinea-Bissau\": \"Frontier\",\n",
    "    \"Iceland\": \"Frontier\",\n",
    "    \"Ivory Coast\": \"Frontier\",\n",
    "    \"Jordan\": \"Frontier\",\n",
    "    \"Kazakhstan\": \"Frontier\",\n",
    "    \"Kenya\": \"Frontier\",\n",
    "    \"Latvia\": \"Frontier\",\n",
    "    \"Lithuania\": \"Frontier\",\n",
    "    \"Mali\": \"Frontier\",\n",
    "    \"Mauritius\": \"Frontier\",\n",
    "    \"Morocco\": \"Frontier\",\n",
    "    \"Niger\": \"Frontier\",\n",
    "    \"Oman\": \"Frontier\",\n",
    "    \"Romania\": \"Frontier\",\n",
    "    \"Senegal\": \"Frontier\",\n",
    "    \"Serbia\": \"Frontier\",\n",
    "    \"Slovenia\": \"Frontier\",\n",
    "    \"Togo\": \"Frontier\",\n",
    "    \"Tunisia\": \"Frontier\",\n",
    "    \"Bangladesh\": \"Frontier\",\n",
    "    \"Pakistan\": \"Frontier\",\n",
    "    \"Sri Lanka\": \"Frontier\",\n",
    "    \"Vietnam\": \"Frontier\",\n",
    "\n",
    "    # Standalone Markets\n",
    "    \"Argentina\": \"Standalone\",\n",
    "    \"Jamaica\": \"Standalone\",\n",
    "    \"Panama\": \"Standalone\",\n",
    "    \"Trinidad and Tobago\": \"Standalone\",\n",
    "    \"Bosnia and Herzegovina\": \"Standalone\",\n",
    "    \"Bulgaria\": \"Standalone\",\n",
    "    \"Lebanon\": \"Standalone\",\n",
    "    \"Malta\": \"Standalone\",\n",
    "    \"Nigeria\": \"Standalone\",\n",
    "    \"Palestine\": \"Standalone\",\n",
    "    \"Ukraine\": \"Standalone\",\n",
    "    \"Russia\": \"Standalone\",\n",
    "    \"Zimbabwe\": \"Standalone\",\n",
    "    \"Venezuela\": \"Standalone\",\n",
    "    \"Liechtenstein\": \"Standalone\",\n",
    "    \"British Virgin Islands\": \"Standalone\",\n",
    "    \"Faroe Islands\": \"Standalone\",\n",
    "    \"Guernsey\": \"Standalone\",\n",
    "    \"Cayman Islands\": \"Standalone\",\n",
    "    \"Jersey\": \"Standalone\",\n",
    "    \"Isle of Man\": \"Standalone\",\n",
    "    \"Bermuda\": \"Standalone\",\n",
    "    \"Monaco\": \"Standalone\",\n",
    "    \"Macau\": \"Standalone\",\n",
    "    \"Puerto Rico\": \"Standalone\",\n",
    "    \"United States Virgin Islands\": \"Standalone\",\n",
    "}\n",
    "\n",
    "msci_map = {\n",
    "    cc.convert(names=code, to='ISO3', not_found=None): value\n",
    "    for code, value in msci_map.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discrete GDP and pop functions\n",
    "import wbgapi as wb\n",
    "\n",
    "def create_continent_map(standard_names):\n",
    "    continents = cc.convert(names=standard_names, to='continent', not_found=None)\n",
    "    return {name: (cont if cont is not None else 'Other')\n",
    "            for name, cont in zip(standard_names, continents)}\n",
    "\n",
    "def create_metric_maps(standard_names, indicators, start_year, end_year, window_size=3):\n",
    "    data = wb.data.DataFrame(list(indicators), standard_names, time=range(2000, end_year.year + 1), labels=False)\n",
    "    data.dropna(axis=1, inplace=True) \n",
    "\n",
    "    yoy_change = data.diff(axis=1)\n",
    "    first_div = yoy_change.T.rolling(window=window_size).mean().T\n",
    "    \n",
    "    yoy_change_first_div = first_div.diff(axis=1)\n",
    "    second_div = yoy_change_first_div.T.rolling(window=window_size).mean().T\n",
    "\n",
    "    latest_year_col = data.columns[-1]\n",
    "    latest_first_div_col = first_div.columns[-1]\n",
    "    latest_second_div_col = second_div.columns[-1]\n",
    "\n",
    "    derivatives = pd.DataFrame(data[latest_year_col])\n",
    "    derivatives.rename(columns={latest_year_col: 'raw_value'}, inplace=True)\n",
    "    derivatives['1st_div'] = first_div[latest_first_div_col] / derivatives['raw_value']\n",
    "    derivatives['2nd_div'] = second_div[latest_second_div_col] / derivatives['raw_value']\n",
    "    \n",
    "    metric_df_reshaped = derivatives.unstack(level='series')\n",
    "    if isinstance(metric_df_reshaped.columns, pd.MultiIndex):\n",
    "         metric_df_final = metric_df_reshaped.swaplevel(0, 1, axis=1)\n",
    "         metric_df_final.sort_index(axis=1, level=0, inplace=True)\n",
    "    else:\n",
    "         metric_df_final = metric_df_reshaped\n",
    "\n",
    "    return metric_df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new country columns\n",
    "indicator_name_map = {\n",
    "    'NY.GDP.PCAP.CD': 'gdp_pcap',\n",
    "    'SP.POP.TOTL': 'population',\n",
    "}\n",
    "\n",
    "continent_map = create_continent_map(standard_names)\n",
    "metric_df = create_metric_maps(standard_names, indicator_name_map.keys(), oldest, latest)\n",
    "\n",
    "metric_suffixes = {\n",
    "    'raw_value': '_value',\n",
    "    '1st_div': '_growth',\n",
    "    '2nd_div': '_acceleration'\n",
    "}\n",
    "\n",
    "continents = list(continent_map.values())\n",
    "# msci_groups = list(msci_map.values())\n",
    "\n",
    "for cont in continents:\n",
    "    filtered_df[f'continent_{cont}'] = 0.0\n",
    "# for group in msci_groups:\n",
    "#     filtered_df[f'msci_{group}'] = 0.0\n",
    "for ind_code, ind_name in indicator_name_map.items():\n",
    "    if ind_code in metric_df.columns.get_level_values(0):\n",
    "        for metric_col, suffix in metric_suffixes.items():\n",
    "            new_col_name = f'{ind_name}{suffix}'\n",
    "            filtered_df[new_col_name] = 0.0\n",
    "\n",
    "for std_name in standard_names:\n",
    "    country_weight_col = f'countries_{std_name}'\n",
    "    if country_weight_col not in filtered_df.columns:\n",
    "        continue\n",
    "    if std_name in continent_map:\n",
    "        continent = continent_map[std_name]\n",
    "        filtered_df[f'continent_{continent}'] += filtered_df[country_weight_col]        \n",
    "    # if std_name in msci_map:\n",
    "    #     market_group = msci_map[std_name]\n",
    "    #     filtered_df[f'msci_{market_group}'] += filtered_df[country_weight_col]\n",
    "\n",
    "    if std_name in metric_df.index:\n",
    "        for ind_code, ind_name in indicator_name_map.items():\n",
    "            if ind_code in metric_df.columns.get_level_values(0):\n",
    "                for metric_col, suffix in metric_suffixes.items():\n",
    "                    value = metric_df.loc[std_name, (ind_code, metric_col)]\n",
    "                    target_col = f'{ind_name}{suffix}'\n",
    "                    filtered_df[target_col] += filtered_df[country_weight_col] * value\n",
    "\n",
    "# Drop single unique value columns\n",
    "numerical_cols = [col for col in filtered_df.columns if filtered_df[col].dtype in [np.int64, np.float64] and col not in ['conId']]\n",
    "single_value_columns = [col for col in numerical_cols if filtered_df[col].nunique() == 1]\n",
    "filtered_df = filtered_df.drop(columns=single_value_columns, errors='ignore')\n",
    "numerical_cols = [col for col in filtered_df.columns if filtered_df[col].dtype in [np.int64, np.float64] and col not in ['conId']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fundamentals compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fundamentals reduced to factor columns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "fundamental_columns = [col for col in filtered_df.columns if col.startswith('fundamentals')]\n",
    "\n",
    "value_columns_inverted = [\n",
    "    'fundamentals_Price/Book',\n",
    "    'fundamentals_Price/Cash',\n",
    "    'fundamentals_Price/Earnings',\n",
    "    'fundamentals_Price/Sales',\n",
    "]\n",
    "\n",
    "leverage_columns_inverted = [\n",
    "    'fundamentals_LTDebt/Shareholders',\n",
    "    'fundamentals_TotalDebt/TotalCapital',\n",
    "    'fundamentals_TotalDebt/TotalEquity',\n",
    "    'fundamentals_TotalAssets/TotalEquity',\n",
    "]\n",
    "\n",
    "profitability_columns = [\n",
    "    'fundamentals_ReturnonAssets1Yr',\n",
    "    'fundamentals_ReturnonAssets3Yr',\n",
    "    'fundamentals_ReturnonCapital',\n",
    "    'fundamentals_ReturnonCapital3Yr',\n",
    "    'fundamentals_ReturnonEquity1Yr',\n",
    "    'fundamentals_ReturnonEquity3Yr',\n",
    "    'fundamentals_ReturnonInvestment1Yr',\n",
    "    'fundamentals_ReturnonInvestment3Yr',\n",
    "    # 'fundamentals_SalestoTotalAssets',\n",
    "    # 'fundamentals_EBITtoInterest',\n",
    "]\n",
    "\n",
    "investment_columns = [\n",
    "    'fundamentals_EPSGrowth-1yr',\n",
    "    'fundamentals_EPS_growth_3yr',\n",
    "    'fundamentals_EPS_growth_5yr',\n",
    "    'fundamentals_EPSGrowth_slope',\n",
    "    'fundamentals_EPSGrowth_second_deriv',\n",
    "    'fundamentals_ReturnonAssets_slope',\n",
    "    'fundamentals_ReturnonCapital_slope',\n",
    "    'fundamentals_ReturnonEquity_slope',\n",
    "    'fundamentals_ReturnonInvestment_slope',\n",
    "]\n",
    "\n",
    "\n",
    "momentum_columns = [\n",
    "    'momentum_3mo',\n",
    "    'momentum_6mo',\n",
    "    'momentum_1y',\n",
    "    'fundamentals_RelativeStrength'\n",
    "]\n",
    "\n",
    "columns_to_scale = value_columns_inverted + leverage_columns_inverted + profitability_columns + investment_columns + momentum_columns\n",
    "\n",
    "if any(x in filtered_df.columns for x in columns_to_scale):\n",
    "    scaler = MinMaxScaler()\n",
    "    filtered_df[columns_to_scale] = scaler.fit_transform(filtered_df[columns_to_scale])\n",
    "\n",
    "    # Value Score\n",
    "    filtered_df['factor_value'] = (1 - filtered_df[value_columns_inverted]).sum(axis=1)\n",
    "    filtered_df['factor_leverage'] = (1 - filtered_df[leverage_columns_inverted]).sum(axis=1)\n",
    "    filtered_df['factor_profitability'] = filtered_df[profitability_columns].sum(axis=1)\n",
    "    # filtered_df['factor_investment'] = filtered_df[investment_columns].sum(axis=1)\n",
    "    filtered_df['factor_momentum'] = filtered_df[momentum_columns].sum(axis=1)\n",
    "\n",
    "    # filtered_df = filtered_df.drop(columns=columns_to_scale, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorganize columns\n",
    "categories = ['factor', 'holding_types', 'stats', 'momentum', 'profile', 'top10', 'population', 'msci', 'gdp', 'continent', 'countries', 'fundamentals', 'industries', 'currencies', 'debtors', 'maturity', 'debt_type', 'lipper', 'dividends', 'marketcap', 'style', 'domicile', 'asset']\n",
    "\n",
    "numerical_cols = [col for col in filtered_df.columns if filtered_df[col].dtype in [np.int64, np.float64] and col not in ['conId']]\n",
    "non_numerical = [col for col in filtered_df.columns if col not in numerical_cols]\n",
    "\n",
    "for category in reversed(categories):\n",
    "    cat_cols = [col for col in numerical_cols if col.startswith(category)]\n",
    "    remaining = [col for col in numerical_cols if col not in cat_cols]\n",
    "    numerical_cols = cat_cols + remaining\n",
    "\n",
    "new_column_order = non_numerical + numerical_cols\n",
    "filtered_df = filtered_df[new_column_order]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_long_short_factor_returns(full_meta_df, returns_df, long_symbols, short_symbols, factor_column=None):\n",
    "    long_df = full_meta_df[full_meta_df['conId'].isin(long_symbols)].set_index('conId')\n",
    "    long_weights = long_df['profile_cap_usd'].reindex(returns_df.columns).fillna(0)\n",
    "    if long_weights.mean() == 0:\n",
    "        print(f'Long {factor_column}')\n",
    "        print(long_df.index)\n",
    "        print()\n",
    "    if factor_column:\n",
    "        factor_weights = (full_meta_df[factor_column].max() - long_df[factor_column]) / (full_meta_df[factor_column].max() - full_meta_df[factor_column].min())\n",
    "        factor_weights = factor_weights.reindex(returns_df.columns).fillna(0)\n",
    "        if factor_weights.sum() != 0:\n",
    "            long_weights *= factor_weights\n",
    "\n",
    "    long_weights /= long_weights.sum()\n",
    "    long_returns = returns_df.dot(long_weights)\n",
    "    \n",
    "    short_df = full_meta_df[full_meta_df['conId'].isin(short_symbols)].set_index('conId')\n",
    "    short_weights = short_df['profile_cap_usd'].reindex(returns_df.columns).fillna(0)\n",
    "    if short_weights.mean() == 0:\n",
    "        print(f'Short {factor_column}')\n",
    "        print(short_df.index)\n",
    "        print()\n",
    "    if factor_column:\n",
    "        factor_weights = (short_df[factor_column] - full_meta_df[factor_column].min()) / (full_meta_df[factor_column].max() - full_meta_df[factor_column].min())\n",
    "        factor_weights = factor_weights.reindex(returns_df.columns).fillna(0)\n",
    "        if factor_weights.sum() != 0:\n",
    "            short_weights *= factor_weights\n",
    "\n",
    "    short_weights /= short_weights.sum()\n",
    "    short_returns = returns_df.dot(short_weights)\n",
    "    \n",
    "    factor_returns = long_returns - short_returns\n",
    "    return factor_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_factors(filtered_df, pct_changes, portfolio_dfs, risk_free_df, scaling_factor=0.5, diffs=None):\n",
    "    differences = []\n",
    "    long = []\n",
    "    short = []\n",
    "    factors = {}\n",
    "    # Market risk premium\n",
    "    factors['factor_market_premium'] = (portfolio_dfs['equity']['pct_change'] - risk_free_df['daily_nominal_rate'])\n",
    "\n",
    "    # SMB_ETF\n",
    "    small_symbols = filtered_df[filtered_df['marketcap_small'] == 1]['conId'].tolist()\n",
    "    large_symbols = filtered_df[filtered_df['marketcap_large'] == 1]['conId'].tolist()\n",
    "\n",
    "    intersection = set(small_symbols) & set(large_symbols)\n",
    "    small_symbols = [s for s in small_symbols if s not in intersection]\n",
    "    large_symbols = [s for s in large_symbols if s not in intersection]\n",
    "    smb_etf = construct_long_short_factor_returns(filtered_df, pct_changes, small_symbols, large_symbols)\n",
    "    factors['factor_smb'] = smb_etf\n",
    "\n",
    "    long.append(len(small_symbols))\n",
    "    short.append(len(large_symbols))\n",
    "    differences.append(np.abs(len(small_symbols) - len(large_symbols)))\n",
    "\n",
    "    # HML_ETF\n",
    "    # value_cols = [col for col in filtered_df.columns if col.startswith('style_') and col.endswith('value')]\n",
    "    # growth_cols = [col for col in filtered_df.columns if col.startswith('style_') and col.endswith('growth')]\n",
    "    # value_symbols = filtered_df[filtered_df[value_cols].ne(0).any(axis=1)]['conId'].tolist()\n",
    "    # growth_symbols = filtered_df[filtered_df[growth_cols].ne(0).any(axis=1)]['conId'].tolist()\n",
    "\n",
    "    # intersection = set(value_symbols) & set(growth_symbols)\n",
    "    # value_symbols = [s for s in value_symbols if s not in intersection]\n",
    "    # growth_symbols = [s for s in growth_symbols if s not in intersection]\n",
    "    # hml_etf = construct_long_short_factor_returns(filtered_df, pct_changes, value_symbols, growth_symbols)\n",
    "    # factors['factor_hml'] = hml_etf\n",
    "\n",
    "    # long.append(len(value_symbols))\n",
    "    # short.append(len(growth_symbols))\n",
    "    # differences.append(np.abs(len(value_symbols) - len(growth_symbols)))\n",
    "\n",
    "    # Metadata\n",
    "    excluded = ['style_', 'marketcap_', 'countries_','momentum_', 'fundamentals_', ]\n",
    "    numerical_cols = [col for col in filtered_df.columns if filtered_df[col].dtype in [np.int64, np.float64] and col not in ['conId']]\n",
    "    for col in numerical_cols:\n",
    "        if not any(col.startswith(prefix) for prefix in excluded) and col in filtered_df.columns:\n",
    "            try:\n",
    "                std = filtered_df[col].std()\n",
    "                mean = filtered_df[col].mean()\n",
    "\n",
    "                upper_boundary = min(filtered_df[col].max(), mean + (scaling_factor * std))\n",
    "                lower_boundary = max(filtered_df[col].min(), mean - (scaling_factor * std))\n",
    "\n",
    "                low_factor_symbols = filtered_df[filtered_df[col] <= lower_boundary]['conId'].tolist()\n",
    "                high_factor_symbols = filtered_df[filtered_df[col] >= upper_boundary]['conId'].tolist()\n",
    "                if col.endswith('variety'):\n",
    "                    var_etf = construct_long_short_factor_returns(filtered_df, pct_changes, low_factor_symbols, high_factor_symbols, factor_column=col)\n",
    "                else:\n",
    "                    var_etf = construct_long_short_factor_returns(filtered_df, pct_changes, high_factor_symbols, low_factor_symbols, factor_column=col)\n",
    "                var_etf = construct_long_short_factor_returns(filtered_df, pct_changes, high_factor_symbols, low_factor_symbols, factor_column=col)\n",
    "                factors[col] = var_etf\n",
    "\n",
    "                long.append(len(low_factor_symbols))\n",
    "                short.append(len(high_factor_symbols))\n",
    "                differences.append(np.abs(len(low_factor_symbols) - len(high_factor_symbols)))\n",
    "            except Exception as e:\n",
    "                print(col)\n",
    "                print(e)\n",
    "                raise\n",
    "        \n",
    "    if diffs:\n",
    "        diffs = {'long': long,\n",
    "                 'short': short,\n",
    "                 'diffs': differences}\n",
    "\n",
    "        return pd.DataFrame(factors), pd.DataFrame(diffs)\n",
    "    return pd.DataFrame(factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prescreen_factors(factors_df, correlation_threshold=0.99, drop_map=None):\n",
    "    if factors_df is None or factors_df.empty or factors_df.shape[1] == 0:\n",
    "        raise ValueError(\"factors_df must be a non-empty DataFrame with at least one column.\")\n",
    "    temp_factors_df = factors_df.copy()\n",
    "\n",
    "    corr_matrix = temp_factors_df.corr().abs()\n",
    "    corr_pairs = corr_matrix.where(np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)).stack()\n",
    "    corr_pairs = corr_pairs.sort_values(ascending=False)\n",
    "\n",
    "    if not drop_map:\n",
    "        drop_map = {}\n",
    "    col_order = list(temp_factors_df.columns)\n",
    "    for (col1, col2), corr_val in corr_pairs.items():\n",
    "        if corr_val < correlation_threshold:\n",
    "            break\n",
    "\n",
    "        already_dropped = {c for drops in drop_map.values() for c in drops}\n",
    "        if col1 in already_dropped or col2 in already_dropped:\n",
    "            continue\n",
    "\n",
    "        if col_order.index(col1) < col_order.index(col2):\n",
    "            keeper, to_drop = col1, col2\n",
    "        else:\n",
    "            keeper, to_drop = col2, col1\n",
    "\n",
    "        drop_map.setdefault(keeper, []).append(to_drop)\n",
    "\n",
    "    cols_to_drop = set(col for drops in drop_map.values() for col in drops)\n",
    "    temp_factors_df = temp_factors_df.drop(columns=cols_to_drop)\n",
    "    return temp_factors_df, drop_map\n",
    "\n",
    "def merge_drop_map(drop_map):\n",
    "    cols_to_drop = set(col for drops in drop_map.values() for col in drops)\n",
    "    final_drop_map = {}\n",
    "    for keeper, direct_drops in drop_map.items():\n",
    "        if keeper not in cols_to_drop:\n",
    "            cols_to_check = list(direct_drops) \n",
    "            all_related_drops = set(direct_drops)\n",
    "            while cols_to_check:\n",
    "                col = cols_to_check.pop(0)\n",
    "                if col in drop_map:\n",
    "                    new_drops = [d for d in drop_map[col] if d not in all_related_drops]\n",
    "                    cols_to_check.extend(new_drops)\n",
    "                    all_related_drops.update(new_drops)\n",
    "            \n",
    "            final_drop_map[keeper] = sorted(list(all_related_drops))\n",
    "    \n",
    "    return final_drop_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLS Regression function\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def run_regressions(distilled_factors):\n",
    "    results = []\n",
    "    for symbol in pct_changes.columns:\n",
    "        etf_excess = pct_changes[symbol] - risk_free_df['daily_nominal_rate']\n",
    "        data = pd.concat([etf_excess.rename('etf_excess'), distilled_factors], axis=1).dropna()\n",
    "\n",
    "        Y = data['etf_excess']\n",
    "        X = sm.add_constant(data.iloc[:, 1:])\n",
    "        model = sm.OLS(Y, X).fit()\n",
    "        result = {\n",
    "            'conId': symbol,\n",
    "            'nobs': model.nobs,\n",
    "            'r_squared': model.rsquared,\n",
    "            'r_squared_adj': model.rsquared_adj,\n",
    "            'f_statistic': model.fvalue,\n",
    "            'f_pvalue': model.f_pvalue,\n",
    "            'aic': model.aic,\n",
    "            'bic': model.bic,\n",
    "            'condition_number': model.condition_number,\n",
    "            'alpha': model.params['const'],\n",
    "            'alpha_pval': model.pvalues['const'],\n",
    "            'alpha_tval': model.tvalues['const'],\n",
    "            'alpha_bse': model.bse['const'],\n",
    "        }\n",
    "        for factor in distilled_factors.columns:\n",
    "            result[f'beta_{factor}'] = model.params[factor]\n",
    "            result[f'pval_beta_{factor}'] = model.pvalues[factor]\n",
    "            result[f'tval_beta_{factor}'] = model.tvalues[factor]\n",
    "            result[f'bse_beta_{factor}'] = model.bse[factor]\n",
    "        results.append(result)\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df\n",
    "    # del X, Y, model, data, etf_excess, result, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Testing correlation thresholds and scaling factors\n",
    "# from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# def calculate_vif(df):\n",
    "#     vif_data = pd.DataFrame()\n",
    "#     vif_data[\"feature\"] = df.columns\n",
    "#     vif_data[\"VIF\"] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]\n",
    "#     return vif_data.sort_values(by='VIF', ascending=False)\n",
    "\n",
    "# # Old greedy iterative VIF pruning\n",
    "# # max_vif_threshold = 99999\n",
    "# # while True:\n",
    "# #     vif_df = calculate_vif(distilled_factors.fillna(0))\n",
    "# #     highest_vif = vif_df['VIF'].iloc[0]\n",
    "# #     if highest_vif > max_vif_threshold and distilled_factors.shape[1] > 2:\n",
    "# #         feature_to_drop = vif_df['feature'].iloc[0]\n",
    "# #         distilled_factors.drop(columns=[feature_to_drop], inplace=True)\n",
    "# #         cols_dropped.add(feature_to_drop)\n",
    "# #         print(f'{feature_to_drop} - {highest_vif}')\n",
    "# #     else:\n",
    "# #         break\n",
    "\n",
    "# if input(\"test corr and scale? (y/n)\").lower() == 'y':\n",
    "#     c_range = np.arange(0.6,1,0.02)\n",
    "#     z_range = np.arange(0,2,0.2)\n",
    "#     results = []\n",
    "#     for c in tqdm(c_range, total=len(c_range)):\n",
    "#         for z in tqdm(z_range, total=len(z_range)):\n",
    "#             factors_df = construct_factors(filtered_df, pct_changes, portfolio_dfs, risk_free_df, scaling_factor=z)\n",
    "#             distilled_factors, _ = prescreen_factors(factors_df, correlation_threshold=c)\n",
    "\n",
    "#             max_vif_threshold = 10\n",
    "#             while True:\n",
    "#                 vif_df = calculate_vif(distilled_factors.fillna(0))\n",
    "#                 highest_vif = vif_df['VIF'].iloc[0]\n",
    "#                 if highest_vif > max_vif_threshold and distilled_factors.shape[1] > 2:\n",
    "#                     feature_to_drop = vif_df['feature'].iloc[0]\n",
    "#                     distilled_factors.drop(columns=[feature_to_drop], inplace=True)\n",
    "#                 else:\n",
    "#                     break\n",
    "\n",
    "#             results_df = run_regressions(distilled_factors)\n",
    "#             results.append((z, c, results_df.condition_number.mean()))\n",
    "\n",
    "#     df_plot = pd.DataFrame(results, columns=['z', 'c', 'vif'])\n",
    "#     df_plot['vif'] = df_plot['vif'].replace([np.inf, -np.inf], np.nan)\n",
    "#     # df_plot['vif'] = df_plot['vif'].fillna(max_finite_vif * 1.5)\n",
    "\n",
    "#     df_pivot = df_plot.pivot(index='c', columns='z', values='vif')\n",
    "\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     sns.heatmap(df_pivot, cmap='viridis', annot=False, cbar_kws={'label': 'Max VIF'})\n",
    "#     plt.xlabel('z-score')\n",
    "#     plt.ylabel('correlation threshold')\n",
    "#     plt.title('VIF Heatmap (Original Grid)')\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct factors\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from IPython.display import Markdown\n",
    "\n",
    "def calculate_vif(df):\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"feature\"] = df.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]\n",
    "    return vif_data.sort_values(by='VIF', ascending=False)\n",
    "\n",
    "factors_df, diffs = construct_factors(filtered_df, pct_changes, portfolio_dfs, risk_free_df, scaling_factor=0.6, diffs=True)\n",
    "\n",
    "# Custom drop\n",
    "low_absolute_beta = ['profile_cap_usd', 'holding_types_equity', 'industries_BasicMaterials', 'continent_Oceania_beta', 'holding_types_bond_beta']#, 'factor_smb_beta']\n",
    "frequently_lassoed = ['gdp_pcap_growth', 'gdp_pcap_acceleration', 'continent_Africa', 'population_value', 'industries_Financials_beta', 'stats_sharpe']\n",
    "walk_forward = ['industries_Healthcare_beta', 'continent_America_beta']#, 'stats_sharpe', 'factor_momentum_beta', 'factor_profitability_beta']\n",
    "custom_drop = low_absolute_beta + frequently_lassoed + walk_forward\n",
    "custom_drop = [c.split('_beta')[0] for c in custom_drop]\n",
    "factors_df = factors_df.drop(columns=custom_drop, errors='ignore')\n",
    "\n",
    "# Screen factors\n",
    "distilled_factors, drop_map = prescreen_factors(factors_df, correlation_threshold=0.95)\n",
    "\n",
    "# corr_matrix = distilled_factors.corr()\n",
    "# vif_df = calculate_vif(distilled_factors.dropna(axis=0))\n",
    "# highest_vif = vif_df['VIF'].iloc[0]\n",
    "# if distilled_factors.shape[1] > 2:\n",
    "#     to_drop = vif_df['feature'].iloc[0]\n",
    "#     distilled_factors.drop(columns=[to_drop], inplace=True)\n",
    "\n",
    "# np.fill_diagonal(corr_matrix.values, 0)\n",
    "# keeper = corr_matrix[to_drop].sort_values(ascending=False).index[0]\n",
    "# drop_map.setdefault(keeper, []).append(to_drop)\n",
    "\n",
    "drop_map = merge_drop_map(drop_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Final factors\n",
    "if drop_map:\n",
    "    display(pd.Series(drop_map))\n",
    "print(distilled_factors.shape)\n",
    "\n",
    "display(Markdown('## Factors included:'))\n",
    "for cat in categories:\n",
    "    cat_list = [col.split(cat)[-1].strip('_').capitalize() for col in distilled_factors.columns if col.startswith(cat)]\n",
    "    if cat_list:\n",
    "        print(f'{(',  ').join(cat_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_vif(distilled_factors.dropna(axis=0).drop(columns='industries_Technology'))\n",
    "# calculate_vif(distilled_factors.dropna(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = distilled_factors.corr().abs()\n",
    "corr_pairs = corr_matrix.where(np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)).stack()\n",
    "corr_pairs.sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ElasticNet regression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "def run_elastic_net(factors_df,\n",
    "                    pct_changes,\n",
    "                    risk_free_df,\n",
    "                    training_cutoff,\n",
    "                    alphas=np.logspace(-4, 1, 50),\n",
    "                    l1_ratio=[.1, .5, .9],\n",
    "                    cv=5,\n",
    "                    tol=5e-4,\n",
    "                    random_state=42):\n",
    "\n",
    "    data = data = (\n",
    "        factors_df.copy()\n",
    "        .join(pct_changes, how='inner')\n",
    "        .join(risk_free_df[['daily_nominal_rate']], how='inner')\n",
    "        .fillna(0)\n",
    "    )\n",
    "\n",
    "    train = data[data.index < training_cutoff]\n",
    "    test = data[data.index >= training_cutoff]\n",
    "\n",
    "    X_train = train[factors_df.columns].values\n",
    "    X_test = test[factors_df.columns].values\n",
    "    \n",
    "    metrics = []\n",
    "    for etf in tqdm(pct_changes.columns, total=len(pct_changes.columns), desc=\"Elastic Net Regression\"):\n",
    "        Y_train = train[etf].values - train['daily_nominal_rate'].values\n",
    "        Y_test = test[etf].values - test['daily_nominal_rate'].values\n",
    "\n",
    "        pipeline = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('enet', ElasticNetCV(alphas=alphas,\n",
    "                                l1_ratio=l1_ratio,\n",
    "                                cv=cv,\n",
    "                                random_state=random_state,\n",
    "                                max_iter=499999,\n",
    "                                tol=tol,\n",
    "                                fit_intercept=True,\n",
    "                                n_jobs=-1)),\n",
    "        ])\n",
    "\n",
    "        try:\n",
    "            pipeline.fit(X_train, Y_train)\n",
    "        except ValueError as e:\n",
    "            print(f\"Skipping {etf} due to error: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Unscale coefficients and intercept\n",
    "        enet = pipeline.named_steps['enet']\n",
    "        scaler = pipeline.named_steps['scaler']\n",
    "        betas_train = enet.coef_ / scaler.scale_\n",
    "        intercept = enet.intercept_ - np.dot(betas_train, scaler.mean_)\n",
    "\n",
    "        # out-of-sample stats\n",
    "        er_test = pipeline.predict(X_test)\n",
    "\n",
    "        # in-sample stats\n",
    "        er_train = pipeline.predict(X_train)\n",
    "\n",
    "        row = {\n",
    "            'conId': etf,\n",
    "            'jensens_alpha': intercept,\n",
    "            'enet_alpha': enet.alpha_,\n",
    "            'l1_ratio': enet.l1_ratio_,\n",
    "            'n_iter': enet.n_iter_,\n",
    "            'dual_gap': enet.dual_gap_,\n",
    "            'n_nonzero': np.sum(np.abs(betas_train) > 1e-6),\n",
    "            # 'mse_path_grid': enet.mse_path_,\n",
    "            'cv_mse_best': np.min(enet.mse_path_.mean(axis=2)),\n",
    "            'cv_mse_average': np.mean(enet.mse_path_.mean(axis=2)),\n",
    "            'cv_mse_worst': np.max(enet.mse_path_.mean(axis=2)),\n",
    "            'mse_test' : mean_squared_error(Y_test, er_test),\n",
    "            'mse_train' : mean_squared_error(Y_train, er_train),\n",
    "            'r2_test' : r2_score(Y_test, er_test),\n",
    "            'r2_train' : r2_score(Y_train, er_train),\n",
    "        }\n",
    "\n",
    "        # Map back coefficients to factor names\n",
    "        for coef, fname in zip(betas_train, factors_df.columns):\n",
    "            row[f'{fname}_beta'] = coef\n",
    "\n",
    "        metrics.append(row)\n",
    "    \n",
    "    results_df = pd.DataFrame(metrics).set_index('conId')\n",
    "    return results_df\n",
    "\n",
    "results_df = run_elastic_net(\n",
    "    factors_df=distilled_factors,\n",
    "    pct_changes=pct_changes,\n",
    "    risk_free_df=risk_free_df,\n",
    "    training_cutoff=training_cutoff,\n",
    "    alphas=np.logspace(-11, -4, 30),\n",
    "    l1_ratio=[0.01, 0.05, 0.1, 0.3, 0.5, 0.7, 0.9, 0.95, 1],\n",
    "    cv=5,\n",
    "    tol=5e-4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.stats import skew\n",
    "# from scipy.optimize import minimize_scalar\n",
    "\n",
    "# def optimize_scalar(series):\n",
    "#     def obj(s):\n",
    "#         if s <= 0:\n",
    "#             return np.inf\n",
    "#         return skew(np.log1p(s * series))**2\n",
    "\n",
    "#     result = minimize_scalar(obj, bounds=(1e-5, 1e20), method='bounded')\n",
    "#     print(result.x)\n",
    "#     return result.x\n",
    "\n",
    "# testing = load('data/walk_forward_results.csv')\n",
    "# testing\n",
    "# # walk_forward = ['factor_momentum_beta', 'industries_Healthcare_beta', 'continent_America_beta', 'factor_profitability_beta']\n",
    "# # testing = testing.drop(columns= walk_forward, errors='ignore')\n",
    "# beta_cols = [col for col in testing if col.endswith('beta')]\n",
    "\n",
    "# testing['r2_test_adj'] = testing['r2_test'].max() - testing['r2_test']\n",
    "# testing['r2_test_adj'] = np.log1p(testing['r2_test_adj'] * optimize_scalar(testing['r2_test_adj']))\n",
    "# testing['r2_test_adj'] = testing['r2_test_adj'].max() - testing['r2_test_adj']\n",
    "\n",
    "# testing['cv_mse_std'] = testing[['cv_mse_best','cv_mse_average','cv_mse_worst']].std(axis=1)\n",
    "# testing['cv_mse_std'] = np.log1p(testing['cv_mse_std'] * optimize_scalar(testing['cv_mse_std']))\n",
    "\n",
    "# testing['screening_score'] = testing['r2_test_adj'] / (1 + testing['cv_mse_std'])\n",
    "\n",
    "# temp = pd.DataFrame()\n",
    "# temp['r2_adj'] = testing[beta_cols].abs().multiply(testing['screening_score'], axis=0).mean()\n",
    "# temp['count'] = (testing[beta_cols].abs() > 1e-6).sum()\n",
    "# temp['count'] = (temp['count'] - temp['count'].min()) / (temp['count'].max() - temp['count'].min())\n",
    "# temp['mean'] = temp.mean(axis=1)\n",
    "# temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factor beta post-screening score \n",
    "from scipy.stats import skew\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "def optimize_scalar(series):\n",
    "    def obj(s):\n",
    "        if s <= 0:\n",
    "            return np.inf\n",
    "        return skew(np.log1p(s * series))**2\n",
    "\n",
    "    result = minimize_scalar(obj, bounds=(1e-5, 1e20), method='bounded')\n",
    "    print(result.x)\n",
    "    return result.x\n",
    "\n",
    "beta_cols = [col for col in results_df if col.endswith('beta')]\n",
    "screening_df = pd.DataFrame(index=results_df.index)\n",
    "\n",
    "screening_df['r2_test'] = results_df['r2_test'].max() - results_df['r2_test']\n",
    "screening_df['r2_test'] = np.log1p(screening_df['r2_test'] * optimize_scalar(screening_df['r2_test']))\n",
    "screening_df['r2_test'] = screening_df['r2_test'].max() - screening_df['r2_test']\n",
    "\n",
    "screening_df['cv_mse_std'] = results_df[['cv_mse_best','cv_mse_average','cv_mse_worst']].std(axis=1)\n",
    "screening_df['cv_mse_std'] = np.log1p(screening_df['cv_mse_std'] * optimize_scalar(screening_df['cv_mse_std']))\n",
    "\n",
    "screening_df['screening_score'] = screening_df['r2_test'] / (1 + screening_df['cv_mse_std'])\n",
    "\n",
    "screening_df = results_df[beta_cols].abs().multiply(screening_df['screening_score'], axis=0).mean()\n",
    "# screening_df[screening_df < screening_df.mean() - screening_df.std()]#.sort_values()\n",
    "screening_df.sort_values()\n",
    "\n",
    "# 0.01 - 0.05\n",
    "# .18 - .2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso count post-screening\n",
    "beta_cols = [col for col in results_df if col.endswith('beta')]\n",
    "good_factor_importance3 = results_df[results_df['r2_test'] > 0.05]\n",
    "\n",
    "if not good_factor_importance3.empty:\n",
    "    good_factor_importance3 = good_factor_importance3[beta_cols]\n",
    "    non_zero = (good_factor_importance3.abs() > 1e-6).sum()\n",
    "    good_factor_importance3 = pd.DataFrame({\n",
    "        'non_zero_percentage': non_zero / len(results_df),\n",
    "        'mean_abs_beta': good_factor_importance3.abs().mean()\n",
    "    }).sort_values('mean_abs_beta', ascending=True)\n",
    "\n",
    "    good_factor_importance3['mean'] = good_factor_importance3.mean(axis=1)\n",
    "    display(good_factor_importance3.sort_values(by='non_zero_percentage'))\n",
    "\n",
    "    # 0.6 - 0.64\n",
    "    # .86 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso count post-screening\n",
    "beta_cols = [col for col in results_df if col.endswith('beta')]\n",
    "# good_factor_importance = results_df[results_df['r2_test'] > 0.05]\n",
    "\n",
    "# if not good_factor_importance.empty:\n",
    "#     # good_factor_importance = good_factor_importance[beta_cols]\n",
    "#     non_zero = (good_factor_importance.abs() > 1e-6).sum()\n",
    "#     good_factor_importance = pd.DataFrame({\n",
    "#         'non_zero_percentage': non_zero / len(results_df),\n",
    "#         'mean_abs_beta': good_factor_importance.abs().mean()\n",
    "#     }).sort_values('mean_abs_beta', ascending=True)\n",
    "\n",
    "    # good_factor_importance['mean'] = good_factor_importance.mean(axis=1)\n",
    "display(good_factor_importance.sort_values(by='non_zero_percentage'))\n",
    "\n",
    "    # 0.6 - 0.64\n",
    "    # .86 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modern portfolio theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factor-based ER\n",
    "beta_cols = [col for col in results_df.columns if col.endswith('beta')]\n",
    "asset_betas = results_df[beta_cols]\n",
    "asset_betas.columns = [col.replace('_beta', '') for col in beta_cols]\n",
    "asset_betas = asset_betas[distilled_factors.columns]\n",
    "\n",
    "factor_premia = distilled_factors.mean()\n",
    "factor_premia[factor_premia > 0] = 0\n",
    "factor_premia *= -1\n",
    "\n",
    "systematic_returns = asset_betas.dot(factor_premia)\n",
    "factor_based_er = results_df['jensens_alpha'] + systematic_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factor-based + model-adjusted ER - mu_utility\n",
    "def optimize_scalar(series):\n",
    "    def obj(s):\n",
    "        if s <= 0:\n",
    "            return np.inf\n",
    "        return skew(np.log1p(s * series))**2\n",
    "\n",
    "    result = minimize_scalar(obj, bounds=(1e-5, 1e20), method='bounded')\n",
    "    print(result.x)\n",
    "    return result.x\n",
    "\n",
    "screening_df = pd.DataFrame(index=results_df.index)\n",
    "screening_df['expected_return'] = factor_based_er\n",
    "screening_df['expected_return'] -= screening_df['expected_return'].min()\n",
    "\n",
    "screening_df['r2_test'] = results_df['r2_test'].max() - results_df['r2_test']\n",
    "screening_df['r2_test'] = np.log1p(screening_df['r2_test'] * optimize_scalar(screening_df['r2_test']))\n",
    "screening_df['r2_test'] = screening_df['r2_test'].max() - screening_df['r2_test']\n",
    "\n",
    "screening_df['cv_mse_std'] = results_df[['cv_mse_best','cv_mse_average','cv_mse_worst']].std(axis=1)\n",
    "screening_df['cv_mse_std'] = np.log1p(screening_df['cv_mse_std'] * optimize_scalar(screening_df['cv_mse_std']))\n",
    "screening_df['cv_mse_std'] = screening_df['cv_mse_std'].max() - screening_df['cv_mse_std']\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "screening_df[['r2_test', 'cv_mse_std']] = scaler.fit_transform(screening_df[['r2_test', 'cv_mse_std']])\n",
    "screening_df['r2_adjusted_er'] = screening_df['expected_return'] * screening_df['r2_test'] * screening_df['cv_mse_std']\n",
    "screening_df['historical_er'] = pct_changes.mean()\n",
    "\n",
    "mu_utility = screening_df['r2_adjusted_er'] \n",
    "mu_historical = pct_changes.mean()\n",
    "\n",
    "screening_df.corr().sort_values(by='historical_er')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COV - S\n",
    "factor_cov_matrix = distilled_factors.cov()\n",
    "idiosyncratic_variances = results_df['mse_train']\n",
    "D = np.diag(results_df['mse_train'])\n",
    "\n",
    "systematic_cov = asset_betas.values @ factor_cov_matrix.values @ asset_betas.values.T\n",
    "S = pd.DataFrame(\n",
    "    systematic_cov + D,\n",
    "    index=results_df.index,\n",
    "    columns=results_df.index\n",
    ")\n",
    "\n",
    "# Risk-free rate \n",
    "rf_rate = risk_free_df['daily_nominal_rate'].iloc[-10:].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Portfolio Mean-Variance + Factor Exposure Balance Optimization\n",
    "import cvxpy as cp\n",
    "from pypfopt import base_optimizer\n",
    "\n",
    "def portfolio_factor_dispersion(w, asset_betas):\n",
    "    portfolio_betas = w @ asset_betas\n",
    "    n_factors = asset_betas.shape[1]\n",
    "    demeaned_betas = portfolio_betas - (cp.sum(portfolio_betas) / n_factors)\n",
    "    return cp.norm(demeaned_betas, 2)\n",
    "\n",
    "def optimize_with_factor_balance(mu, S, asset_betas, cuttoff_threshold, upper_bounds, solver='CLARABEL'):\n",
    "    n_assets = len(mu)\n",
    "    w = cp.Variable(n_assets)\n",
    "\n",
    "    expected_return = mu.values @ w\n",
    "    portfolio_risk = cp.quad_form(w, S)\n",
    "    factor_dispersion = portfolio_factor_dispersion(w, asset_betas.values)\n",
    "    \n",
    "    objective = cp.Maximize(\n",
    "        expected_return\n",
    "        - portfolio_risk \n",
    "        - factor_dispersion\n",
    "    )\n",
    "    \n",
    "    constraints = [cp.sum(w) == 1, w >= 0, w <= upper_bounds]\n",
    "\n",
    "    problem = cp.Problem(objective, constraints)\n",
    "    problem.solve(solver=solver)\n",
    "\n",
    "    if problem.status not in [\"optimal\", \"optimal_inaccurate\"]:\n",
    "        print(f\"Warning: Optimal solution not found. Status: {problem.status}\")\n",
    "        return None\n",
    "\n",
    "    weights = pd.Series(w.value, index=mu.index)\n",
    "    weights[np.abs(weights) < cuttoff_threshold] = 0\n",
    "    weights /= weights.sum()\n",
    "    \n",
    "    return weights\n",
    "\n",
    "\n",
    "# Higher value = more penalty for that term.\n",
    "cuttoff_threshold = .05\n",
    "upper_bounds = 1\n",
    "\n",
    "weights = optimize_with_factor_balance(\n",
    "    mu=mu_utility,\n",
    "    S=S,\n",
    "    asset_betas=asset_betas,\n",
    "    cuttoff_threshold=cuttoff_threshold,\n",
    "    upper_bounds=upper_bounds,\n",
    ")\n",
    "\n",
    "df = weights[weights > 0].sort_values(ascending=False)\n",
    "for k,v in df.items():\n",
    "    row = filtered_df[filtered_df['conId'] == k]\n",
    "    symbol = row['symbol'].iloc[0]\n",
    "    print(f'{symbol}: {round(v*100, 2)}%')\n",
    "\n",
    "\n",
    "rf_rate = risk_free_df['daily_nominal_rate'].iloc[-10:].mean()\n",
    "expected_return, volatility, sharpe_ratio = base_optimizer.portfolio_performance(weights, mu_utility, S, risk_free_rate=rf_rate)\n",
    "\n",
    "print(f'num_etfs: {len(weights[weights > 0])}')\n",
    "final_portfolio_betas = weights @ asset_betas\n",
    "print(f'beta_std: {final_portfolio_betas.std()}')\n",
    "print(f'volatility: {volatility}')\n",
    "print(f'sharpe: {sharpe_ratio}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian optimization of hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import gp_minimize\n",
    "from skopt.space import Real\n",
    "from skopt.utils import use_named_args\n",
    "\n",
    "\n",
    "def progress_callback(res):\n",
    "    pbar.update(1)\n",
    "\n",
    "space  = [\n",
    "    Real(0, .05, prior='uniform', name='cuttoff_threshold'),\n",
    "    Real(.2, 1, prior='uniform', name='upper_bounds')\n",
    "    ]\n",
    "@use_named_args(space)\n",
    "def objective(**params):\n",
    "    weights = optimize_with_factor_balance(\n",
    "        mu=mu_utility,\n",
    "        S=S,\n",
    "        asset_betas=asset_betas,\n",
    "        cuttoff_threshold=params['cuttoff_threshold'],\n",
    "        upper_bounds=params['upper_bounds'],\n",
    "    )\n",
    "    if weights.isna().sum():\n",
    "        score = 100 / params['upper_bounds'] - params['cuttoff_threshold']\n",
    "        print(f'Empty: {round(params['cuttoff_threshold'], 3)} - {round(params['upper_bounds'], 3)}  Score: {score}')\n",
    "        return score\n",
    "\n",
    "    num_assets = len(weights[weights > 0])\n",
    "    final_portfolio_betas = weights @ asset_betas\n",
    "\n",
    "    er_model, std_model, sharpe_model = base_optimizer.portfolio_performance(weights, mu_utility, S, risk_free_rate=rf_rate)\n",
    "    er_hist, _, sharpe_hist = base_optimizer.portfolio_performance(weights, mu_historical, S, risk_free_rate=rf_rate)\n",
    "    score = (sharpe_model) / np.sqrt(num_assets)\n",
    "\n",
    "    evaluated_results.append({\n",
    "        'cuttoff_threshold': params['cuttoff_threshold'],\n",
    "        'upper_bounds': params['upper_bounds'],\n",
    "        'score': score,\n",
    "        'sharpe_model': sharpe_model,\n",
    "        'sharpe_hist': sharpe_hist,\n",
    "        'er_model': er_model,\n",
    "        'er_hist': er_hist,\n",
    "        'volatility': std_model,\n",
    "        'factor_beta_std': final_portfolio_betas.std(),\n",
    "        'num_assets': num_assets,\n",
    "        'weights': weights[weights > 0],\n",
    "    })\n",
    "\n",
    "    return -score\n",
    "\n",
    "\n",
    "evaluated_results = []\n",
    "n_calls = 32\n",
    "n_initial_points = n_calls//2\n",
    "n_initial_points = 2**4\n",
    "\n",
    "n_calls = n_initial_points # Delete later\n",
    "if 'pbar' in globals():\n",
    "    pbar.close()\n",
    "pbar = tqdm(total=n_calls)\n",
    "\n",
    "result = gp_minimize(\n",
    "    func=objective,\n",
    "    dimensions=space,\n",
    "    n_calls=n_calls,\n",
    "    # initial_point_generator='lhs',\n",
    "    initial_point_generator='sobol',\n",
    "    n_initial_points=n_initial_points,\n",
    "    random_state=42,\n",
    "    callback=[progress_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot parameter exploration 2d\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "eval_df = pd.DataFrame(evaluated_results)\n",
    "# eval_df['score'] = (eval_df['sharpe_model'] - eval_df['factor_beta_std']) / np.sqrt(eval_df['num_assets'])\n",
    "\n",
    "x = 'cuttoff_threshold'\n",
    "y = 'score'\n",
    "\n",
    "plt.scatter(eval_df[x], eval_df[y], c=eval_df['sharpe_model'], cmap='viridis')\n",
    "# plt.scatter(optimal_dict[x], optimal_dict[y], color='red', marker='D')\n",
    "plt.xlabel(x)\n",
    "plt.ylabel(y)\n",
    "plt.colorbar()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 3D\n",
    "import plotly.express as px\n",
    "\n",
    "def to_text(df):\n",
    "    lines = []\n",
    "    for k,v in df.items():\n",
    "        row = filtered_df[filtered_df['conId'] == k]\n",
    "        symbol = row['symbol'].iloc[0]\n",
    "        lines.append(f'{symbol}: {round(v*100, 2)}%')\n",
    "    return '<br>'.join(lines)\n",
    "\n",
    "eval_df = pd.DataFrame(evaluated_results)\n",
    "eval_df['weights'] = eval_df['weights'].apply(lambda x: to_text(x.sort_values(ascending=False)))\n",
    "\n",
    "fig = px.scatter_3d(\n",
    "    eval_df,\n",
    "    x='upper_bounds',\n",
    "    y='cuttoff_threshold',\n",
    "    z='score',\n",
    "    color='sharpe_model',\n",
    "    size='num_assets',\n",
    "    color_continuous_scale=px.colors.sequential.Viridis,\n",
    "    hover_data={\n",
    "        'score': True,\n",
    "        'num_assets': True,\n",
    "        'sharpe_model': True,\n",
    "        'factor_beta_std': True,\n",
    "        'cuttoff_threshold': True,\n",
    "        'weights': True,\n",
    "        }\n",
    ")\n",
    "fig.update_layout(height=800)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the heatmap\n",
    "eval_df = pd.DataFrame(evaluated_results)\n",
    "eval_corr = eval_df.drop('weights', axis=1).corr()\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(eval_corr, annot=True, cmap='coolwarm', center=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypfopt import DiscreteAllocation\n",
    "latest_prices = ... # Get the most recent prices for your ETFs\n",
    "da = DiscreteAllocation(cleaned_weights, latest_prices, total_portfolio_value=25000)\n",
    "allocation, leftover = da.lp_portfolio()\n",
    "print(\"Discrete allocation:\", allocation)\n",
    "print(f\"Funds remaining: ${leftover:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
