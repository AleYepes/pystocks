{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ib_async import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from scipy.spatial.distance import squareform\n",
    "import dcor\n",
    "import gc\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "from time import sleep\n",
    "import csv\n",
    "from scipy.optimize import minimize\n",
    "from fredapi import Fred\n",
    "import pandas_datareader.data as web\n",
    "import math\n",
    "import re\n",
    "import ast\n",
    "\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kind = 'midpoint'\n",
    "kind = 'trades'\n",
    "# kind = 'indices'\n",
    "\n",
    "if kind == 'midpoint':\n",
    "    root = 'data/daily-midpoint/'\n",
    "elif kind == 'trades':\n",
    "    root = 'data/daily-trades/'\n",
    "elif kind == 'indices':\n",
    "    root = 'data/indices/'\n",
    "\n",
    "data_path = root + 'series/'\n",
    "verified_path = root + 'verified_files.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to ibkr\n",
    "util.startLoop()\n",
    "\n",
    "ib = IB()\n",
    "ib.connect('127.0.0.1', 7497, clientId=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_historical(symbol, exchange, currency, duration='40 Y', kind=None):\n",
    "    contract = Stock(symbol, exchange, currency)\n",
    "    if kind == 'midpoint':\n",
    "        data = ib.reqHistoricalData(\n",
    "            contract, \n",
    "            endDateTime='',\n",
    "            durationStr=duration,\n",
    "            barSizeSetting='1 day', \n",
    "            whatToShow='MIDPOINT', \n",
    "            useRTH=True,\n",
    "        )\n",
    "    elif kind == 'trades' or kind == 'indices':\n",
    "        data = ib.reqHistoricalData(\n",
    "            contract, \n",
    "            endDateTime='',\n",
    "            durationStr=duration,\n",
    "            barSizeSetting='1 day', \n",
    "            whatToShow='TRADES', \n",
    "            useRTH=True,\n",
    "        )\n",
    "    length = len(data) - 1 if data and exchange == 'SMART' else len(data)\n",
    "    return data, length, exchange\n",
    "\n",
    "def save_data(data_path, data, symbol, exchange, currency):\n",
    "    if data:\n",
    "        data_df = util.df(data)\n",
    "        data_df['date'] = pd.to_datetime(data_df['date'])\n",
    "        data_df = data_df.sort_values(by='date').reset_index(drop=True)\n",
    "        data_df.to_csv(f'{data_path}{symbol}-{exchange}-{currency}.csv', index=False)\n",
    "        # print(f'{symbol} saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get missing historical series\n",
    "if kind == 'indices':\n",
    "    raise Exception('Incorrect kind for this. Needs to be trades or midpoint)')\n",
    "\n",
    "years = ['40 Y', '20 Y', '10 Y', '5 Y', '3 Y', '2 Y', '1 Y']\n",
    "# years = ['1 Y']\n",
    "for duration in years:\n",
    "    contracts_df = pd.read_csv('data/contract_elaborated.csv')\n",
    "    contracts_df['search_exchange'] = contracts_df['search_exchange'].str.extract(r'\\((.*?)\\)').fillna('')\n",
    "    file_list = os.listdir(data_path)\n",
    "    file_list = [name.split('-')[0] for name in file_list]\n",
    "\n",
    "    missing_symbols = contracts_df[~contracts_df['symbol'].isin(file_list)].copy()\n",
    "    count = 0\n",
    "    for _, row in tqdm(missing_symbols.iterrows(), total=len(missing_symbols), desc=f\"Getting {duration} series\"):\n",
    "        symbol = row['symbol']\n",
    "        search_exchange = row['search_exchange']\n",
    "        suggested_exchange = row['exchange']\n",
    "        primary_exchange = row['primaryExchange']\n",
    "        currency = row['currency']\n",
    "        \n",
    "        results = []\n",
    "        if search_exchange:\n",
    "            results.append(get_historical(symbol, search_exchange, currency, duration=duration, kind=kind))\n",
    "            if suggested_exchange != search_exchange:\n",
    "                results.append(get_historical(symbol, suggested_exchange, currency, duration=duration, kind=kind))\n",
    "            if primary_exchange != suggested_exchange and primary_exchange != search_exchange:\n",
    "                results.append(get_historical(symbol, primary_exchange, currency, duration=duration, kind=kind))\n",
    "        else:\n",
    "            results.append(get_historical(symbol, suggested_exchange, currency, duration=duration, kind=kind))\n",
    "            if primary_exchange != suggested_exchange:\n",
    "                results.append(get_historical(symbol, primary_exchange, currency, duration=duration, kind=kind))\n",
    "        results.append(get_historical(symbol, 'SMART', currency, duration=duration, kind=kind))\n",
    "        results = sorted(results, key=lambda x: x[1], reverse=True)\n",
    "        if results[0][1]:\n",
    "            save_data(data_path, results[0][0], symbol, results[0][2], currency)\n",
    "            count +=1\n",
    "\n",
    "    print(f'{duration}: {count} scraped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check scraping differences\n",
    "# def get_csv_lengths(directory):\n",
    "#     \"\"\"Returns a dictionary mapping (symbol, exchange, currency) to their row counts.\"\"\"\n",
    "#     csv_lengths = {}\n",
    "#     for file in os.listdir(directory):\n",
    "#         if file.endswith(\".csv\"):\n",
    "#             file_path = os.path.join(directory, file)\n",
    "#             try:\n",
    "#                 df = pd.read_csv(file_path)\n",
    "#                 file_key = os.path.splitext(file)[0]  # Remove .csv extension\n",
    "#                 parts = file_key.split('-')\n",
    "#                 if len(parts) == 3:\n",
    "#                     symbol, exchange, currency = parts\n",
    "#                     csv_lengths[(symbol, exchange, currency)] = len(df)\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error reading {file_path}: {e}\")\n",
    "#                 csv_lengths[(symbol, exchange, currency)] = None\n",
    "#     return csv_lengths\n",
    "\n",
    "# def main(dir1, dir2, dir3):\n",
    "#     \"\"\"Generates a DataFrame with symbol, exchange, currency, and row counts from three directories.\"\"\"\n",
    "#     lengths1 = get_csv_lengths(dir1)\n",
    "#     lengths2 = get_csv_lengths(dir2)\n",
    "#     lengths3 = get_csv_lengths(dir3)\n",
    "    \n",
    "#     # Collect all unique (symbol, exchange, currency) combinations\n",
    "#     all_keys = set(lengths1.keys()) | set(lengths2.keys()) | set(lengths3.keys())\n",
    "    \n",
    "#     data = []\n",
    "#     for key in sorted(all_keys):\n",
    "#         symbol, exchange, currency = key\n",
    "#         data.append([symbol, exchange, currency, lengths1.get(key, 'N/A'), lengths2.get(key, 'N/A'), lengths3.get(key, 'N/A')])\n",
    "    \n",
    "#     return pd.DataFrame(data, columns=[\"symbol\", \"exchange\", \"currency\", dir1, dir2, dir3])\n",
    "\n",
    "# # Example usage\n",
    "# dir1 = \"data/indices/series/\"\n",
    "# dir2 = \"data/daily-midpoint/series/\"\n",
    "# dir3 = \"data/daily-trades/series/\"\n",
    "# df = main(dir1, dir2, dir3)\n",
    "# # df[df.duplicated(subset='symbol', keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get indices\n",
    "# '''\n",
    "# SPY - SnP 500 -- US centric\n",
    "# VTI - Vanguard Total Stock Market -- US centric\n",
    "# VEU - Vanguard All-World Ex-US -- Global\n",
    "# VXUS - Vanguard Total International -- Global\n",
    "# BND - Vanguard Total Bond -- US\n",
    "# BNDX - Vanguard Total International Bond -- Global\n",
    "# '''\n",
    "# indices = [Stock('SPY', 'SMART', 'USD'), Stock('VTI', 'SMART', 'USD'), Stock('VEU', 'SMART', 'USD'), Stock('VXUS', 'SMART', 'USD'), Stock('BND', 'SMART', 'USD'), Stock('BNDX', 'SMART', 'USD')]\n",
    "# index_path = 'data/indices/series/'\n",
    "# for contract in tqdm(indices, total=len(indices), desc=f\"Getting index series\"):\n",
    "#     data,_,_ = get_historical(data, contract.symbol, contract.exchange, contract.currency, kind=kind)\n",
    "#     save_data(index_path, data, contract.symbol, contract.exchange, contract.currency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update historical series\n",
    "file_list = os.listdir(data_path)\n",
    "\n",
    "for file_name in tqdm(file_list, total=len(file_list), desc=f\"Updating {data_path}\"):\n",
    "    symbol, exchange, currency = file_name.replace('.csv', '').split('-')\n",
    "    \n",
    "    file_path = os.path.join(data_path, file_name)\n",
    "    data_df = pd.read_csv(file_path)\n",
    "    data_df['date'] = pd.to_datetime(data_df['date'])\n",
    "    last_date = data_df['date'].max()\n",
    "    time_missing = (datetime.now() - last_date).days\n",
    "    if time_missing > 364:\n",
    "        time_missing = math.ceil(time_missing / 364)\n",
    "        duration = f'{time_missing} Y'\n",
    "    else:\n",
    "        duration = f'{time_missing} D'\n",
    "    \n",
    "    if time_missing:\n",
    "        new_data,_,_ = get_historical(symbol, exchange, currency, duration=duration, kind=kind)\n",
    "        if new_data:\n",
    "            new_data_df = util.df(new_data)\n",
    "            new_data_df['date'] = pd.to_datetime(new_data_df['date'])\n",
    "            updated_data_df = pd.concat([new_data_df, data_df]).drop_duplicates(subset='date').sort_values(by='date').reset_index(drop=True)\n",
    "            updated_data_df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Prep indices\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare indices\n",
    "def melt(data_df):\n",
    "    value_columns = ['open', 'close']\n",
    "    id_columns = [col for col in data_df.columns.to_list() if col not in value_columns]\n",
    "    melted_df = data_df.melt(id_vars=id_columns, value_vars=value_columns, var_name='kind', value_name='value')\n",
    "    return melted_df.sort_values(by=['date', 'kind'], ascending=[True, False]).reset_index(drop=True)\n",
    "\n",
    "# Load indices and merge them all into one df\n",
    "indices = {}\n",
    "file_list = os.listdir('data/indices/series/')\n",
    "for file in file_list:\n",
    "    symbol = os.path.splitext(file)[0].split('-')[0]\n",
    "    indices[symbol] = pd.read_csv('data/indices/series/' + file)\n",
    "\n",
    "# Melt indices, filters, and calc pct_change. ASSUMES that indices are sorted chronologically\n",
    "training_start_date = pd.to_datetime('2020-02-01')\n",
    "month_ago = datetime.today() - timedelta(days=31)\n",
    "\n",
    "day_gap = 6 # SET ACCEPTABLE DAY GAP\n",
    "\n",
    "melted_indices, index_returns = [], {}\n",
    "for symbol, df in tqdm(indices.items(), total=len(indices), desc=f'Melting and filtering {kind} indices'):\n",
    "    df = melt(df)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "    latest_date = df['date'].iloc[-1]\n",
    "    earliest_date = df['date'].iloc[0]\n",
    "    dates = df['date'].unique()\n",
    "    date_gaps = dates[1:] - dates[:-1]\n",
    "    df['symbol'] = symbol\n",
    "    df['pct_change'] = df['value'].pct_change()\n",
    "    index_returns[symbol] = df['pct_change'].mean()\n",
    "    melted_indices.append(df)\n",
    "print(f'Loaded {len(melted_indices)} out of {len(file_list)} series ({round(len(melted_indices)/len(file_list)*100, 4)}%)')\n",
    "\n",
    "# Concatenate and pivot data\n",
    "index_df = pd.concat(melted_indices, ignore_index=True)\n",
    "index_df = index_df.pivot(index=['date', 'kind'], columns='symbol', values='pct_change')\n",
    "index_df = index_df.sort_values(by=['date', 'kind'], ascending=[True, False]).reset_index()#.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Clean historical data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define verified files\n",
    "try:\n",
    "    with open(verified_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        verified_files = [line.strip() for line in lines]\n",
    "    file_list = os.listdir(data_path)\n",
    "except FileNotFoundError:\n",
    "    file_list = os.listdir(data_path)\n",
    "    contracts_df = pd.read_csv('data/contract_elaborated.csv')\n",
    "\n",
    "    verified_files = []\n",
    "    for file_name in tqdm(file_list, total=len(file_list), desc=f\"Verifying {kind} series with elaborated data\"):\n",
    "        symbol, exchange, currency = file_name.replace('.csv', '').split('-')\n",
    "        try:\n",
    "            contract_details = ib.reqContractDetails(Stock(symbol, exchange, currency))\n",
    "            id = contract_details[0].secIdList[0].value\n",
    "\n",
    "            if contracts_df[contracts_df['symbol'] == symbol]['isin'].iloc[0] == id:\n",
    "                \n",
    "                instrument_name = contracts_df[contracts_df['symbol'] == symbol]['longName'].iloc[0]\n",
    "                instrument_name = instrument_name.replace('-', '').replace('+', '')\n",
    "                for word in instrument_name.split():\n",
    "                    if re.fullmatch(r'\\d+X', word):\n",
    "                        if int(word[:-1]) > 1:\n",
    "                            continue\n",
    "                        if word.startswith(('LV', 'LEV')):\n",
    "                            print(f'    {instrument_name}')\n",
    "                            \n",
    "                verified_files.append(file_name.split('-')[0])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    with open(verified_path, 'w') as f:\n",
    "        for item in verified_files:\n",
    "            f.write(str(item) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare historical training data\n",
    "# def melt(data_df, value_columns=None):\n",
    "#     if not value_columns:\n",
    "#         value_columns = ['open', 'close']\n",
    "#     id_columns = [col for col in data_df.columns.to_list() if col not in value_columns]\n",
    "#     melted_df = data_df.melt(id_vars=id_columns, value_vars=value_columns, var_name='kind', value_name='value')\n",
    "#     return melted_df.sort_values(by=['date', 'kind'], ascending=[True, False]).reset_index(drop=True)\n",
    "\n",
    "def clean_series(df, window=14, z=3):\n",
    "    # Ensure dates and numeric types\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    for col in ['open', 'high', 'low', 'close', 'volume', 'average', 'barCount']:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    if kind == 'trades' or 'indices':\n",
    "        price_col = 'average'\n",
    "    else:\n",
    "        price_col = 'close'\n",
    "\n",
    "    # Basic consistency checks\n",
    "    # df['inconsistent'] = (df['low'] > df['high']) | (df[price_col] < df['low']) | (df[price_col] > df['high'])\n",
    "    df['negatives'] = (df[price_col] < 0) | (df['volume'] < 0) # | (df['low'] < 0) | (df['high'] < 0)\n",
    "    \n",
    "    # df = df[~(df['open'] > (df['open'].median() + df['open'].std() * z))]\n",
    "    # df = df[~(df[price_col] > (df[price_col].median() + df[price_col].std() * z))]\n",
    "    # Outlier detection\n",
    "    df['total_outlier'] = (df[price_col] > (df[price_col].median() + df[price_col].std() * z)) | (df[price_col] < (df[price_col].median() - df[price_col].std() * z)) \n",
    "    \n",
    "    rolling_median = df[price_col].rolling(window=window, center=True, min_periods=1).mean()\n",
    "    rolling_std = df[price_col].rolling(window=window, center=True, min_periods=1).std()\n",
    "    std_threshold = z * rolling_std\n",
    "    df['std_outlier'] = np.abs(df[price_col] - rolling_median) > std_threshold\n",
    "\n",
    "    rolling_mad = df[price_col].rolling(window=window, center=True, min_periods=1).apply(lambda x: np.mean(np.abs(x - np.median(x))), raw=True)\n",
    "    mad_threshold = z * rolling_mad\n",
    "    df['mad_outlier'] = np.abs(df[price_col] - rolling_median) > mad_threshold\n",
    "    \n",
    "    rolling_iqr = df[price_col].rolling(window=window, center=True, min_periods=1).apply(lambda x: np.subtract(*np.percentile(x, [75, 25])), raw=True)\n",
    "    iqr_threshold = z * rolling_iqr\n",
    "    df['iqr_outlier'] = np.abs(df[price_col] - rolling_median) > iqr_threshold\n",
    "\n",
    "    df['all'] = df['std_outlier'] & df['mad_outlier'] & df['iqr_outlier']\n",
    "    \n",
    "    \n",
    "    return df, len(df), df['negatives'].sum(), df['inconsistent'].sum(), df['total_outlier'].sum(), df['std_outlier'].sum(), df['mad_outlier'].sum(), df['iqr_outlier'].sum(), df['all'].sum()\n",
    "    return df.drop(['barCount'], axis=1), len(df), df['negatives'].sum(), df['inconsistent'].sum(), df['total_outlier'].sum(), df['std_outlier'].sum(), df['mad_outlier'].sum(), df['iqr_outlier'].sum()\n",
    "\n",
    "\n",
    "    # Fill in rows with values that do not change the series' variance and ER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load historical data and merge them all into one df\n",
    "meta, check = [], {}\n",
    "file_list = os.listdir(data_path)\n",
    "for file in tqdm(file_list, total=len(file_list)):\n",
    "    symbol = os.path.splitext(file)[0].split('-')[0]\n",
    "    if symbol in verified_files:\n",
    "        df = pd.read_csv(data_path + file)\n",
    "        df, length, negatives, inconsistent, total_outlier, std_outlier, mad_outlier, iqr_outlier, all= clean_series(df, window=14, z=4)\n",
    "        meta.append(pd.DataFrame({'symbol': [symbol], 'length': length, 'negatives': [negatives], 'inconsistent': [inconsistent], 'total_outlier': [total_outlier], 'std_outlier': [std_outlier], 'mad_outlier': [mad_outlier], 'iqr_outlier': [iqr_outlier], 'all': [all]}))\n",
    "        check[symbol] = df\n",
    "meta = pd.concat(meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta['all'] = meta['mad_outlier'] & meta['iqr_outlier']\n",
    "meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = check['PJSR']\n",
    "test['all'] = test['std_outlier'] & test['mad_outlier'] & test['iqr_outlier']\n",
    "y = test['average']\n",
    "x = test['date']\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(x, y, marker='.', linestyle='-', color='b', label='Sample Series')\n",
    "\n",
    "# Adding title and labels\n",
    "plt.title('Sample Series Plot')\n",
    "plt.xlabel('X-axis')\n",
    "plt.ylabel('Y-axis')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "display(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Prep historical data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test day gap\n",
    "# dfs = {}\n",
    "# for file in os.listdir(data_path):\n",
    "#     symbol = os.path.splitext(file)[0].split('-')[0]\n",
    "#     if symbol in verified_files:\n",
    "#         dfs[symbol] = pd.read_csv(data_path + file)\n",
    "\n",
    "# days, nums, lens, firsts = [], [], [], []\n",
    "# for day in range(5,30):\n",
    "#     days.append(day)\n",
    "\n",
    "#     melted_dfs = []\n",
    "#     expected_returns = {}\n",
    "#     for symbol, df in tqdm(dfs.items(), total=len(dfs), desc=f'{day}'):\n",
    "#         df = melt(df)\n",
    "#         df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "#         latest_date = df['date'].iloc[-1]\n",
    "#         earliest_date = df['date'].iloc[0]\n",
    "#         length_required = pd.to_datetime('2020-02-01')\n",
    "#         month_ago = datetime.today() - timedelta(days=30)\n",
    "\n",
    "#         dates = df['date'].unique()\n",
    "#         date_diffs = dates[1:] - dates[:-1]\n",
    "        \n",
    "#         if latest_date >= month_ago and earliest_date <= length_required and not (date_diffs > pd.Timedelta(days=day)).any():\n",
    "#             df['symbol'] = symbol\n",
    "#             df['pct_change'] = df['value'].pct_change()\n",
    "#             expected_returns[symbol] = df['pct_change'].mean()\n",
    "#             melted_dfs.append(df)\n",
    "#     # print(f'Loaded {len(melted_dfs)} out of {len(file_list)} series ({round(len(melted_dfs)/len(file_list)*100, 4)}%)')\n",
    "\n",
    "#     # Concatenate and pivot data\n",
    "#     returns_df = pd.concat(melted_dfs, ignore_index=True)\n",
    "#     returns_df = returns_df.pivot(index=['date', 'kind'], columns='symbol', values='pct_change')\n",
    "#     returns_df = returns_df.sort_values(by=['date', 'kind'], ascending=[True, False]).reset_index().dropna()\n",
    "#     lens.append(len(returns_df))\n",
    "#     nums.append(len(returns_df.columns))\n",
    "#     firsts.append(returns_df.date.iloc[0])\n",
    "\n",
    "# gap_data_df = pd.DataFrame({\n",
    "#     'day_gap': days,\n",
    "#     'num_etfs': nums,\n",
    "#     'period_length': lens,\n",
    "#     'first_day':firsts})\n",
    "\n",
    "# gap_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare historical training data\n",
    "# def melt(data_df, value_columns=None):\n",
    "#     if not value_columns:\n",
    "#         value_columns = ['open', 'close']\n",
    "#     id_columns = [col for col in data_df.columns.to_list() if col not in value_columns]\n",
    "#     melted_df = data_df.melt(id_vars=id_columns, value_vars=value_columns, var_name='kind', value_name='value')\n",
    "#     return melted_df.sort_values(by=['date'], ascending=[True, False]).reset_index(drop=True)\n",
    "\n",
    "# Load historical data and merge them all into one df\n",
    "dfs = {}\n",
    "file_list = os.listdir(data_path)\n",
    "for file in file_list:\n",
    "    symbol = os.path.splitext(file)[0].split('-')[0]\n",
    "    if symbol in verified_files:\n",
    "        dfs[symbol] = pd.read_csv(data_path + file)\n",
    "\n",
    "\n",
    "# Melt dfs, filters, and calc pct_change. ASSUMES that dfs are sorted chronologically\n",
    "training_start_date = pd.to_datetime('2020-02-01')\n",
    "month_ago = datetime.today() - timedelta(days=31)\n",
    "\n",
    "day_gap = 6 # SET ACCEPTABLE DAY GAP\n",
    "\n",
    "melted_dfs, expected_returns = [], {}\n",
    "for symbol, df in tqdm(dfs.items(), total=len(dfs), desc=f'Filtering {kind} dfs'):\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "    latest_date = df['date'].iloc[-1]\n",
    "    earliest_date = df['date'].iloc[0]\n",
    "    dates = df['date'].unique()\n",
    "    date_gaps = dates[1:] - dates[:-1]\n",
    "    \n",
    "    if (kind == 'indices') or (latest_date >= month_ago and earliest_date <= training_start_date and (date_gaps <= pd.Timedelta(days=day_gap)).all()):\n",
    "        df['symbol'] = symbol\n",
    "        df['pct_change'] = df['average'].pct_change()\n",
    "        expected_returns[symbol] = df['pct_change'].mean()\n",
    "        melted_dfs.append(df)\n",
    "print(f'Loaded {len(melted_dfs)} out of {len(file_list)} series ({round(len(melted_dfs)/len(file_list)*100, 4)}%)')\n",
    "\n",
    "# Concatenate and pivot data\n",
    "returns_df = pd.concat(melted_dfs, ignore_index=True)\n",
    "returns_df = returns_df.pivot(index=['date'], columns='symbol', values='pct_change')\n",
    "returns_df = returns_df.sort_values(by=['date'], ascending=[True]).reset_index()\n",
    "\n",
    "# Define training boundaries\n",
    "training_cutoff_date = datetime.today() - timedelta(days=365)\n",
    "training_df = returns_df[returns_df['date'] <= training_cutoff_date]\n",
    "training_matrix = training_df.drop(['date'], axis=1).dropna().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate risk-free-rate for training window\n",
    "treasury_rate = web.DataReader('DGS10', 'fred', training_cutoff_date-timedelta(days=365), training_cutoff_date)\n",
    "nominal_rf_rate = treasury_rate.mean() / 100\n",
    "\n",
    "fred = Fred(api_key='30ae0e4e7713662116edf836cec71562')\n",
    "cpi_data = fred.get_series('CPIAUCSL', training_cutoff_date-timedelta(days=365), training_cutoff_date) # CPI\n",
    "inflation_rate = (cpi_data.iloc[-1] - cpi_data.iloc[0]) / cpi_data.iloc[0]\n",
    "\n",
    "real_rf_rate = (1 + nominal_rf_rate) / (1 + inflation_rate) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate corr and cov for historical training data\n",
    "training_array = training_matrix.values # Convert training matrix to numpy array\n",
    "symbol_list = training_matrix.columns.tolist()\n",
    "num_symbols = len(symbol_list)\n",
    "corr_matrix = np.zeros((num_symbols, num_symbols)) # Pre-allocate numpy array for correlation\n",
    "cov_matrix = np.zeros((num_symbols, num_symbols))  # Pre-allocate numpy array for covariance\n",
    "\n",
    "for i, sym_i in tqdm(enumerate(symbol_list), total=num_symbols, desc=f\"Calculating distance stats sqr\"):\n",
    "    for j, sym_j in enumerate(symbol_list):\n",
    "        if i <= j:  # Compute only for upper triangle (including diagonal)\n",
    "            stats = dcor.distance_stats(training_array[:, i], training_array[:, j])\n",
    "            corr_value = stats.correlation_xy\n",
    "            cov_value = stats.covariance_xy\n",
    "\n",
    "            corr_matrix[i, j] = corr_value\n",
    "            corr_matrix[j, i] = corr_value  # Fill symmetric value\n",
    "\n",
    "            cov_matrix[i, j] = cov_value\n",
    "            cov_matrix[j, i] = cov_value  # Fill symmetric value\n",
    "\n",
    "corr_df = pd.DataFrame(corr_matrix, index=symbol_list, columns=symbol_list) # Convert numpy array back to df for output\n",
    "cov_df = pd.DataFrame(cov_matrix, index=symbol_list, columns=symbol_list)   # Convert numpy array back to df for output\n",
    "\n",
    "corr_df.to_csv(f'{root}corr_df.csv', index=False)\n",
    "cov_df.to_csv(f'{root}cov_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Compute etf combinations based on optimal k_clusters\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load corr and cov\n",
    "corr_df = pd.read_csv(f'{root}corr_df.csv')\n",
    "cov_df = pd.read_csv(f'{root}cov_df.csv')\n",
    "symbol_list = corr_df.columns\n",
    "\n",
    "symbol2index = dict(zip(corr_df.columns, corr_df.index))\n",
    "index2symbol = dict(zip(corr_df.index, corr_df.columns))\n",
    "corr_df.rename(columns=symbol2index, inplace=True)\n",
    "cov_df.rename(columns=symbol2index, inplace=True)\n",
    "\n",
    "distance_matrix = (1 - corr_df).to_numpy()\n",
    "np.fill_diagonal(distance_matrix, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thresholds / cluster_num graphs\n",
    "methods = ['single', 'ward', 'average', 'complete', 'weighted', 'centroid', 'median']\n",
    "methods = ['ward']\n",
    "for method in methods:\n",
    "    linked = sch.linkage(squareform(distance_matrix), method=method)\n",
    "    \n",
    "    num_clusters = range(len(corr_df), 1, -1)\n",
    "    thresholds = linked[:, 2]\n",
    "\n",
    "    # inertias = []\n",
    "    # for n_clusters in num_clusters:\n",
    "    #     cluster_labels = fcluster(linked, t=n_clusters, criterion='maxclust')\n",
    "    #     inertia = 0\n",
    "    #     for cluster in np.unique(cluster_labels):\n",
    "    #         members = distance_matrix.values[cluster_labels == cluster]\n",
    "    #         centroid = members.mean(axis=0)  # Cluster centroid\n",
    "    #         inertia += np.sum((members - centroid) ** 2)\n",
    "    #     inertias.append(inertia)\n",
    "\n",
    "    # plt.figure(figsize=(12, 6))\n",
    "    # plt.plot(num_clusters, inertias, marker='o', label=f\"Method {method}\")\n",
    "    # plt.title(f\"Inertia/Num ({method})\")\n",
    "    # plt.xlabel('Number of Clusters')\n",
    "    # plt.ylabel('Inertia (Sum of Squared Distances)')\n",
    "    # plt.grid(True)\n",
    "    # plt.legend()\n",
    "    # plt.show()\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(num_clusters, thresholds, marker='o')\n",
    "    plt.title(f\"Threshold/Num ({method})\")\n",
    "    plt.xlabel('Number of Clusters')\n",
    "    plt.ylabel('Threshold (Distance)')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouettes and dendrograms\n",
    "def product(row):\n",
    "    product = 1\n",
    "    for value in row.values():\n",
    "        product *= value\n",
    "    return product\n",
    "\n",
    "ks = []\n",
    "scores = []\n",
    "counts = []\n",
    "for k in range(2, min(len(distance_matrix), 9)):\n",
    "    clusters = AgglomerativeClustering(n_clusters=k, linkage='ward').fit_predict(distance_matrix)\n",
    "    score = silhouette_score(distance_matrix, clusters, metric='precomputed')\n",
    "    ks.append(k)\n",
    "    scores.append(score)\n",
    "    unique_clusters, label_counts = np.unique(clusters, return_counts=True)\n",
    "    label_counts_dict = dict(zip(unique_clusters, label_counts))\n",
    "    counts.append(label_counts_dict)\n",
    "\n",
    "silhouettes = pd.DataFrame({\n",
    "    'k': ks,\n",
    "    'score': scores,\n",
    "    'counts': counts\n",
    "})\n",
    "silhouettes['combitions'] = silhouettes['counts'].apply(product)\n",
    "silhouettes = silhouettes.sort_values(by='score', ascending=False)\n",
    "best_k = silhouettes.k.iloc[0]\n",
    "\n",
    "# best_k = 3\n",
    "\n",
    "display(silhouettes)\n",
    "methods = ['single', 'ward', 'average', 'complete', 'weighted', 'centroid', 'median']\n",
    "methods = ['ward']\n",
    "for method in methods:\n",
    "    # Now compute the linkage using a condensed distance matrix\n",
    "    linked = sch.linkage(squareform(distance_matrix), method=method)\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    sch.dendrogram(linked, labels=corr_df.index, leaf_rotation=90)\n",
    "    plt.title(f\"Method {method}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check cluster data\n",
    "def evaluate_literal(val):\n",
    "    try:\n",
    "        return ast.literal_eval(val)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return val\n",
    "\n",
    "contracts_df = pd.read_csv('data/contract_elaborated.csv')\n",
    "\n",
    "columns_to_explode = ['profile', 'style', 'lipper', 'fundamentals', 'holding_types', 'dividends', 'industries', 'countries', 'currencies', 'debtors', 'maturity', 'debt_type']\n",
    "for col in columns_to_explode:\n",
    "    contracts_df[col] = contracts_df[col].fillna('[]')\n",
    "\n",
    "contracts_df = contracts_df.applymap(evaluate_literal)\n",
    "\n",
    "# columns_to_explode = ['profile', 'style', 'lipper', 'dividends', 'debtors', 'maturity', 'debt_type']\n",
    "# columns_to_explode = ['profile']\n",
    "\n",
    "# for col in tqdm(columns_to_explode, total=len(columns_to_explode)):\n",
    "#     contracts_df = contracts_df.explode(col)\n",
    "#     contracts_df.reset_index(drop=True, inplace=True)\n",
    "#     display(contracts_df)\n",
    "    \n",
    "#     tuple_df = pd.DataFrame(contracts_df[col].tolist(), index=contracts_df.index)\n",
    "#     tuple_df.columns = [f\"{col}_{label}\" for label in tuple_df.iloc[0]]\n",
    "    \n",
    "#     contracts_df.drop(columns=[col], inplace=True)\n",
    "#     contracts_df = pd.concat([contracts_df, tuple_df], axis=1)\n",
    "\n",
    "\n",
    "# clusters = AgglomerativeClustering(n_clusters=best_k, linkage='ward').fit_predict(distance_matrix)\n",
    "# cluster_df = pd.DataFrame({\n",
    "#     'symbol': symbol_list,\n",
    "#     'cluster': clusters\n",
    "# })\n",
    "\n",
    "# contracts_df = contracts_df.merge(cluster_df, on='symbol', how='left')\n",
    "# contracts_df = contracts_df[~contracts_df['cluster'].isna()]\n",
    "# contracts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "def safe_literal_eval(val):\n",
    "    if isinstance(val, str):\n",
    "        try:\n",
    "            return ast.literal_eval(val)\n",
    "        except (ValueError, SyntaxError):\n",
    "            return np.nan\n",
    "    return val\n",
    "\n",
    "contracts_df = pd.read_csv('data/contract_elaborated.csv')\n",
    "\n",
    "columns_to_explode = ['profile', 'style', 'lipper', 'fundamentals', 'holding_types', 'dividends', 'industries', 'countries', 'currencies', 'debtors', 'maturity', 'debt_type']\n",
    "\n",
    "for col in columns_to_explode:\n",
    "    contracts_df[col] = contracts_df[col].fillna('[]')\n",
    "    contracts_df[col] = contracts_df[col].apply(safe_literal_eval)\n",
    "\n",
    "contracts_df = contracts_df.explode('holding_types')\n",
    "contracts_df[['holding_type', 'percentage']] = pd.DataFrame(contracts_df['holding_types'].tolist(), index=contracts_df.index)\n",
    "contracts_df = contracts_df.pivot_table(index=contracts_df.index, columns='holding_type', values='percentage', aggfunc='first')\n",
    "contracts_df = pd.concat([contracts_df, contracts_df.groupby(level=0).first()], axis=1)\n",
    "contracts_df = contracts_df.loc[:,~contracts_df.columns.duplicated()]\n",
    "contracts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Calculate Minimum Variance Portfolios\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Portfolio Optimization Functions\n",
    "def portfolio_variance(weights, cov_matrix):\n",
    "    return weights.T @ cov_matrix @ weights\n",
    "\n",
    "def portfolio_expected_return(weights, expected_returns_arr):\n",
    "    return weights @ expected_returns_arr\n",
    "\n",
    "def minimize_portfolio_variance(cov_matrix, expected_returns_arr):\n",
    "    num_assets = len(cov_matrix)\n",
    "    initial_weights = np.array([1/num_assets] * num_assets)\n",
    "    bounds = tuple((0, 1) for _ in range(num_assets))\n",
    "    constraints = ({'type': 'eq', 'fun': lambda weights: np.sum(weights) - 1})\n",
    "\n",
    "    optimization_result = minimize(portfolio_variance,\n",
    "                                    initial_weights,\n",
    "                                    args=(cov_matrix,),\n",
    "                                    method='SLSQP',\n",
    "                                    bounds=bounds,\n",
    "                                    constraints=constraints)\n",
    "\n",
    "    if optimization_result.success:\n",
    "        optimized_weights = optimization_result.x\n",
    "        port_variance = optimization_result.fun\n",
    "        port_std = np.sqrt(port_variance)\n",
    "        port_er = portfolio_expected_return(optimized_weights, expected_returns_arr)\n",
    "\n",
    "        return (optimized_weights, port_std, port_er)\n",
    "    else:\n",
    "        return (np.nan, np.nan, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Portfolio Optimization Functions\n",
    "def compute_distance_sum(combination, distance_matrix):\n",
    "    distance_sum = 0\n",
    "    for i_idx, j_idx in itertools.combinations(combination, 2):\n",
    "        distance_sum += distance_matrix[i_idx, j_idx]\n",
    "    return distance_sum\n",
    "\n",
    "def portfolio_variance(weights, cov_matrix):\n",
    "    return weights.T @ cov_matrix @ weights\n",
    "\n",
    "def portfolio_expected_return(weights, expected_returns_arr):\n",
    "    return weights @ expected_returns_arr\n",
    "\n",
    "def sharpe_ratio(weights, expected_returns_arr, cov_matrix, risk_free_rate):\n",
    "    port_er = portfolio_expected_return(weights, expected_returns_arr)\n",
    "    port_variance = portfolio_variance(weights, cov_matrix)\n",
    "    port_std = np.sqrt(port_variance)\n",
    "    return (port_er - risk_free_rate) / port_std\n",
    "\n",
    "def negative_sharpe_ratio(weights, expected_returns_arr, cov_matrix, risk_free_rate):\n",
    "    return -sharpe_ratio(weights, expected_returns_arr, cov_matrix, risk_free_rate)\n",
    "\n",
    "def find_tangency_portfolio(cov_matrix, expected_returns_arr, risk_free_rate):\n",
    "    num_assets = len(cov_matrix)\n",
    "    initial_weights = np.array([1/num_assets] * num_assets)\n",
    "    bounds = tuple((0, 1) for _ in range(num_assets))\n",
    "    constraints = ({'type': 'eq', 'fun': lambda weights: np.sum(weights) - 1})\n",
    "\n",
    "    optimization_result = minimize(negative_sharpe_ratio,\n",
    "                                    initial_weights,\n",
    "                                    args=(expected_returns_arr, cov_matrix, risk_free_rate),\n",
    "                                    method='SLSQP',\n",
    "                                    bounds=bounds,\n",
    "                                    constraints=constraints)\n",
    "\n",
    "    if optimization_result.success:\n",
    "        optimized_weights = optimization_result.x\n",
    "        variance = portfolio_variance(optimized_weights, cov_matrix)\n",
    "        std = np.sqrt(variance)\n",
    "        er = portfolio_expected_return(optimized_weights, expected_returns_arr)\n",
    "        sharpe_ratio = -(optimization_result.fun)\n",
    "\n",
    "        return (optimized_weights, std, er, sharpe_ratio)\n",
    "    else:\n",
    "        return (np.nan, np.nan, np.nan, np.nan)\n",
    "    \n",
    "def sort_top_combinations(array, sort_index):\n",
    "    valid_rows = ~np.isnan(array[:, sort_index])\n",
    "    valid_array = array[valid_rows]\n",
    "    if valid_array.size > 0:\n",
    "        sort_values = valid_array[:, sort_index]\n",
    "        sort_indices = np.argsort(sort_values)[::-1]\n",
    "        array[valid_rows] = valid_array[sort_indices]\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_symbols = len(corr_df.index)\n",
    "num_metrics = best_k*2 + 4\n",
    "num_combinations = math.comb(num_symbols, best_k)\n",
    "combination_array = np.empty((num_combinations, num_metrics), dtype='float32')\n",
    "\n",
    "# Calculate distance sums and populate the NumPy array\n",
    "for i, combination in tqdm(enumerate(itertools.combinations(range(0,num_symbols), best_k)), total=num_combinations, desc=\"Calculating distance sums\"):\n",
    "    combination_array[i, :best_k] = combination\n",
    "    combination_cov_df = cov_df.loc[combination, combination]\n",
    "    combination_expected_returns = np.array([expected_returns[index2symbol[index]] for index in combination])\n",
    "\n",
    "    index_indicator = best_k + best_k + 3\n",
    "    combination_array[i, best_k: index_indicator] = find_tangency_portfolio(combination_cov_df, combination_expected_returns, real_rf_rate)\n",
    "    combination_array[i, index_indicator: index_indicator + 1] = compute_distance_sum(combination, distance_matrix)\n",
    "\n",
    "    # population growth rate\n",
    "\n",
    "\n",
    "\n",
    "# TODO - not to be sorted by best_k\n",
    "sorted_indices = np.argsort(combination_array[:, best_k], kind='mergesort')[::-1]\n",
    "combination_array = combination_array[sorted_indices]\n",
    "del sorted_indices\n",
    "combination_array, len(combination_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_symbols = len(corr_df.index)\n",
    "num_metrics = best_k*2 + 4\n",
    "num_combinations_possible = math.comb(num_symbols, best_k)\n",
    "\n",
    "top_n = 5000 # Define how many top combinations to keep\n",
    "\n",
    "top_combinations_array = np.empty((top_n, num_metrics), dtype='float32')\n",
    "top_combinations_array[:] = np.nan\n",
    "rows_filled = 0\n",
    "\n",
    "\n",
    "for combination_tuple in tqdm(itertools.combinations(range(0,num_symbols), best_k), total=num_combinations_possible, desc=\"Calculating Tangency Portfolios\"):\n",
    "    combination_cov_df = cov_df.loc[combination_tuple, combination_tuple]\n",
    "    combination_expected_returns = np.array([expected_returns[index2symbol[index]] for index in combination_tuple])\n",
    "    weights, std, er, sharpe = find_tangency_portfolio(combination_cov_df, combination_expected_returns, real_rf_rate)\n",
    "    rating = sharpe * compute_distance_sum(combination_tuple, distance_matrix)\n",
    "\n",
    "    if rows_filled < top_n:\n",
    "        top_combinations_array[rows_filled, :best_k] = combination_tuple\n",
    "        top_combinations_array[rows_filled, best_k:best_k*2] = weights\n",
    "        top_combinations_array[rows_filled, best_k*2: num_metrics] = [std, er, sharpe, rating]\n",
    "        rows_filled += 1\n",
    "        if rows_filled == top_n:\n",
    "            top_combinations_array = sort_top_combinations(top_combinations_array, -1)\n",
    "\n",
    "    else:\n",
    "        if rating > top_combinations_array[-1, -1]:\n",
    "            top_combinations_array[rows_filled-1, :best_k] = combination_tuple\n",
    "            top_combinations_array[rows_filled-1, best_k:best_k*2] = weights\n",
    "            top_combinations_array[rows_filled-1, best_k*2: num_metrics] = [std, er, sharpe, rating]\n",
    "            top_combinations_array = sort_top_combinations(top_combinations_array, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove NaN rows before further processing\n",
    "top_combinations_array_cleaned = top_combinations_array[~np.isnan(top_combinations_array[:, best_k])]\n",
    "\n",
    "print(\"Top\", top_n, \"Combinations by Sharpe Ratio:\")\n",
    "for row in top_combinations_array_cleaned:\n",
    "    combination_indices = row[:best_k].astype(int)\n",
    "    asset_symbols = [index2symbol[index] for index in combination_indices]\n",
    "    # asset_symbols = [index for index in combination_indices]\n",
    "    weights, std, er, sharpe, rating = row[best_k:best_k+best_k], row[best_k+best_k], row[best_k+best_k+1], row[best_k+best_k+2], row[-1]\n",
    "    print(f\"Assets: {asset_symbols}, Weights: {weights}, Std Dev: {std:.4f}, Expected Return: {er:.4f}, Sharpe Ratio: {sharpe:.4f}, Rating: {rating:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_weights(best_k, step=0.05):\n",
    "    weights_list = []\n",
    "    def recurse_weights(current_weights, remaining_weight, current_step):\n",
    "        if len(current_weights) == best_k - 1:\n",
    "            current_weights.append(remaining_weight)\n",
    "            if remaining_weight >= 0:\n",
    "                weights_list.append(current_weights)\n",
    "            return\n",
    "        for w in np.arange(0, remaining_weight + step, step):\n",
    "            recurse_weights(current_weights + [w], remaining_weight - w, step)\n",
    "    recurse_weights([], 1, step)\n",
    "    return np.array(weights_list)\n",
    "\n",
    "\n",
    "sampled_symbols = sampled_etfs['symbol'].tolist()\n",
    "sample_ER = pd.DataFrame({'symbol': sampled_symbols, 'er': [expected_returns[s] for s in sampled_symbols]})\n",
    "cov_matrix = returns_df[sampled_symbols].dropna().cov()#.values\n",
    "\n",
    "weights_array = generate_weights(len(sampled_symbols))\n",
    "\n",
    "# Now calculate portfolio ER and STD for each weight permutation\n",
    "portfolio_variances = np.array([np.dot(w.T, np.dot(cov_matrix, w)) for w in weights_array])\n",
    "portfolio_std = np.sqrt(portfolio_variances)\n",
    "portfolio_er = weights_array.dot(sample_ER['er'])\n",
    "\n",
    "# Create a dictionary for frontier_df\n",
    "data = {f'w{i+1}': weights_array[:, i] for i in range(weights_array.shape[1])}\n",
    "data['portfolio_er'] = portfolio_er\n",
    "data['portfolio_std'] = portfolio_std\n",
    "frontier_df = pd.DataFrame(data)\n",
    "\n",
    "# Find efficient portfolios \n",
    "frontier_df = frontier_df.sort_values('portfolio_std')#.reset_index(drop=True)\n",
    "extreme_portfolios = frontier_df.loc[(frontier_df == 1).any(axis=1)]\n",
    "\n",
    "efficient_portfolios = pd.DataFrame(columns=frontier_df.columns)\n",
    "current_max_ER = -np.inf\n",
    "for idx, row in frontier_df.iterrows():\n",
    "    if row['portfolio_er'] > current_max_ER:\n",
    "        efficient_portfolios = pd.concat([efficient_portfolios, row.to_frame().T], ignore_index=True)\n",
    "        current_max_ER = row['portfolio_er']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a second-degree polynomial to the efficient frontier\n",
    "coeffs = np.polyfit(efficient_portfolios['portfolio_er'], efficient_portfolios['portfolio_std'], 2)\n",
    "poly_func = np.poly1d(coeffs)\n",
    "\n",
    "x_vals = np.linspace(efficient_portfolios['portfolio_er'].min(), efficient_portfolios['portfolio_er'].max(), 100)\n",
    "y_vals = poly_func(x_vals)\n",
    "\n",
    "plt.scatter(frontier_df['portfolio_std'], frontier_df['portfolio_er'], alpha=0.1, label='All Portfolios')\n",
    "plt.plot(efficient_portfolios['portfolio_std'], efficient_portfolios['portfolio_er'], 'ro', label='Efficient Frontier')\n",
    "plt.plot(extreme_portfolios['portfolio_std'], extreme_portfolios['portfolio_er'], 'yo', label='Extreme Portfolios')\n",
    "plt.plot(y_vals, x_vals, 'b-', label='Fitted Curve')\n",
    "plt.xlabel('Portfolio Standard Deviation')\n",
    "plt.ylabel('Portfolio Expected Return')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
