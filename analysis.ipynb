{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ib_async import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from scipy.spatial.distance import squareform\n",
    "import dcor\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "from time import sleep\n",
    "import csv\n",
    "from scipy.optimize import minimize\n",
    "from fredapi import Fred\n",
    "import pandas_datareader.data as web\n",
    "import math\n",
    "import re\n",
    "import ast\n",
    "import traceback\n",
    "\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep functions\n",
    "def evaluate_literal(val):\n",
    "    try:\n",
    "        return ast.literal_eval(val)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return val\n",
    "    \n",
    "def load(path):\n",
    "    df = pd.read_csv(path)\n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].apply(evaluate_literal)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kind = 'midpoint'\n",
    "kind = 'trades'\n",
    "# kind = 'indices'\n",
    "\n",
    "if kind == 'midpoint':\n",
    "    root = 'data/daily-midpoint/'\n",
    "elif kind == 'trades':\n",
    "    root = 'data/daily-trades/'\n",
    "elif kind == 'indices':\n",
    "    root = 'data/indices/'\n",
    "\n",
    "data_path = root + 'series/'\n",
    "verified_path = root + 'verified_files.csv'\n",
    "\n",
    "if kind in ['trades', 'indices']:\n",
    "    price_col = 'average'\n",
    "else:\n",
    "    price_col = 'close'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify files\n",
    "fund_df = load('data/fundamentals.csv')\n",
    "\n",
    "try:\n",
    "    verified_df = pd.read_csv(verified_path)\n",
    "except FileNotFoundError:\n",
    "    util.startLoop()\n",
    "    ib = IB()\n",
    "    ib.connect('127.0.0.1', 7497, clientId=2)\n",
    "\n",
    "    file_list = os.listdir(data_path)\n",
    "    verified_files = []\n",
    "\n",
    "    for file_name in tqdm(file_list, total=len(file_list), desc=\"Verifying files\"):\n",
    "        if not file_name.endswith('.csv'):\n",
    "            continue\n",
    "        try:\n",
    "            symbol, exchange, currency = file_name.replace('.csv', '').split('-')\n",
    "            symbol_data = fund_df[(fund_df['symbol'] == symbol) & (fund_df['currency'] == currency)]\n",
    "            if symbol_data.empty:\n",
    "                continue\n",
    "\n",
    "            contract_details = ib.reqContractDetails(Stock(symbol, exchange, currency))\n",
    "            if not contract_details:\n",
    "                continue\n",
    "            isin = contract_details[0].secIdList[0].value\n",
    "\n",
    "            if symbol_data['isin'].iloc[0] != isin:\n",
    "                continue\n",
    "\n",
    "            instrument_name = symbol_data['longName'].iloc[0].replace('-', '').replace('+', '')\n",
    "            leveraged = any(\n",
    "                re.fullmatch(r'\\d+X', word) and int(word[:-1]) > 1 or word.lower().startswith(('lv', 'lev'))\n",
    "                for word in instrument_name.split()\n",
    "            )\n",
    "            if leveraged:\n",
    "                continue\n",
    "\n",
    "            verified_files.append({'symbol': symbol, 'currency': currency})\n",
    "        except ValueError as e:\n",
    "            print(f\"Invalid filename format {file_name}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_name}: {e}\")\n",
    "\n",
    "    verified_df = pd.DataFrame(verified_files)\n",
    "    verified_df.to_csv(verified_path, index=False)\n",
    "\n",
    "    ib.disconnect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge historical series with fundamentals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_series_types(df, price_col):\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.sort_values('date').reset_index(drop=True)\n",
    "    for col in ['volume', price_col]:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    return df\n",
    "\n",
    "def validate_raw_prices(df, price_col):\n",
    "    invalid_price_mask = df[price_col] <= 0\n",
    "    inconsistent_mask = pd.Series(False, index=df.index)\n",
    "    if 'low' in df.columns and 'high' in df.columns:\n",
    "        inconsistent_mask = (df['low'] > df['high'])\n",
    "\n",
    "    local_error_mask = invalid_price_mask | inconsistent_mask\n",
    "    df = df[~local_error_mask].copy()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def handle_stale_periods(df, price_col, max_stale_days=5):\n",
    "    stale_groups = (df[price_col].diff() != 0).cumsum()\n",
    "    if stale_groups.empty:\n",
    "        return df\n",
    "    \n",
    "    period_lengths = df.groupby(stale_groups)[price_col].transform('size')\n",
    "    long_stale_mask = period_lengths > max_stale_days\n",
    "    \n",
    "    is_intermediate_stale_row = (stale_groups.duplicated(keep='first') & \n",
    "                             stale_groups.duplicated(keep='last'))\n",
    "    \n",
    "    rows_to_drop_mask = long_stale_mask & is_intermediate_stale_row\n",
    "    df = df[~rows_to_drop_mask].copy()\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load historical series\n",
    "if 'copied' not in globals() or input('reload csvs? (y/n)').lower() == 'y':\n",
    "    latest = (datetime.now() - timedelta(days=365 * 6))\n",
    "    meta = []\n",
    "    file_list = os.listdir(data_path)\n",
    "    for file in tqdm(file_list, total=len(file_list)):\n",
    "        if not file.endswith('.csv'):\n",
    "            continue\n",
    "        \n",
    "        parts = os.path.splitext(file)[0].split('-')\n",
    "        symbol, exchange, currency = parts[0], parts[1], parts[2]\n",
    "        if not ((verified_df['symbol'] == symbol) & (verified_df['currency'] == currency)).any():\n",
    "            continue\n",
    "        \n",
    "        # Load and clean raw series\n",
    "        try:\n",
    "            df = load(data_path + file)\n",
    "            df = ensure_series_types(df, price_col)\n",
    "            df = validate_raw_prices(df, price_col)\n",
    "            df = handle_stale_periods(df, price_col)\n",
    "            # df = adjust_for_splits(df, price_col)\n",
    "\n",
    "            df['pct_change'] = df[price_col].pct_change()\n",
    "            if df['date'].max() > latest:\n",
    "                latest = df['date'].max()\n",
    "\n",
    "            meta.append({\n",
    "                'symbol': symbol,\n",
    "                'currency': currency,\n",
    "                'exchange_api': exchange,\n",
    "                'df': df[['date', price_col, 'volume', 'pct_change']],\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR {file}: {e}\")\n",
    "            \n",
    "    meta = pd.DataFrame(meta)\n",
    "    copied = meta.copy()\n",
    "    copied['df'] = copied['df'].apply(lambda x: x.copy()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESET\n",
    "meta = copied.copy()\n",
    "meta['df'] = copied['df'].apply(lambda x: x.copy()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_and_nullify_global_outliers(meta_df, price_col, z_threshold=120.0, window=5):\n",
    "    all_pct_changes = pd.concat(\n",
    "        [row['df']['pct_change'] for _, row in meta_df.iterrows()],\n",
    "        ignore_index=True\n",
    "    ).dropna()\n",
    "    all_pct_changes = all_pct_changes[~np.isinf(all_pct_changes) & (all_pct_changes != 0)]\n",
    "\n",
    "    global_median_return = all_pct_changes.median()\n",
    "    global_mad = (all_pct_changes - global_median_return).abs().median()\n",
    "\n",
    "    outlier_series = {}\n",
    "    for idx, row in tqdm(meta_df.iterrows(), total=len(meta_df)):\n",
    "        df = row['df']\n",
    "        df = df.reset_index(drop=True)\n",
    "        if df['pct_change'].isnull().all():\n",
    "            continue\n",
    "        cols_to_null = [price_col, 'volume', 'high', 'low', 'pct_change']\n",
    "        cols_to_null = [c for c in cols_to_null if c in df.columns]\n",
    "\n",
    "        absolute_modified_z = (df['pct_change'] - global_median_return).abs() / global_mad\n",
    "        outlier_mask = absolute_modified_z > z_threshold\n",
    "        \n",
    "        if outlier_mask.any():\n",
    "            data_dict = absolute_modified_z[outlier_mask].describe()\n",
    "\n",
    "            candidate_indices = df.index[outlier_mask]\n",
    "            for df_idx in candidate_indices:\n",
    "                price_to_check_idx = df_idx - 1\n",
    "                price_to_check = df.loc[price_to_check_idx, price_col]\n",
    "                local_window_start = max(0, price_to_check_idx - window)\n",
    "                local_window = df.loc[local_window_start : price_to_check_idx - 1, price_col].dropna()\n",
    "                local_mean = local_window.mean()\n",
    "                local_std = local_window.std()\n",
    "                if local_std != 0: \n",
    "                    price_z_score = abs(price_to_check - local_mean) / local_std\n",
    "                    if price_z_score > z_threshold / 10:\n",
    "                        df.loc[price_to_check_idx, cols_to_null] = np.nan\n",
    "\n",
    "                price_to_check = df.loc[df_idx, price_col]\n",
    "                local_window_end = min(df_idx + window, df.index[outlier_mask].max())\n",
    "                local_window = df.loc[df_idx + 1: local_window_end, price_col].dropna()\n",
    "                local_mean = local_window.mean()\n",
    "                local_std = local_window.std()\n",
    "                if local_std != 0:\n",
    "                    price_z_score = abs(price_to_check - local_mean) / local_std\n",
    "                    if price_z_score > z_threshold / 10:\n",
    "                        df.loc[df_idx, cols_to_null] = np.nan\n",
    "\n",
    "\n",
    "            data_dict['new_length'] = len(df)\n",
    "            outlier_series[row['symbol']] = data_dict\n",
    "            \n",
    "            df['pct_change'] = df[price_col].pct_change(fill_method=None)\n",
    "            \n",
    "            meta_df.at[idx, 'df'] = df\n",
    "\n",
    "    return outlier_series\n",
    "\n",
    "z_threshold = 50\n",
    "window = 5\n",
    "modified_series_info = detect_and_nullify_global_outliers(meta, price_col=price_col, z_threshold=z_threshold, window=window)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check global outliers ### NATURALLY PLOTS A FEW STRAGLERS\n",
    "# def global_return_filter(meta_df, z_threshold=120.0):\n",
    "#     all_pct_changes = pd.concat(\n",
    "#         [row['df']['pct_change'] for _, row in meta_df.iterrows()],\n",
    "#         ignore_index=True\n",
    "#     )\n",
    "#     all_pct_changes.dropna(inplace=True)\n",
    "#     all_pct_changes = all_pct_changes[~np.isinf(all_pct_changes) & (all_pct_changes != 0)]\n",
    "\n",
    "#     global_median_return = all_pct_changes.median()\n",
    "#     global_mad = (all_pct_changes - global_median_return).abs().median()\n",
    "\n",
    "#     outlier_series = {}\n",
    "#     for _, row in tqdm(meta_df.iterrows(), total=len(meta_df)):\n",
    "#         df = row['df']        \n",
    "#         absolute_modified_z = (df['pct_change'] - global_median_return).abs() / global_mad\n",
    "#         if absolute_modified_z.max() > z_threshold:\n",
    "#             outlier_series[row['symbol']] = absolute_modified_z.describe()\n",
    "\n",
    "#     return outlier_series, global_mad, global_median_return\n",
    "\n",
    "# z_threshold = 50\n",
    "# globally_defective_symbols, global_mad, global_median_return = global_return_filter(meta, z_threshold=z_threshold)\n",
    "# globally_defective_symbols = pd.DataFrame(globally_defective_symbols)\n",
    "\n",
    "# meta_indexed = meta.set_index('symbol')\n",
    "# for symbol in globally_defective_symbols.T.sort_values(by='max', ascending=True).index.tolist():\n",
    "#     df = meta_indexed.loc[symbol, 'df'].copy()\n",
    "#     df = df.reset_index(drop=True)\n",
    "\n",
    "#     absolute_modified_z = (df['pct_change'] - global_median_return).abs() / global_mad\n",
    "#     outlier_mask = absolute_modified_z > z_threshold\n",
    "\n",
    "#     corrected_outlier_mask = pd.Series(False, index=df.index)\n",
    "#     for df_idx in df.index[outlier_mask]:\n",
    "#         # Check data points before\n",
    "#         price_to_check_idx = df_idx - 1\n",
    "#         price_to_check = df.at[price_to_check_idx, price_col]\n",
    "#         local_window_start = max(0, price_to_check_idx - window)\n",
    "#         local_window = df.loc[local_window_start : price_to_check_idx - 1, price_col].dropna()\n",
    "#         local_mean = local_window.mean()\n",
    "#         local_std = local_window.std()\n",
    "#         if local_std != 0:\n",
    "#             price_z_score = abs(price_to_check - local_mean) / local_std\n",
    "#             if price_z_score > z_threshold / 10:\n",
    "#                 corrected_outlier_mask.at[price_to_check_idx] = True\n",
    "\n",
    "#         # Check data points after\n",
    "#         price_to_check = df.at[df_idx, price_col]\n",
    "#         local_window_end = min(df_idx + window, df.index[outlier_mask].max())\n",
    "#         local_window = df.loc[df_idx + 1: local_window_end, price_col].dropna()\n",
    "#         local_mean = local_window.mean()\n",
    "#         local_std = local_window.std()\n",
    "#         if local_std != 0:\n",
    "#             price_z_score = abs(price_to_check - local_mean) / local_std\n",
    "#             if price_z_score > z_threshold / 10:\n",
    "#                 corrected_outlier_mask.at[df_idx] = True\n",
    "\n",
    "#     if corrected_outlier_mask.any():\n",
    "#         # Plotting\n",
    "#         plt.figure(figsize=(10, 6))\n",
    "#         plt.plot(df['date'], df[price_col], marker='o', label='Normal')\n",
    "#         plt.scatter(df.loc[corrected_outlier_mask, 'date'],\n",
    "#                     df.loc[corrected_outlier_mask, price_col],\n",
    "#                     color='red', label='Outlier', zorder=5)\n",
    "\n",
    "#         plt.title(f\"Symbol: {symbol}\")\n",
    "#         plt.xlabel(\"Date\")\n",
    "#         plt.ylabel(price_col)\n",
    "#         plt.legend()\n",
    "#         plt.tight_layout()\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate series gap stats\n",
    "oldest = latest - pd.Timedelta(days=365 * 6)\n",
    "business_days = pd.date_range(start=oldest, end=latest, freq='B')\n",
    "\n",
    "# Calculate statistics for each DataFrame in meta\n",
    "for idx, row in tqdm(meta.iterrows(), total=len(meta)):\n",
    "    df = row['df']\n",
    "    merged = pd.merge(pd.DataFrame({'date': business_days}), df, on='date', how='left')\n",
    "    \n",
    "    # Calculate gaps\n",
    "    present = merged[price_col].notna()\n",
    "    present_idx = np.flatnonzero(present)\n",
    "    gaps = []\n",
    "    length = len(merged)\n",
    "\n",
    "    if present_idx.size > 0:\n",
    "        if present_idx[0] > 0:\n",
    "            gaps.append(present_idx[0])\n",
    "        if present_idx.size > 1:\n",
    "            internal_gaps = np.diff(present_idx) - 1\n",
    "            gaps.extend(gap for gap in internal_gaps if gap > 0)\n",
    "        if present_idx[-1] < length - 1:\n",
    "            gaps.append(length - 1 - present_idx[-1])\n",
    "    else:\n",
    "        gaps = [length]\n",
    "\n",
    "    gaps = np.array(gaps, dtype=int)\n",
    "    gaps = gaps[gaps > 0]\n",
    "    max_gap = float(gaps.max()) if gaps.size > 0 else 0.0\n",
    "    std_gap = float(gaps.std()) if gaps.size > 0 else 0.0\n",
    "    missing = length - present.sum()\n",
    "    pct_missing = missing / length\n",
    "\n",
    "    # Update meta with statistics\n",
    "    meta.at[idx, 'df'] = merged\n",
    "    meta.at[idx, 'max_gap'] = max_gap\n",
    "    meta.at[idx, 'missing'] = missing\n",
    "    meta.at[idx, 'pct_missing'] = pct_missing\n",
    "\n",
    "print(f'Latest: {latest}')\n",
    "print(f'Oldest: {oldest}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove series with large day gaps\n",
    "meta['max_gap_log'] = np.log1p(meta['max_gap'])\n",
    "meta['max_gap_log'] = meta['max_gap_log'] / meta['max_gap_log'].max()\n",
    "meta['exclusion_score'] = meta['pct_missing'] + meta['max_gap_log']\n",
    "\n",
    "condition = ((meta['max_gap_log'] < meta['max_gap_log'].mean()) & \n",
    "             (meta['pct_missing'] < meta['pct_missing'].mean()))\n",
    "filtered = meta[condition].sort_values(by='exclusion_score', ascending=False).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate/extrapolate price column and merge with fund\n",
    "for idx, row in tqdm(filtered.iterrows(), total=len(filtered)):\n",
    "    df = row['df']\n",
    "    df[price_col] = df[price_col].interpolate(method='akima', limit_direction='both')\n",
    "    if df[price_col].isna().any():\n",
    "        df[price_col] = df[price_col].ffill()\n",
    "        df[price_col] = df[price_col].bfill()\n",
    "    \n",
    "    df['pct_change'] = df[price_col].pct_change()\n",
    "    filtered.at[idx, 'df'] = df.set_index('date')\n",
    "\n",
    "filtered = pd.merge(filtered, fund_df, on=['symbol', 'currency'], how='inner').drop(['max_gap', 'missing', 'pct_missing', 'exclusion_score', 'max_gap_log'], axis=1)\n",
    "\n",
    "del meta, fund_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Delete duplicates ETF files\n",
    "# duplicates = meta[meta.duplicated(subset=['symbol', 'currency'], keep=False)].copy()\n",
    "# duplicates['not_smart'] = duplicates['exchange_api'] != 'SMART'\n",
    "\n",
    "# sorted_duplicates = duplicates.sort_values(\n",
    "#     by=['symbol', 'currency', 'length', 'not_smart'],\n",
    "#     ascending=[True, True, False, False]\n",
    "# )\n",
    "\n",
    "# rows_to_keep = sorted_duplicates.groupby(['symbol', 'currency']).head(1)\n",
    "# rows_to_delete = duplicates[~duplicates.index.isin(rows_to_keep.index)]\n",
    "# for idx, row in rows_to_delete.iterrows():\n",
    "#     file_name = f\"{row['symbol']}-{row['exchange_api']}-{row['currency']}.csv\"\n",
    "#     file_path = os.path.join(data_path, file_name)\n",
    "#     if os.path.exists(file_path):\n",
    "#         os.remove(file_path)\n",
    "#         print(f\"Deleted {file_path}\")\n",
    "#     else:\n",
    "#         print(f\"File not found: {file_path}\")\n",
    "\n",
    "# del duplicates, sorted_duplicates, rows_to_keep, rows_to_delete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot asset class portfolios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Risk-free series calculation\n",
    "import pandas as pd\n",
    "import pandas_datareader.data as web\n",
    "\n",
    "# 3-month bill/interest rate tickers (FRED/OECD) for each country\n",
    "tickers = {\n",
    "    'US': 'DTB3',\n",
    "    'Canada': 'IR3TIB01CAM156N',\n",
    "    'Germany': 'IR3TIB01DEM156N',\n",
    "    'UK': 'IR3TIB01GBM156N',\n",
    "    'France': 'IR3TIB01FRA156N',\n",
    "}\n",
    "\n",
    "# Fetch each series and convert from percentage to decimal\n",
    "bonds = {}\n",
    "failed = []\n",
    "for country, ticker in tickers.items():\n",
    "    try:\n",
    "        series = web.DataReader(ticker, 'fred', oldest, latest)\n",
    "        bonds[country] = series / 100.0\n",
    "    except Exception:\n",
    "        try:\n",
    "            series = web.DataReader(ticker, 'oecd', oldest, latest)\n",
    "            bonds[country] = series / 100.0\n",
    "        except Exception as oecd_err:\n",
    "            failed.append(country)\n",
    "\n",
    "# Combine into a single DataFrame\n",
    "df_bonds = pd.concat(bonds, axis=1)\n",
    "df_bonds.columns = [c for c in tickers if c not in failed]\n",
    "df_bonds = df_bonds.interpolate(method='akima').bfill().ffill()\n",
    "\n",
    "fred = Fred(api_key='30ae0e4e7713662116edf836cec71562')\n",
    "cpi_data = fred.get_series('CPIAUCSL', oldest, latest)\n",
    "risk_free_df = pd.concat([df_bonds.mean(axis=1).rename('nominal_rate'), cpi_data.rename('cpi')], axis=1)\n",
    "\n",
    "# Match with the other price series\n",
    "risk_free_df = risk_free_df.reindex(business_days, copy=False)\n",
    "risk_free_df = risk_free_df.interpolate(method='akima').bfill().ffill()\n",
    "\n",
    "risk_free_df['inflation_rate'] = risk_free_df['cpi'].pct_change()\n",
    "risk_free_df['daily_nominal_rate'] = risk_free_df['nominal_rate'] / 252\n",
    "# risk_free_df['real_rate'] = (1 + risk_free_df['daily_nominal_rate']) / (1 + risk_free_df['inflation_rate']) - 1\n",
    "\n",
    "print(f'Short-term bonds used from: {df_bonds.columns.to_list()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add pct_change cols to dfs and create pct_changes\n",
    "cols_to_exclude = ['conId']\n",
    "numerical_cols = [col for col in filtered.columns if filtered[col].dtype in [np.int64, np.float64] and col not in cols_to_exclude]\n",
    "\n",
    "pct_changes = pd.concat(\n",
    "        [row['df']['pct_change'].rename(row['conId']) \n",
    "        for _, row in filtered.iterrows()], axis=1\n",
    "    )\n",
    "\n",
    "# Remove uninformative cols for market portfolios \n",
    "uninformative_cols = [col for col in numerical_cols if filtered[col].nunique(dropna=True) <= 1]\n",
    "filtered = filtered.drop(columns=uninformative_cols)\n",
    "filtered = filtered.dropna(axis=1, how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add rate of change fundamentals\n",
    "def calculate_slope(value1, value2, time1, time2):\n",
    "    return (value1 - value2) / (time1 - time2)\n",
    "\n",
    "\n",
    "rate_fundamentals = [('EPSGrowth-1yr', 'EPS_growth_3yr', 'EPS_growth_5yr'),\n",
    "                     ('ReturnonAssets1Yr', 'ReturnonAssets3Yr'),\n",
    "                     ('ReturnonCapital', 'ReturnonCapital3Yr'),\n",
    "                     ('ReturnonEquity1Yr', 'ReturnonEquity3Yr'),\n",
    "                     ('ReturnonInvestment1Yr', 'ReturnonInvestment3Yr')]\n",
    "\n",
    "for cols in rate_fundamentals:\n",
    "    base_name = cols[0].replace('-1yr', '').replace('1Yr', '')\n",
    "    slope_col = f'fundamentals_{base_name}_slope'\n",
    "    \n",
    "    if len(cols) == 3:\n",
    "        col_1yr, col_3yr, col_5yr = cols\n",
    "\n",
    "        filtered[slope_col] = calculate_slope(\n",
    "            filtered[f'fundamentals_{col_1yr}'],\n",
    "            filtered[f'fundamentals_{col_5yr}'],\n",
    "            1, 5\n",
    "        )\n",
    "\n",
    "        if 'EPS' in base_name:\n",
    "            slope_1yr_3yr = calculate_slope(\n",
    "                filtered[f'fundamentals_{col_1yr}'],\n",
    "                filtered[f'fundamentals_{col_3yr}'],\n",
    "                1, 3\n",
    "            )\n",
    "            slope_3yr_5yr = calculate_slope(\n",
    "                filtered[f'fundamentals_{col_3yr}'],\n",
    "                filtered[f'fundamentals_{col_5yr}'],\n",
    "                3, 5\n",
    "            )\n",
    "            \n",
    "            second_deriv_col = f'fundamentals_{base_name}_second_deriv'\n",
    "            filtered[second_deriv_col] = calculate_slope(\n",
    "                slope_1yr_3yr,\n",
    "                slope_3yr_5yr,\n",
    "                1, 3\n",
    "            )\n",
    "    elif len(cols) == 2:\n",
    "        col_1yr, col_3yr = cols\n",
    "        filtered[slope_col] = calculate_slope(\n",
    "            filtered[f'fundamentals_{col_1yr}'],\n",
    "            filtered[f'fundamentals_{col_3yr}'],\n",
    "            1, 3\n",
    "        )\n",
    "\n",
    "# Add new cols to numericals\n",
    "numerical_cols = [col for col in filtered.columns if filtered[col].dtype in [np.int64, np.float64] and col not in cols_to_exclude]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ETF return stats and split training and tests sets\n",
    "def get_return_stats(df, training_cutoff, momentum_cutoffs, risk_free_df):\n",
    "    training_df = df[df.index < training_cutoff]\n",
    "    training_rf = risk_free_df[risk_free_df.index < training_cutoff]\n",
    "\n",
    "    excess_returns = training_df['pct_change'] - training_rf['daily_nominal_rate']\n",
    "    sharpe = excess_returns.mean() / excess_returns.std()\n",
    "\n",
    "    er = training_df['pct_change'].mean()\n",
    "    std = training_df['pct_change'].std()\n",
    "    avg_volume = training_df['volume'].mean()\n",
    "\n",
    "    momentum_3mo = training_df[training_df.index >= momentum_cutoffs['3mo']]['pct_change'].mean()\n",
    "    momentum_6mo = training_df[training_df.index >= momentum_cutoffs['6mo']]['pct_change'].mean()\n",
    "    momentum_1y  = training_df[training_df.index >= momentum_cutoffs['1y']]['pct_change'].mean()\n",
    "    momentum_3y  = training_df[training_df.index >= momentum_cutoffs['3y']]['pct_change'].mean()\n",
    "\n",
    "    return pd.Series(\n",
    "        [momentum_3mo, momentum_6mo, momentum_1y, momentum_3y, er, sharpe, std, avg_volume],\n",
    "        index=['momentum_3mo', 'momentum_6mo', 'momentum_1y', 'momentum_3y', 'stats_er', 'stats_sharpe', 'stats_std', 'stats_avg_volume']\n",
    "    )\n",
    "\n",
    "training_cutoff = latest - pd.Timedelta(days=365)\n",
    "momentum_cutoffs = {\n",
    "    '3y':  training_cutoff - pd.Timedelta(days=365 * 3),\n",
    "    '1y':  training_cutoff - pd.Timedelta(days=365),\n",
    "    '6mo': training_cutoff - pd.Timedelta(days=365 // 2),\n",
    "    '3mo': training_cutoff - pd.Timedelta(days=365 // 4),\n",
    "}\n",
    "\n",
    "# Apply to each row\n",
    "filtered[['momentum_3mo', 'momentum_6mo', 'momentum_1y', 'momentum_3y', 'stats_er', 'stats_sharpe', 'stats_std', 'stats_avg_volume']] = filtered['df'].apply(lambda df: get_return_stats(df, training_cutoff, momentum_cutoffs, risk_free_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all asset type indices/portfolios\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "holding_cols = [col for col in filtered.columns if col.startswith('holding_') and col != 'holding_types_variety'] + ['total']\n",
    "portfolio_dfs = {}\n",
    "\n",
    "for holding_col in holding_cols:\n",
    "    name = holding_col.split('_')[-1]\n",
    "    if holding_col == 'total':\n",
    "        weight = filtered['profile_cap_usd']\n",
    "    else:\n",
    "        weight = (filtered['profile_cap_usd'] * filtered[holding_col])\n",
    " \n",
    "    total_market_cap = (weight).sum()\n",
    "    filtered['weight'] = weight / total_market_cap\n",
    "    \n",
    "    weights = filtered.set_index('conId')['weight']\n",
    "    portfolio_return = pct_changes.dot(weights)\n",
    "    initial_price = 1\n",
    "    portfolio_price = initial_price * (1 + portfolio_return.fillna(0)).cumprod()\n",
    "\n",
    "    portfolio_df = pd.DataFrame({\n",
    "        'date': portfolio_price.index,\n",
    "        price_col: portfolio_price.values,\n",
    "        'pct_change': portfolio_return.values\n",
    "    }).set_index('date')\n",
    "\n",
    "    portfolio_dfs[name] = portfolio_df\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.title(f'{name.capitalize()} portfolio  -  ${format(total_market_cap, ',.0f')}')\n",
    "    plt.plot(portfolio_df.index, portfolio_df[price_col], marker='o')\n",
    "    plt.show()\n",
    "\n",
    "filtered.drop('weight', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Manual plot\n",
    "# symbol_test = 'SHV'\n",
    "# x = filtered[filtered['symbol'] == symbol_test].df.iloc[0].index\n",
    "# y = filtered[filtered['symbol'] == symbol_test].df.iloc[0]['average']#.pct_change()\n",
    "\n",
    "# y = risk_free_df['daily_nominal_rate']\n",
    "# x = risk_free_df.index\n",
    "# # y = df_bonds['UK']\n",
    "# # x = df_bonds.index\n",
    "# # y = portfolio_dfs['equity']['pct_change'] - risk_free_df['daily_nominal_rate']\n",
    "# # x = portfolio_dfs['equity'].index\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(x, y, marker='o')\n",
    "# # plt.xlim(market_portfolio_df['date'].min(), market_portfolio_df['date'].max())\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoid dummy trap\n",
    "empty_subcategories = {\n",
    "'holding_types': ['other'],\n",
    "'countries': ['Unidentified'], \n",
    "'currencies': ['<NoCurrency>'],\n",
    "'industries': ['NonClassifiedEquity', 'NotClassified-NonEquity'],\n",
    "'top10': ['OtherAssets', 'AccountsPayable','AccountsReceivable','AccountsReceivable&Pay','AdministrationFees','CustodyFees','ManagementFees','OtherAssetsandLiabilities','OtherAssetslessLiabilities', 'OtherFees','OtherLiabilities','Tax','Tax--ManagementFees'],\n",
    "'debtors': ['OTHER'],\n",
    "'maturity': ['%MaturityOther'],\n",
    "'debt_type': ['%QualityNotAvailable', '%QualityNotRated'],\n",
    "'manual': ['other']\n",
    "}\n",
    "\n",
    "dummy_trap_cols = []\n",
    "for k, lst in empty_subcategories.items():\n",
    "    for i in lst:\n",
    "        if k == 'manual':\n",
    "            dummy_trap_cols.append(i)\n",
    "        else:\n",
    "            dummy_trap_cols.append(f'{k}_{i}')\n",
    "    \n",
    "filtered = filtered.drop(columns=dummy_trap_cols, axis=1, errors='ignore')\n",
    "numerical_cols = [col for col in filtered.columns if filtered[col].dtype in [np.int64, np.float64] and col not in cols_to_exclude]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorganize columns\n",
    "columns_to_move = ['bond', 'equity', 'cash', 'other', 'tradable']\n",
    "categories = ['holding_types', 'stats', 'momentum', 'profile', 'top10', 'countries', 'fundamentals', 'industries', 'currencies', 'debtors', 'maturity', 'debt_type', 'lipper', 'dividends', 'marketcap', 'style', 'domicile', 'asset']\n",
    "\n",
    "metadata = [col for col in numerical_cols if col not in ['conId']]\n",
    "non_numerical = [col for col in filtered.columns if col not in metadata]\n",
    "\n",
    "for category in reversed(categories):\n",
    "    cat_cols = [col for col in metadata if col.startswith(category)]\n",
    "    remaining = [col for col in metadata if col not in cat_cols]\n",
    "    metadata = cat_cols + remaining\n",
    "\n",
    "new_column_order = non_numerical + metadata\n",
    "filtered = filtered[new_column_order]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_long_short_factor_returns(full_meta_df, returns_df, long_symbols, short_symbols, factor_column=None):\n",
    "    long_df = full_meta_df[full_meta_df['conId'].isin(long_symbols)].set_index('conId')\n",
    "    long_weights = long_df['profile_cap_usd'].reindex(returns_df.columns).fillna(0)\n",
    "    if long_weights.mean() == 0:\n",
    "        print(f'Long {factor_column}')\n",
    "        print(long_df.index)\n",
    "        print()\n",
    "    if factor_column:\n",
    "        factor_weights = (full_meta_df[factor_column].max() - long_df[factor_column]) / (full_meta_df[factor_column].max() - full_meta_df[factor_column].min())\n",
    "        factor_weights = factor_weights.reindex(returns_df.columns).fillna(0)\n",
    "        if factor_weights.sum() != 0:\n",
    "            long_weights *= factor_weights\n",
    "\n",
    "    long_weights /= long_weights.sum()\n",
    "    long_returns = returns_df.dot(long_weights)\n",
    "    \n",
    "    short_df = full_meta_df[full_meta_df['conId'].isin(short_symbols)].set_index('conId')\n",
    "    short_weights = short_df['profile_cap_usd'].reindex(returns_df.columns).fillna(0)\n",
    "    if short_weights.mean() == 0:\n",
    "        print(f'Short {factor_column}')\n",
    "        print(short_df.index)\n",
    "        print()\n",
    "    if factor_column:\n",
    "        factor_weights = (short_df[factor_column] - full_meta_df[factor_column].min()) / (full_meta_df[factor_column].max() - full_meta_df[factor_column].min())\n",
    "        factor_weights = factor_weights.reindex(returns_df.columns).fillna(0)\n",
    "        if factor_weights.sum() != 0:\n",
    "            short_weights *= factor_weights\n",
    "\n",
    "    short_weights /= short_weights.sum()\n",
    "    short_returns = returns_df.dot(short_weights)\n",
    "    \n",
    "    factor_returns = long_returns - short_returns\n",
    "    return factor_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct all factors from metadata + a few custom factors\n",
    "\n",
    "differences = []\n",
    "def construct_factors(filtered, pct_changes, portfolio_dfs, risk_free_df, scaling_factor=0.5):\n",
    "    factors = {}\n",
    "    # Market risk premium\n",
    "    factors['market_premium'] = (portfolio_dfs['equity']['pct_change'] - risk_free_df['daily_nominal_rate'])\n",
    "\n",
    "    # SMB_ETF\n",
    "    small_symbols = filtered[filtered['marketcap_small'] == 1]['conId'].tolist()\n",
    "    large_symbols = filtered[filtered['marketcap_large'] == 1]['conId'].tolist()\n",
    "\n",
    "    intersection = set(small_symbols) & set(large_symbols)\n",
    "    small_symbols = [s for s in small_symbols if s not in intersection]\n",
    "    large_symbols = [s for s in large_symbols if s not in intersection]\n",
    "    smb_etf = construct_long_short_factor_returns(filtered, pct_changes, small_symbols, large_symbols)\n",
    "    factors['smb'] = smb_etf\n",
    "\n",
    "    differences.append(np.abs(len(small_symbols) - len(large_symbols)))\n",
    "\n",
    "    # HML_ETF\n",
    "    value_cols = [col for col in filtered.columns if col.startswith('style_') and col.endswith('value')]\n",
    "    growth_cols = [col for col in filtered.columns if col.startswith('style_') and col.endswith('growth')]\n",
    "    value_symbols = filtered[filtered[value_cols].ne(0).any(axis=1)]['conId'].tolist()\n",
    "    growth_symbols = filtered[filtered[growth_cols].ne(0).any(axis=1)]['conId'].tolist()\n",
    "\n",
    "    intersection = set(value_symbols) & set(growth_symbols)\n",
    "    value_symbols = [s for s in value_symbols if s not in intersection]\n",
    "    growth_symbols = [s for s in growth_symbols if s not in intersection]\n",
    "    hml_etf = construct_long_short_factor_returns(filtered, pct_changes, value_symbols, growth_symbols)\n",
    "    factors['hml'] = hml_etf\n",
    "\n",
    "    differences.append(np.abs(len(value_symbols) - len(growth_symbols)))\n",
    "\n",
    "    # Metadata\n",
    "    metadata = [col for col in numerical_cols if col not in ['conId']]\n",
    "    for col in numerical_cols:\n",
    "        std = filtered[col].std()\n",
    "        mean = filtered[col].mean()\n",
    "\n",
    "        upper_boundary = min(filtered[col].max(), mean + (scaling_factor * std))\n",
    "        lower_boundary = max(filtered[col].min(), mean - (scaling_factor * std))\n",
    "\n",
    "        low_factor_symbols = filtered[filtered[col] <= lower_boundary]['conId'].tolist()\n",
    "        high_factor_symbols = filtered[filtered[col] >= upper_boundary]['conId'].tolist()\n",
    "\n",
    "        var_etf = construct_long_short_factor_returns(filtered, pct_changes, low_factor_symbols, high_factor_symbols, factor_column=col)\n",
    "        factors[col] = var_etf\n",
    "\n",
    "        differences.append(np.abs(len(low_factor_symbols) - len(high_factor_symbols)))\n",
    "\n",
    "    return pd.DataFrame(factors)#.fillna(0)\n",
    "\n",
    "factors_df = construct_factors(filtered, pct_changes, portfolio_dfs, risk_free_df, scaling_factor=0.5) # Optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prescreen_factors(factors_df, correlation_threshold=0.80):\n",
    "    if factors_df is None or factors_df.empty or factors_df.shape[1] == 0:\n",
    "        raise ValueError(\"factors_df must be a non-empty DataFrame with at least one column.\")\n",
    "    temp_factors_df = factors_df.copy()\n",
    "\n",
    "    corr_matrix = temp_factors_df.corr().abs()\n",
    "    corr_pairs = corr_matrix.where(np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)).stack()\n",
    "    corr_pairs = corr_pairs.sort_values(ascending=False)\n",
    "\n",
    "    drop_map = {}\n",
    "    col_order = list(temp_factors_df.columns)\n",
    "    for (col1, col2), corr_val in corr_pairs.items():\n",
    "        if corr_val < correlation_threshold:\n",
    "            break\n",
    "\n",
    "        already_dropped = {c for drops in drop_map.values() for c in drops}\n",
    "        if col1 in already_dropped or col2 in already_dropped:\n",
    "            continue\n",
    "\n",
    "        if col_order.index(col1) < col_order.index(col2):\n",
    "            keeper, to_drop = col1, col2\n",
    "        else:\n",
    "            keeper, to_drop = col2, col1\n",
    "\n",
    "        drop_map.setdefault(keeper, []).append(to_drop)\n",
    "\n",
    "    # Merge drop results\n",
    "    cols_to_drop = set(col for drops in drop_map.values() for col in drops)\n",
    "    final_drop_map = {}\n",
    "    for keeper, direct_drops in drop_map.items():\n",
    "        if keeper not in cols_to_drop:\n",
    "            cols_to_check = list(direct_drops) \n",
    "            all_related_drops = set(direct_drops)\n",
    "            while cols_to_check:\n",
    "                col = cols_to_check.pop(0)\n",
    "                if col in drop_map:\n",
    "                    new_drops = [d for d in drop_map[col] if d not in all_related_drops]\n",
    "                    cols_to_check.extend(new_drops)\n",
    "                    all_related_drops.update(new_drops)\n",
    "            \n",
    "            final_drop_map[keeper] = sorted(list(all_related_drops))\n",
    "\n",
    "    temp_factors_df = temp_factors_df.drop(columns=cols_to_drop)\n",
    "    return temp_factors_df, final_drop_map\n",
    "\n",
    "distilled_factors, col_map = prescreen_factors(factors_df, correlation_threshold=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "def calculate_vif(df):\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"feature\"] = df.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]\n",
    "    return vif_data.sort_values(by='VIF', ascending=False)\n",
    "\n",
    "    \n",
    "# max_vif_threshold = 99999\n",
    "# while True:\n",
    "#     vif_df = calculate_vif(distilled_factors.fillna(0))\n",
    "#     highest_vif = vif_df['VIF'].iloc[0]\n",
    "#     if highest_vif > max_vif_threshold and distilled_factors.shape[1] > 2:\n",
    "#         feature_to_drop = vif_df['feature'].iloc[0]\n",
    "#         distilled_factors.drop(columns=[feature_to_drop], inplace=True)\n",
    "#         cols_dropped.add(feature_to_drop)\n",
    "#         print(f'{feature_to_drop} - {highest_vif}')\n",
    "#     else:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scales = {}\n",
    "z_range = np.arange(0,2,0.05)\n",
    "for z in tqdm(z_range, total=len(z_range)):\n",
    "    factors_df = construct_factors(filtered, pct_changes, portfolio_dfs, risk_free_df, scaling_factor=z)\n",
    "    distilled_factors, _ = prescreen_factors(factors_df, correlation_threshold=0.99)\n",
    "\n",
    "    vif_df = calculate_vif(distilled_factors.fillna(0))\n",
    "    scales[z] = vif_df['VIF'].iloc[0]\n",
    "\n",
    "x = [z for z in scales]\n",
    "y = [vif for vif in scales.values()]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, y, marker='o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLS Regression function\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def run_regressions(distilled_factors):\n",
    "    results = []\n",
    "    for symbol in tqdm(pct_changes.columns, desc=\"Running Regressions\"):\n",
    "        etf_excess = pct_changes[symbol] - risk_free_df['daily_nominal_rate']\n",
    "        data = pd.concat([etf_excess.rename('etf_excess'), distilled_factors], axis=1).dropna()\n",
    "\n",
    "        Y = data['etf_excess']\n",
    "        X = sm.add_constant(data.iloc[:, 1:])\n",
    "        model = sm.OLS(Y, X).fit()\n",
    "        result = {\n",
    "            'conId': symbol,\n",
    "            'nobs': model.nobs,\n",
    "            'r_squared': model.rsquared,\n",
    "            'r_squared_adj': model.rsquared_adj,\n",
    "            'f_statistic': model.fvalue,\n",
    "            'f_pvalue': model.f_pvalue,\n",
    "            'aic': model.aic,\n",
    "            'bic': model.bic,\n",
    "            'condition_number': model.condition_number,\n",
    "            'alpha': model.params['const'],\n",
    "            'alpha_pval': model.pvalues['const'],\n",
    "            'alpha_tval': model.tvalues['const'],\n",
    "            'alpha_bse': model.bse['const'],\n",
    "        }\n",
    "        for factor in distilled_factors.columns:\n",
    "            result[f'beta_{factor}'] = model.params[factor]\n",
    "            result[f'pval_beta_{factor}'] = model.pvalues[factor]\n",
    "            result[f'tval_beta_{factor}'] = model.tvalues[factor]\n",
    "            result[f'bse_beta_{factor}'] = model.bse[factor]\n",
    "        results.append(result)\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df\n",
    "    # del X, Y, model, data, etf_excess, result, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCR\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def run_pcr(factors_df,\n",
    "            pct_changes,\n",
    "            risk_free_df,\n",
    "            cv=5,\n",
    "            random_state=42):\n",
    "\n",
    "    data = factors_df.copy().fillna(0)\n",
    "    X = data[factors_df.columns]\n",
    "    n_samples, n_factors = X.shape\n",
    "    search_limit = min(n_factors, int(n_samples * (1 - 1/cv)) - 1)\n",
    "\n",
    "    summary_rows = []\n",
    "    pcr_cv_scores = {}\n",
    "    neg_mse_scorer = make_scorer(mean_squared_error, greater_is_better=False) # GridSearchCV maximizes the score, so we use negative MSE\n",
    "    for etf in tqdm(pct_changes.columns, desc=\"PCR Regression\"):\n",
    "        Y = data[etf].values - risk_free_df['daily_nominal_rate'].values\n",
    "\n",
    "        pipeline = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('pca', PCA(random_state=random_state)),\n",
    "            ('reg', LinearRegression())\n",
    "        ])\n",
    "\n",
    "        param_grid = {\n",
    "            'pca__n_components': list(range(1, search_limit + 1))\n",
    "        }\n",
    "\n",
    "        grid_search = GridSearchCV(estimator=pipeline,\n",
    "                                   param_grid=param_grid,\n",
    "                                   scoring=neg_mse_scorer,\n",
    "                                   cv=cv,\n",
    "                                   n_jobs=-1)\n",
    "\n",
    "        try:\n",
    "            grid_search.fit(X.values, Y)\n",
    "\n",
    "            best_pipeline = grid_search.best_estimator_\n",
    "            best_n_components = grid_search.best_params_['pca__n_components']\n",
    "            intercept = best_pipeline.named_steps['reg'].intercept_\n",
    "            pc_coeffs = best_pipeline.named_steps['reg'].coef_\n",
    "\n",
    "            pca_loadings = best_pipeline.named_steps['pca'].components_\n",
    "\n",
    "            # Calculate effective coefficients on the original factors\n",
    "            effective_betas_scaled = pca_loadings.T @ pc_coeffs\n",
    "            pipeline_scaler = best_pipeline.named_steps['scaler']\n",
    "            effective_betas_unscaled = effective_betas_scaled / pipeline_scaler.scale_\n",
    "\n",
    "            # Store CV results for this asset\n",
    "            cv_results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "            mean_mse_scores = -cv_results_df.set_index(cv_results_df['param_pca__n_components'])['mean_test_score']\n",
    "            pcr_cv_scores[etf] = mean_mse_scores\n",
    "            best_cv_mse = mean_mse_scores.loc[best_n_components]\n",
    "\n",
    "            row = {\n",
    "                'conId': etf,\n",
    "                'intercept': intercept,\n",
    "                'best_n_components': best_n_components,\n",
    "                'cv_mse_mean': best_cv_mse\n",
    "            }\n",
    "\n",
    "            for coef, fname in zip(effective_betas_unscaled, factors_df.columns):\n",
    "                row[f'coef_{fname}'] = coef\n",
    "\n",
    "            summary_rows.append(row)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {etf} due to fitting error: {e}\")\n",
    "            continue\n",
    "\n",
    "    summary_df = pd.DataFrame(summary_rows)\n",
    "    return summary_df, pcr_cv_scores\n",
    "\n",
    "# results_pcr, pcr_cv_stats = run_pcr(\n",
    "#     factors_df=distilled_factors,\n",
    "#     pct_changes=pct_changes,\n",
    "#     risk_free_df=risk_free_df,\n",
    "#     cv=4\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics[0]['l1_ratio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ElasticNet regression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "metrics = []\n",
    "\n",
    "def run_elastic_net(factors_df,\n",
    "                    pct_changes,\n",
    "                    risk_free_df,\n",
    "                    training_cutoff,\n",
    "                    alphas=np.logspace(-4, 1, 50),\n",
    "                    l1_ratio=[.1, .5, .9],\n",
    "                    cv=5,\n",
    "                    random_state=42):\n",
    "\n",
    "    global metrics\n",
    "\n",
    "    data = data = (\n",
    "        factors_df.copy()\n",
    "        .join(pct_changes, how='inner')\n",
    "        .join(risk_free_df[['daily_nominal_rate']], how='inner')\n",
    "        .fillna(0)\n",
    "    )\n",
    "\n",
    "    train = data[data.index < training_cutoff]\n",
    "    test = data[data.index >= training_cutoff]\n",
    "\n",
    "    X_train = train[factors_df.columns].values\n",
    "    X_test = test[factors_df.columns].values\n",
    "    \n",
    "    # metrics = []\n",
    "    for etf in tqdm(pct_changes.columns, desc=\"Elastic Net Regression\"):\n",
    "        Y_train = train[etf].values - train['daily_nominal_rate'].values\n",
    "        Y_test = test[etf].values - test['daily_nominal_rate'].values\n",
    "\n",
    "        pipeline = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('enet', ElasticNetCV(alphas=alphas,\n",
    "                             l1_ratio=l1_ratio,\n",
    "                             cv=cv,\n",
    "                             random_state=random_state,\n",
    "                             max_iter=499999,\n",
    "                            #  tol=5e-5,\n",
    "                             fit_intercept=True,\n",
    "                             n_jobs=-1)),\n",
    "        ])\n",
    "\n",
    "        try:\n",
    "            pipeline.fit(X_train, Y_train)\n",
    "        except ValueError as e:\n",
    "            print(f\"Skipping {etf} due to error: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Unscale coefficients and intercept\n",
    "        enet = pipeline.named_steps['enet']\n",
    "        scaler = pipeline.named_steps['scaler']\n",
    "        betas_train = enet.coef_ / scaler.scale_\n",
    "        intercept = enet.intercept_ - np.dot(betas_train, scaler.mean_)\n",
    "\n",
    "        # out-of-sample stats\n",
    "        er_test = pipeline.predict(X_test)\n",
    "\n",
    "        # in-sample stats\n",
    "        er_train = pipeline.predict(X_train)\n",
    "\n",
    "        row = {\n",
    "            'conId': etf,\n",
    "            'alpha': intercept,\n",
    "            'enet_alpha': enet.alpha_,\n",
    "            'l1_ratio': enet.l1_ratio_,\n",
    "            'n_iter': enet.n_iter_,\n",
    "            'dual_gap': enet.dual_gap_,\n",
    "            'n_nonzero': np.sum(np.abs(betas_train) > 1e-6),\n",
    "            'mse_path_grid': enet.mse_path_,\n",
    "            'cv_mse_best': np.min(enet.mse_path_.mean(axis=2)),\n",
    "            'cv_mse_average': np.mean(enet.mse_path_.mean(axis=2)),\n",
    "            'cv_mse_worst': np.max(enet.mse_path_.mean(axis=2)),\n",
    "            'mse_test' : mean_squared_error(Y_test, er_test),\n",
    "            'mse_train' : mean_squared_error(Y_train, er_train),\n",
    "            'r2_test' : r2_score(Y_test, er_test),\n",
    "            'r2_train' : r2_score(Y_train, er_train),\n",
    "        }\n",
    "\n",
    "        # Map back coefficients to factor names\n",
    "        for coef, fname in zip(betas_train, factors_df.columns):\n",
    "            row[f'beta_{fname}'] = coef\n",
    "\n",
    "        metrics.append(row)\n",
    "    \n",
    "    results_df = pd.DataFrame(metrics).set_index('conId')\n",
    "    return results_df\n",
    "\n",
    "results_df = run_elastic_net(\n",
    "    factors_df=distilled_factors,\n",
    "    pct_changes=pct_changes,\n",
    "    risk_free_df=risk_free_df,\n",
    "    training_cutoff=training_cutoff,\n",
    "    alphas=np.logspace(-4, 1, 50),\n",
    "    l1_ratio=[0.3, 0.5, 0.7, 0.9, 0.95, 1],\n",
    "    cv=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute factor premia and expected returns\n",
    "factor_premia = distilled_factors.mean()\n",
    "\n",
    "betas = (\n",
    "    results_enet\n",
    "    # results_pcr\n",
    "    .set_index('conId')\n",
    "    .filter(like='coef_')\n",
    "    .rename(columns=lambda c: c.replace('coef_', ''))\n",
    ")\n",
    "betas = betas[distilled_factors.columns]\n",
    "\n",
    "expected_excess = betas.dot(factor_premia) + results_enet.set_index('conId')['intercept']\n",
    "expected_excess.name = 'expected_excess_return'\n",
    "\n",
    "expected_returns_df = pd.concat([results_enet.set_index('conId'), expected_excess], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs = {}\n",
    "for i in range(90,97,2):\n",
    "    start = datetime.now()\n",
    "    distilled_factors = prescreen_factors(filtered, factors_df, correlation_threshold=i/100).fillna(0)\n",
    "\n",
    "    max_vif_threshold = 2\n",
    "    while True:\n",
    "        vif_df = calculate_vif(distilled_factors)\n",
    "        highest_vif = vif_df['VIF'].iloc[0]\n",
    "        if highest_vif > max_vif_threshold and distilled_factors.shape[1] > 2:\n",
    "            feature_to_drop = vif_df['feature'].iloc[0]\n",
    "            distilled_factors.drop(columns=[feature_to_drop], inplace=True)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    results_df = run_regressions(distilled_factors)\n",
    "    corrs[i] = (distilled_factors, results_df, (datetime.now() - start).seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "stats = []\n",
    "\n",
    "for k, v in copy.items():\n",
    "    factors, results, time = v\n",
    "    entry = {\n",
    "        'k': float(f\"0.{k}\"),\n",
    "        'time': time,\n",
    "        'factor_count': factors.shape[-1],\n",
    "        'r_squared': results.r_squared.mean(),\n",
    "        'r_squared_adj': results.r_squared_adj.mean(),\n",
    "        'f_statistic': results.f_statistic.mean(),\n",
    "        'f_pvalue': results.f_pvalue.mean(),\n",
    "        'aic': results.aic.mean(),\n",
    "        'bic': results.bic.mean(),\n",
    "        'condition_number': results.condition_number.mean(),\n",
    "        'alpha': results.alpha.mean(),\n",
    "        'alpha_pval': results.alpha_pval.mean(),\n",
    "        'alpha_tval': results.alpha_tval.mean(),\n",
    "    }\n",
    "\n",
    "    pval_cols = [col for col in results.columns if col.startswith('pval')]\n",
    "    entry['beta_pvals_mean'] = results[pval_cols].mean().mean()\n",
    "    entry['beta_pvals_std'] = results[pval_cols].mean().std()\n",
    "    entry['beta_pvals_min'] = results[pval_cols].mean().min()\n",
    "    entry['beta_pvals_median'] = results[pval_cols].mean().median()\n",
    "    entry['beta_pvals_max'] = results[pval_cols].mean().max()\n",
    "\n",
    "    tval_cols = [col for col in results.columns if col.startswith('tval')]\n",
    "    tvals_abs = results[tval_cols].abs().mean()\n",
    "    entry['beta_tvals_mean'] = tvals_abs.mean()\n",
    "    entry['beta_tvals_std'] = tvals_abs.std()\n",
    "    entry['beta_tvals_min'] = tvals_abs.min()\n",
    "    entry['beta_tvals_median'] = tvals_abs.median()\n",
    "    entry['beta_tvals_max'] = tvals_abs.max()\n",
    "\n",
    "    stats.append(entry)\n",
    "\n",
    "df_stats = pd.DataFrame(stats).sort_values('k')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(style='whitegrid', context='notebook')\n",
    "\n",
    "metrics_to_plot = [\n",
    "    'time', 'factor_count', 'r_squared', 'r_squared_adj', 'f_statistic',\n",
    "    'f_pvalue', 'aic', 'bic', 'condition_number', 'alpha', 'alpha_pval', 'alpha_tval',\n",
    "    'beta_pvals_mean', 'beta_pvals_std', 'beta_pvals_min', 'beta_pvals_median', 'beta_pvals_max',\n",
    "    'beta_tvals_mean', 'beta_tvals_std', 'beta_tvals_min', 'beta_tvals_median', 'beta_tvals_max'\n",
    "]\n",
    "\n",
    "# Create subplots\n",
    "ncols = 3\n",
    "nrows = -(-len(metrics_to_plot) // ncols)  # Ceiling division\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(18, 5 * nrows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, metric in zip(axes, metrics_to_plot):\n",
    "    sns.lineplot(data=df_stats, x='k', y=metric, marker='o', ax=ax)\n",
    "    ax.set_title(metric)\n",
    "    ax.set_xlabel('Correlation threshold (k)')\n",
    "    ax.set_ylabel(metric)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "high r2 => proportion of volatility explained by factors\n",
    "low factor pval => high stat significance\n",
    "beta == 1 => equal movement/sensitivity to benchmark\n",
    "beta > 1 => higher sensitivity\n",
    "beta < 1 => lower sensitivity\n",
    "beta < 0 => inverse sensitivity\n",
    "aic lower => better overall fit\n",
    "bic lower => better overall fit\n",
    "condition number => indicates presence of multicollinearity\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_premia = factors_df.mean() * 252\n",
    "annual_risk_free_rate = risk_free_df['daily_nominal_rate'].mean() * 252\n",
    "\n",
    "try:\n",
    "    results_df = results_df.set_index('conId')\n",
    "except Exception:\n",
    "    pass\n",
    "factor_names = factor_premia.index\n",
    "\n",
    "# Calculate expected returns for each ETF\n",
    "expected_returns = {}\n",
    "for symbol in results_df.index:\n",
    "    # The regression was on excess returns, so the intercept (alpha) is already excess of Rf\n",
    "    # However, standard practice is to add alpha to the factor-based return.\n",
    "    # E[Ri - Rf] = alpha + sum(beta * lambda) => E[Ri] = Rf + alpha + sum(beta*lambda)\n",
    "    \n",
    "    annual_alpha = results_df.loc[symbol, 'alpha'] * 252\n",
    "    if results_df.loc[symbol, 'alpha_pval'] > 0.05:\n",
    "        # If alpha is not statistically significant, it's safer to treat it as zero\n",
    "        annual_alpha = 0\n",
    "\n",
    "    factor_component = 0\n",
    "    for factor in factor_names:\n",
    "        beta = results_df.loc[symbol, f'beta_{factor}']\n",
    "        # The factor_premia is already annualized\n",
    "        lambda_j = factor_premia[factor]\n",
    "        factor_component += beta * lambda_j\n",
    "        \n",
    "    expected_returns[symbol] = annual_risk_free_rate + annual_alpha + factor_component\n",
    "\n",
    "# Convert to a pandas Series\n",
    "expected_returns_series = pd.Series(expected_returns, name='expected_annual_return')\n",
    "\n",
    "print(expected_returns_series.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual plot series\n",
    "con_id = 265109869.0\n",
    "df = filtered[filtered['conId'] == con_id]['df'].iloc[0].copy()\n",
    "display(filtered[filtered['conId'] == con_id])\n",
    "\n",
    "# Step 5: Forward fill missing values (optional, adjust as needed)\n",
    "# df[price_col] = df[price_col].fillna(0)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df.index, df[price_col], marker='o')\n",
    "plt.ylim(0, df[price_col].max()*1.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old brownian interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# def brownian_bridge(t, t0, t1, x0, x1, sigma):\n",
    "#     \"\"\"Generate points using Brownian bridge between (t0, x0) and (t1, x1).\"\"\"\n",
    "#     dt = t1 - t0\n",
    "#     mu = x0 + (x1 - x0) * (t - t0) / dt  # Linear interpolation for mean\n",
    "#     variance = sigma**2 * (t1 - t) * (t - t0) / dt\n",
    "#     return mu + np.random.normal(0, np.sqrt(variance))\n",
    "\n",
    "# # Example\n",
    "# t = np.linspace(0, 10, 11)  # Original time points\n",
    "# prices = np.random.normal(100, 5, len(t))  # Simulated price series\n",
    "# sigma = np.std(prices)  # Variance of the series\n",
    "\n",
    "# # Interpolate to finer grid\n",
    "# t_new = np.linspace(0, 10, 21)  # New time points\n",
    "# prices_new = np.zeros(len(t_new))\n",
    "\n",
    "# # Copy original points and interpolate gaps\n",
    "# for i in range(len(t) - 1):\n",
    "#     idx = np.where((t_new >= t[i]) & (t_new <= t[i+1]))[0]\n",
    "#     for j in idx:\n",
    "#         prices_new[j] = brownian_bridge(t_new[j], t[i], t[i+1], prices[i], prices[i+1], sigma)\n",
    "\n",
    "# # Verify variance\n",
    "# print(\"Original variance:\", np.var(prices))\n",
    "# print(\"Interpolated variance:\", np.var(prices_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from scipy.interpolate import CubicSpline\n",
    "\n",
    "# def brownian_bridge(t, t0, t1, x0, x1, sigma):\n",
    "#     \"\"\"Generate points using Brownian bridge between (t0, x0) and (t1, x1).\"\"\"\n",
    "#     dt = t1 - t0\n",
    "#     mu = x0 + (x1 - x0) * (t - t0) / dt  # Linear interpolation for mean\n",
    "#     variance = sigma**2 * (t1 - t) * (t - t0) / dt\n",
    "#     return mu + np.random.normal(0, np.sqrt(variance))\n",
    "\n",
    "# # Generate original price series\n",
    "# t = np.linspace(0, 10, 11)  # Original time points\n",
    "# prices = np.random.normal(100, 5, len(t))  # Simulated price series\n",
    "# sigma = np.std(prices)  # Standard deviation for Brownian bridge\n",
    "\n",
    "# # Interpolate to finer grid\n",
    "# t_new = np.linspace(0, 10, 21)  # New time points\n",
    "# prices_new = np.zeros(len(t_new))  # Brownian bridge interpolation\n",
    "# prices_lin = np.zeros(len(t_new))  # Linear interpolation\n",
    "# prices_spl = np.zeros(len(t_new))  # Spline interpolation\n",
    "\n",
    "# # Brownian bridge interpolation\n",
    "# for i in range(len(t) - 1):\n",
    "#     idx = np.where((t_new >= t[i]) & (t_new <= t[i+1]))[0]\n",
    "#     for j in idx:\n",
    "#         prices_new[j] = brownian_bridge(t_new[j], t[i], t[i+1], prices[i], prices[i+1], sigma)\n",
    "\n",
    "# # Linear interpolation\n",
    "# prices_lin = np.interp(t_new, t, prices)\n",
    "\n",
    "# # Spline interpolation\n",
    "# spline = CubicSpline(t, prices)\n",
    "# prices_spl = spline(t_new)\n",
    "\n",
    "# # Verify variances\n",
    "# print(\"Original variance:\", np.var(prices))\n",
    "# print(\"Brownian bridge variance:\", np.var(prices_new))\n",
    "# print(\"Linear interpolation variance:\", np.var(prices_lin))\n",
    "# print(\"Spline interpolation variance:\", np.var(prices_spl))\n",
    "\n",
    "# # Plotting\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(t, prices, 'o-', label='Original Prices', markersize=8)\n",
    "# plt.plot(t_new, prices_new, 'x-', label='Brownian Bridge Interpolation')\n",
    "# plt.plot(t_new, prices_lin, 's-', label='Linear Interpolation')\n",
    "# plt.plot(t_new, prices_spl, 'd-', label='Spline Interpolation')\n",
    "# plt.title('Price Series Interpolation Comparison')\n",
    "# plt.xlabel('Time')\n",
    "# plt.ylabel('Price')\n",
    "# plt.grid(True)\n",
    "# plt.legend()\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Graph correlations\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# numerical_cols = [col for col in filtered.columns if filtered[col].dtype in [np.int64, np.float64] and col not in cols_to_exclude]\n",
    "\n",
    "# # drop columns with missing values\n",
    "# corr_df = filtered[numerical_cols].corr()\n",
    "# # corr_df.dropna(axis=1, how='all', inplace=True)\n",
    "# # corr_df.dropna(axis=0, how='all', inplace=True)\n",
    "\n",
    "# plt.figure(figsize=(50, 50))\n",
    "# sns.heatmap(corr_df, cmap='coolwarm')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Prep historical data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop\n",
    "# # Test day gap\n",
    "# dfs = {}\n",
    "# for file in os.listdir(data_path):\n",
    "#     symbol = os.path.splitext(file)[0].split('-')[0]\n",
    "#     if symbol in verified_files:\n",
    "#         dfs[symbol] = pd.read_csv(data_path + file)\n",
    "\n",
    "# days, nums, lens, firsts = [], [], [], []\n",
    "# for day in range(5,30):\n",
    "#     days.append(day)\n",
    "\n",
    "#     melted_dfs = []\n",
    "#     expected_returns = {}\n",
    "#     for symbol, df in tqdm(dfs.items(), total=len(dfs), desc=f'{day}'):\n",
    "#         df = melt(df)\n",
    "#         df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "#         latest_date = df['date'].iloc[-1]\n",
    "#         earliest_date = df['date'].iloc[0]\n",
    "#         length_required = pd.to_datetime('2020-02-01')\n",
    "#         month_ago = datetime.today() - timedelta(days=30)\n",
    "\n",
    "#         dates = df['date'].unique()\n",
    "#         date_diffs = dates[1:] - dates[:-1]\n",
    "        \n",
    "#         if latest_date >= month_ago and earliest_date <= length_required and not (date_diffs > pd.Timedelta(days=day)).any():\n",
    "#             df['symbol'] = symbol\n",
    "#             df['pct_change'] = df['value'].pct_change()\n",
    "#             expected_returns[symbol] = df['pct_change'].mean()\n",
    "#             melted_dfs.append(df)\n",
    "#     # print(f'Loaded {len(melted_dfs)} out of {len(file_list)} series ({round(len(melted_dfs)/len(file_list)*100, 4)}%)')\n",
    "\n",
    "#     # Concatenate and pivot data\n",
    "#     returns_df = pd.concat(melted_dfs, ignore_index=True)\n",
    "#     returns_df = returns_df.pivot(index=['date', 'kind'], columns='symbol', values='pct_change')\n",
    "#     returns_df = returns_df.sort_values(by=['date', 'kind'], ascending=[True, False]).reset_index().dropna()\n",
    "#     lens.append(len(returns_df))\n",
    "#     nums.append(len(returns_df.columns))\n",
    "#     firsts.append(returns_df.date.iloc[0])\n",
    "\n",
    "# gap_data_df = pd.DataFrame({\n",
    "#     'day_gap': days,\n",
    "#     'num_etfs': nums,\n",
    "#     'period_length': lens,\n",
    "#     'first_day':firsts})\n",
    "\n",
    "# gap_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare historical training data\n",
    "# def melt(data_df, value_columns=None):\n",
    "#     if not value_columns:\n",
    "#         value_columns = ['open', 'close']\n",
    "#     id_columns = [col for col in data_df.columns.to_list() if col not in value_columns]\n",
    "#     melted_df = data_df.melt(id_vars=id_columns, value_vars=value_columns, var_name='kind', value_name='value')\n",
    "#     return melted_df.sort_values(by=['date'], ascending=[True, False]).reset_index(drop=True)\n",
    "\n",
    "# Load historical data and merge them all into one df\n",
    "dfs = {}\n",
    "file_list = os.listdir(data_path)\n",
    "for file in file_list:\n",
    "    symbol = os.path.splitext(file)[0].split('-')[0]\n",
    "    if symbol in verified_files:\n",
    "        dfs[symbol] = pd.read_csv(data_path + file)\n",
    "\n",
    "\n",
    "# Melt dfs, filters, and calc pct_change. ASSUMES that dfs are sorted chronologically\n",
    "training_start_date = pd.to_datetime('2020-02-01')\n",
    "month_ago = datetime.today() - timedelta(days=31)\n",
    "\n",
    "day_gap = 6 # SET ACCEPTABLE DAY GAP\n",
    "\n",
    "melted_dfs, expected_returns = [], {}\n",
    "for symbol, df in tqdm(dfs.items(), total=len(dfs), desc=f'Filtering {kind} dfs'):\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "    latest_date = df['date'].iloc[-1]\n",
    "    earliest_date = df['date'].iloc[0]\n",
    "    dates = df['date'].unique()\n",
    "    date_gaps = dates[1:] - dates[:-1]\n",
    "    \n",
    "    if (kind == 'indices') or (latest_date >= month_ago and earliest_date <= training_start_date and (date_gaps <= pd.Timedelta(days=day_gap)).all()):\n",
    "        df['symbol'] = symbol\n",
    "        df['pct_change'] = df['average'].pct_change()\n",
    "        expected_returns[symbol] = df['pct_change'].mean()\n",
    "        melted_dfs.append(df)\n",
    "print(f'Loaded {len(melted_dfs)} out of {len(file_list)} series ({round(len(melted_dfs)/len(file_list)*100, 4)}%)')\n",
    "\n",
    "# Concatenate and pivot data\n",
    "returns_df = pd.concat(melted_dfs, ignore_index=True)\n",
    "returns_df = returns_df.pivot(index=['date'], columns='symbol', values='pct_change')\n",
    "returns_df = returns_df.sort_values(by=['date'], ascending=[True]).reset_index()\n",
    "\n",
    "# Define training boundaries\n",
    "training_cutoff_date = datetime.today() - timedelta(days=365)\n",
    "training_df = returns_df[returns_df['date'] <= training_cutoff_date]\n",
    "training_matrix = training_df.drop(['date'], axis=1).dropna().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate risk-free-rate for training window\n",
    "treasury_rate = web.DataReader('DGS10', 'fred', training_cutoff_date-timedelta(days=365), training_cutoff_date)\n",
    "nominal_rf_rate = treasury_rate.mean() / 100\n",
    "\n",
    "fred = Fred(api_key='30ae0e4e7713662116edf836cec71562')\n",
    "cpi_data = fred.get_series('CPIAUCSL', training_cutoff_date-timedelta(days=365), training_cutoff_date) # CPI\n",
    "inflation_rate = (cpi_data.iloc[-1] - cpi_data.iloc[0]) / cpi_data.iloc[0]\n",
    "\n",
    "real_rf_rate = (1 + nominal_rf_rate) / (1 + inflation_rate) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate corr and cov for historical training data\n",
    "training_array = training_matrix.values # Convert training matrix to numpy array\n",
    "symbol_list = training_matrix.columns.tolist()\n",
    "num_symbols = len(symbol_list)\n",
    "corr_matrix = np.zeros((num_symbols, num_symbols)) # Pre-allocate numpy array for correlation\n",
    "cov_matrix = np.zeros((num_symbols, num_symbols))  # Pre-allocate numpy array for covariance\n",
    "\n",
    "for i, sym_i in tqdm(enumerate(symbol_list), total=num_symbols, desc=f\"Calculating distance stats sqr\"):\n",
    "    for j, sym_j in enumerate(symbol_list):\n",
    "        if i <= j:  # Compute only for upper triangle (including diagonal)\n",
    "            stats = dcor.distance_stats(training_array[:, i], training_array[:, j])\n",
    "            corr_value = stats.correlation_xy\n",
    "            cov_value = stats.covariance_xy\n",
    "\n",
    "            corr_matrix[i, j] = corr_value\n",
    "            corr_matrix[j, i] = corr_value  # Fill symmetric value\n",
    "\n",
    "            cov_matrix[i, j] = cov_value\n",
    "            cov_matrix[j, i] = cov_value  # Fill symmetric value\n",
    "\n",
    "corr_df = pd.DataFrame(corr_matrix, index=symbol_list, columns=symbol_list) # Convert numpy array back to df for output\n",
    "cov_df = pd.DataFrame(cov_matrix, index=symbol_list, columns=symbol_list)   # Convert numpy array back to df for output\n",
    "\n",
    "corr_df.to_csv(f'{root}corr_df.csv', index=False)\n",
    "cov_df.to_csv(f'{root}cov_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Compute etf combinations based on optimal k_clusters\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load corr and cov\n",
    "corr_df = pd.read_csv(f'{root}corr_df.csv')\n",
    "cov_df = pd.read_csv(f'{root}cov_df.csv')\n",
    "symbol_list = corr_df.columns\n",
    "\n",
    "symbol2index = dict(zip(corr_df.columns, corr_df.index))\n",
    "index2symbol = dict(zip(corr_df.index, corr_df.columns))\n",
    "corr_df.rename(columns=symbol2index, inplace=True)\n",
    "cov_df.rename(columns=symbol2index, inplace=True)\n",
    "\n",
    "distance_matrix = (1 - corr_df).to_numpy()\n",
    "np.fill_diagonal(distance_matrix, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thresholds / cluster_num graphs\n",
    "methods = ['single', 'ward', 'average', 'complete', 'weighted', 'centroid', 'median']\n",
    "methods = ['ward']\n",
    "for method in methods:\n",
    "    linked = sch.linkage(squareform(distance_matrix), method=method)\n",
    "    \n",
    "    num_clusters = range(len(corr_df), 1, -1)\n",
    "    thresholds = linked[:, 2]\n",
    "\n",
    "    # inertias = []\n",
    "    # for n_clusters in num_clusters:\n",
    "    #     cluster_labels = fcluster(linked, t=n_clusters, criterion='maxclust')\n",
    "    #     inertia = 0\n",
    "    #     for cluster in np.unique(cluster_labels):\n",
    "    #         members = distance_matrix.values[cluster_labels == cluster]\n",
    "    #         centroid = members.mean(axis=0)  # Cluster centroid\n",
    "    #         inertia += np.sum((members - centroid) ** 2)\n",
    "    #     inertias.append(inertia)\n",
    "\n",
    "    # plt.figure(figsize=(12, 6))\n",
    "    # plt.plot(num_clusters, inertias, marker='o', label=f\"Method {method}\")\n",
    "    # plt.title(f\"Inertia/Num ({method})\")\n",
    "    # plt.xlabel('Number of Clusters')\n",
    "    # plt.ylabel('Inertia (Sum of Squared Distances)')\n",
    "    # plt.grid(True)\n",
    "    # plt.legend()\n",
    "    # plt.show()\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(num_clusters, thresholds, marker='o')\n",
    "    plt.title(f\"Threshold/Num ({method})\")\n",
    "    plt.xlabel('Number of Clusters')\n",
    "    plt.ylabel('Threshold (Distance)')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouettes and dendrograms\n",
    "def product(row):\n",
    "    product = 1\n",
    "    for value in row.values():\n",
    "        product *= value\n",
    "    return product\n",
    "\n",
    "ks = []\n",
    "scores = []\n",
    "counts = []\n",
    "for k in range(2, min(len(distance_matrix), 9)):\n",
    "    clusters = AgglomerativeClustering(n_clusters=k, linkage='ward').fit_predict(distance_matrix)\n",
    "    score = silhouette_score(distance_matrix, clusters, metric='precomputed')\n",
    "    ks.append(k)\n",
    "    scores.append(score)\n",
    "    unique_clusters, label_counts = np.unique(clusters, return_counts=True)\n",
    "    label_counts_dict = dict(zip(unique_clusters, label_counts))\n",
    "    counts.append(label_counts_dict)\n",
    "\n",
    "silhouettes = pd.DataFrame({\n",
    "    'k': ks,\n",
    "    'score': scores,\n",
    "    'counts': counts\n",
    "})\n",
    "silhouettes['combitions'] = silhouettes['counts'].apply(product)\n",
    "silhouettes = silhouettes.sort_values(by='score', ascending=False)\n",
    "best_k = silhouettes.k.iloc[0]\n",
    "\n",
    "# best_k = 3\n",
    "\n",
    "display(silhouettes)\n",
    "methods = ['single', 'ward', 'average', 'complete', 'weighted', 'centroid', 'median']\n",
    "methods = ['ward']\n",
    "for method in methods:\n",
    "    # Now compute the linkage using a condensed distance matrix\n",
    "    linked = sch.linkage(squareform(distance_matrix), method=method)\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    sch.dendrogram(linked, labels=corr_df.index, leaf_rotation=90)\n",
    "    plt.title(f\"Method {method}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Calculate Minimum Variance Portfolios\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Portfolio Optimization Functions\n",
    "def portfolio_variance(weights, cov_matrix):\n",
    "    return weights.T @ cov_matrix @ weights\n",
    "\n",
    "def portfolio_expected_return(weights, expected_returns_arr):\n",
    "    return weights @ expected_returns_arr\n",
    "\n",
    "def minimize_portfolio_variance(cov_matrix, expected_returns_arr):\n",
    "    num_assets = len(cov_matrix)\n",
    "    initial_weights = np.array([1/num_assets] * num_assets)\n",
    "    bounds = tuple((0, 1) for _ in range(num_assets))\n",
    "    constraints = ({'type': 'eq', 'fun': lambda weights: np.sum(weights) - 1})\n",
    "\n",
    "    optimization_result = minimize(portfolio_variance,\n",
    "                                    initial_weights,\n",
    "                                    args=(cov_matrix,),\n",
    "                                    method='SLSQP',\n",
    "                                    bounds=bounds,\n",
    "                                    constraints=constraints)\n",
    "\n",
    "    if optimization_result.success:\n",
    "        optimized_weights = optimization_result.x\n",
    "        port_variance = optimization_result.fun\n",
    "        port_std = np.sqrt(port_variance)\n",
    "        port_er = portfolio_expected_return(optimized_weights, expected_returns_arr)\n",
    "\n",
    "        return (optimized_weights, port_std, port_er)\n",
    "    else:\n",
    "        return (np.nan, np.nan, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Portfolio Optimization Functions\n",
    "def compute_distance_sum(combination, distance_matrix):\n",
    "    distance_sum = 0\n",
    "    for i_idx, j_idx in itertools.combinations(combination, 2):\n",
    "        distance_sum += distance_matrix[i_idx, j_idx]\n",
    "    return distance_sum\n",
    "\n",
    "def portfolio_variance(weights, cov_matrix):\n",
    "    return weights.T @ cov_matrix @ weights\n",
    "\n",
    "def portfolio_expected_return(weights, expected_returns_arr):\n",
    "    return weights @ expected_returns_arr\n",
    "\n",
    "def sharpe_ratio(weights, expected_returns_arr, cov_matrix, risk_free_rate):\n",
    "    port_er = portfolio_expected_return(weights, expected_returns_arr)\n",
    "    port_variance = portfolio_variance(weights, cov_matrix)\n",
    "    port_std = np.sqrt(port_variance)\n",
    "    return (port_er - risk_free_rate) / port_std\n",
    "\n",
    "def negative_sharpe_ratio(weights, expected_returns_arr, cov_matrix, risk_free_rate):\n",
    "    return -sharpe_ratio(weights, expected_returns_arr, cov_matrix, risk_free_rate)\n",
    "\n",
    "def find_tangency_portfolio(cov_matrix, expected_returns_arr, risk_free_rate):\n",
    "    num_assets = len(cov_matrix)\n",
    "    initial_weights = np.array([1/num_assets] * num_assets)\n",
    "    bounds = tuple((0, 1) for _ in range(num_assets))\n",
    "    constraints = ({'type': 'eq', 'fun': lambda weights: np.sum(weights) - 1})\n",
    "\n",
    "    optimization_result = minimize(negative_sharpe_ratio,\n",
    "                                    initial_weights,\n",
    "                                    args=(expected_returns_arr, cov_matrix, risk_free_rate),\n",
    "                                    method='SLSQP',\n",
    "                                    bounds=bounds,\n",
    "                                    constraints=constraints)\n",
    "\n",
    "    if optimization_result.success:\n",
    "        optimized_weights = optimization_result.x\n",
    "        variance = portfolio_variance(optimized_weights, cov_matrix)\n",
    "        std = np.sqrt(variance)\n",
    "        er = portfolio_expected_return(optimized_weights, expected_returns_arr)\n",
    "        sharpe_ratio = -(optimization_result.fun)\n",
    "\n",
    "        return (optimized_weights, std, er, sharpe_ratio)\n",
    "    else:\n",
    "        return (np.nan, np.nan, np.nan, np.nan)\n",
    "    \n",
    "def sort_top_combinations(array, sort_index):\n",
    "    valid_rows = ~np.isnan(array[:, sort_index])\n",
    "    valid_array = array[valid_rows]\n",
    "    if valid_array.size > 0:\n",
    "        sort_values = valid_array[:, sort_index]\n",
    "        sort_indices = np.argsort(sort_values)[::-1]\n",
    "        array[valid_rows] = valid_array[sort_indices]\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_symbols = len(corr_df.index)\n",
    "num_metrics = best_k*2 + 4\n",
    "num_combinations = math.comb(num_symbols, best_k)\n",
    "combination_array = np.empty((num_combinations, num_metrics), dtype='float32')\n",
    "\n",
    "# Calculate distance sums and populate the NumPy array\n",
    "for i, combination in tqdm(enumerate(itertools.combinations(range(0,num_symbols), best_k)), total=num_combinations, desc=\"Calculating distance sums\"):\n",
    "    combination_array[i, :best_k] = combination\n",
    "    combination_cov_df = cov_df.loc[combination, combination]\n",
    "    combination_expected_returns = np.array([expected_returns[index2symbol[index]] for index in combination])\n",
    "\n",
    "    index_indicator = best_k + best_k + 3\n",
    "    combination_array[i, best_k: index_indicator] = find_tangency_portfolio(combination_cov_df, combination_expected_returns, real_rf_rate)\n",
    "    combination_array[i, index_indicator: index_indicator + 1] = compute_distance_sum(combination, distance_matrix)\n",
    "\n",
    "    # population growth rate\n",
    "\n",
    "\n",
    "\n",
    "# TODO - not to be sorted by best_k\n",
    "sorted_indices = np.argsort(combination_array[:, best_k], kind='mergesort')[::-1]\n",
    "combination_array = combination_array[sorted_indices]\n",
    "del sorted_indices\n",
    "combination_array, len(combination_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_symbols = len(corr_df.index)\n",
    "num_metrics = best_k*2 + 4\n",
    "num_combinations_possible = math.comb(num_symbols, best_k)\n",
    "\n",
    "top_n = 5000 # Define how many top combinations to keep\n",
    "\n",
    "top_combinations_array = np.empty((top_n, num_metrics), dtype='float32')\n",
    "top_combinations_array[:] = np.nan\n",
    "rows_filled = 0\n",
    "\n",
    "\n",
    "for combination_tuple in tqdm(itertools.combinations(range(0,num_symbols), best_k), total=num_combinations_possible, desc=\"Calculating Tangency Portfolios\"):\n",
    "    combination_cov_df = cov_df.loc[combination_tuple, combination_tuple]\n",
    "    combination_expected_returns = np.array([expected_returns[index2symbol[index]] for index in combination_tuple])\n",
    "    weights, std, er, sharpe = find_tangency_portfolio(combination_cov_df, combination_expected_returns, real_rf_rate)\n",
    "    rating = sharpe * compute_distance_sum(combination_tuple, distance_matrix)\n",
    "\n",
    "    if rows_filled < top_n:\n",
    "        top_combinations_array[rows_filled, :best_k] = combination_tuple\n",
    "        top_combinations_array[rows_filled, best_k:best_k*2] = weights\n",
    "        top_combinations_array[rows_filled, best_k*2: num_metrics] = [std, er, sharpe, rating]\n",
    "        rows_filled += 1\n",
    "        if rows_filled == top_n:\n",
    "            top_combinations_array = sort_top_combinations(top_combinations_array, -1)\n",
    "\n",
    "    else:\n",
    "        if rating > top_combinations_array[-1, -1]:\n",
    "            top_combinations_array[rows_filled-1, :best_k] = combination_tuple\n",
    "            top_combinations_array[rows_filled-1, best_k:best_k*2] = weights\n",
    "            top_combinations_array[rows_filled-1, best_k*2: num_metrics] = [std, er, sharpe, rating]\n",
    "            top_combinations_array = sort_top_combinations(top_combinations_array, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove NaN rows before further processing\n",
    "top_combinations_array_cleaned = top_combinations_array[~np.isnan(top_combinations_array[:, best_k])]\n",
    "\n",
    "print(\"Top\", top_n, \"Combinations by Sharpe Ratio:\")\n",
    "for row in top_combinations_array_cleaned:\n",
    "    combination_indices = row[:best_k].astype(int)\n",
    "    asset_symbols = [index2symbol[index] for index in combination_indices]\n",
    "    # asset_symbols = [index for index in combination_indices]\n",
    "    weights, std, er, sharpe, rating = row[best_k:best_k+best_k], row[best_k+best_k], row[best_k+best_k+1], row[best_k+best_k+2], row[-1]\n",
    "    print(f\"Assets: {asset_symbols}, Weights: {weights}, Std Dev: {std:.4f}, Expected Return: {er:.4f}, Sharpe Ratio: {sharpe:.4f}, Rating: {rating:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
