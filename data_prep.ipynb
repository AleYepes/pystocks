{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import ast\n",
    "import pycountry\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep functions\n",
    "def evaluate_literal(val):\n",
    "    try:\n",
    "        return ast.literal_eval(val)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return val\n",
    "    \n",
    "def load(path):\n",
    "    df = pd.read_csv(path)\n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].apply(evaluate_literal)\n",
    "    return df\n",
    "\n",
    "def save(df):\n",
    "    final_df = df[df.apply(is_row_valid, axis=1)]\n",
    "    final_df = clean_df(final_df)\n",
    "    try:\n",
    "        temp_df = load('data/contract_elaborated.csv')\n",
    "        temp_df = clean_df(temp_df)\n",
    "        final_df = pd.concat([final_df, temp_df]).drop_duplicates(subset=['symbol', 'exact_search', 'search_exchange', 'search_symbol'])\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "    # Filter out the duplicates with 'exact_search' is False\n",
    "    duplicates_df = final_df[final_df.duplicated(subset='symbol', keep=False)]\n",
    "    final_df = final_df.drop(duplicates_df[duplicates_df['exact_search'] == False].index)\n",
    "\n",
    "    final_df.to_csv('data/contract_elaborated.csv', index=False)\n",
    "\n",
    "def is_numerical(val):\n",
    "    try:\n",
    "        val = str(val).replace('%', '')\n",
    "        float(val)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def is_valid_tuple(tuple, column):\n",
    "    def extract_float(value):\n",
    "        match = re.match(r'[^0-9]*([0-9.,]+)', value)\n",
    "        if match:\n",
    "            return float(match.group(1).replace(',', ''))\n",
    "        return None\n",
    "    \n",
    "    label, value = tuple\n",
    "    if not isinstance(label, str): # keep\n",
    "        # if label != None: # Comment out for more rigid filter\n",
    "        return False\n",
    "    if value is None:\n",
    "        return True # Comment out for more rigid filter\n",
    "        return False \n",
    "    if is_numerical(value):\n",
    "        return True\n",
    "    \n",
    "    if column == 'profile':\n",
    "        # if value and label:\n",
    "        return True\n",
    "    if column == 'fundamentals':\n",
    "        if value.isupper():\n",
    "            return True\n",
    "    if column == 'dividends':\n",
    "        if value == 'Unknown':\n",
    "            return True\n",
    "        extract_float_value = extract_float(value)\n",
    "        if extract_float_value is not None:\n",
    "            return True\n",
    "    if column == 'style':\n",
    "        if isinstance(value, bool):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def is_row_valid(row):\n",
    "    for col in row.index:\n",
    "        if isinstance(row[col], list):\n",
    "            # if col == 'fundamentals':\n",
    "            #     if len(row[col]) not in [4,5,21,22,   23]: #4, 5, 21, 22 are the acceptable num of fund values, 23 is for little bugs\n",
    "            #         print(len(row[col]))\n",
    "            #         return False\n",
    "            for tuple in row[col]:\n",
    "                if not is_valid_tuple(tuple, col):\n",
    "                    print(tuple)\n",
    "                    return False\n",
    "    return True\n",
    "\n",
    "def has_bad_multiplier(long_name):\n",
    "    cleaned = long_name.replace('-', '').replace('+', '')\n",
    "    for word in cleaned.split():\n",
    "        if re.fullmatch(r'\\d+X', word):\n",
    "            if int(word[:-1]) > 1:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def get_remaining():\n",
    "    contract_details = load('data/contract_details.csv')\n",
    "    try:\n",
    "        final_df = load('data/contract_elaborated.csv')\n",
    "        final_df = final_df[final_df.apply(is_row_valid, axis=1)]\n",
    "\n",
    "        exclusion_condition = (final_df['exchange_bug'] == True) | (final_df['exact_search'] == True) | (~final_df['profile'].isna())\n",
    "        # exclusion_condition = (final_df['exchange_bug'] == True) | (final_df['exact_search'] == True)\n",
    "        symbols_to_exclude = final_df[exclusion_condition]['symbol']\n",
    "        remaining = contract_details[~contract_details['symbol'].isin(symbols_to_exclude)]\n",
    "\n",
    "        # # To debug invalid rows\n",
    "        # remaining = final_df.copy()\n",
    "        # remaining = remaining[~remaining.apply(is_row_valid, axis=1)]\n",
    "    except FileNotFoundError:\n",
    "        remaining = contract_details.copy()\n",
    "        \n",
    "    remaining = remaining[~remaining['longName'].apply(has_bad_multiplier)]\n",
    "    remaining = remaining[['symbol', 'exchange', 'primaryExchange', 'validExchanges', 'currency', 'conId', 'longName', 'stockType', 'isin']]\n",
    "    return remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning functions\n",
    "def clean_labels(label, col):\n",
    "    if col == 'industries':\n",
    "        if isinstance(label, str):\n",
    "            if label.endswith('-Discontinuedeff09/19/2020'):\n",
    "                return label.split('-')[0]\n",
    "        return label\n",
    "    \n",
    "    elif col == 'holding_types':\n",
    "        if isinstance(label, str):\n",
    "            if label.startswith('■'):\n",
    "                return label[1:]\n",
    "            elif label.startswith('1'):\n",
    "                return label[1:]\n",
    "        return label\n",
    "    elif col == 'debtors':\n",
    "        if isinstance(label, str):\n",
    "            if ('（') in label:\n",
    "                return label.replace('（', '(')\n",
    "        return label\n",
    "    elif col == 'fundamentals':\n",
    "        if isinstance(label, str):\n",
    "            if label == 'LTDebt/ShareholdersEquity':\n",
    "                return 'LTDebt/Shareholders'\n",
    "        return label\n",
    "    return label\n",
    "    \n",
    "def correct_digit(value_str):\n",
    "    try:\n",
    "        digit = re.sub(r'[^\\d.-]', '', value_str).strip()\n",
    "        return float(digit)\n",
    "    except Exception:\n",
    "        return value_str\n",
    "\n",
    "def clean_values(value_str, col):\n",
    "    # print(value_str)\n",
    "    if col == 'profile':\n",
    "        return value_str\n",
    "    if isinstance(value_str, str):\n",
    "        if value_str.endswith('%'):\n",
    "            return correct_digit(value_str.replace('%',''))/100\n",
    "        try:\n",
    "            return correct_digit(value_str)\n",
    "        except Exception:\n",
    "            return value_str\n",
    "    return value_str\n",
    "\n",
    "def clean_df(df):\n",
    "    for col in df.columns:\n",
    "        # print(col)\n",
    "        df[col] = df[col].apply(evaluate_literal)\n",
    "        df[col] = df[col].apply(lambda x: [(clean_labels(item[0], col), item[1]) if isinstance(item, tuple) and len(item) == 2 else item for item in x] if isinstance(x, list) else x)\n",
    "        df[col] = df[col].apply(lambda x: [(item[0], clean_values(item[1], col)) if isinstance(item, tuple) and len(item) == 2 else item for item in x] if isinstance(x, list) else x)\n",
    "        df[col] = df[col].apply(lambda x: sorted(x, key=lambda item: item[0] if isinstance(item, tuple) and item[0] else '') if isinstance(x, list) else x)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode columns\n",
    "contracts_df = load('data/contract_elaborated.csv')\n",
    "contracts_df = clean_df(contracts_df)\n",
    "contracts_df = contracts_df[contracts_df.apply(is_row_valid, axis=1)]\n",
    "\n",
    "contracts_df['bond'] = contracts_df[['debtors', 'maturity', 'debt_type']].notna().any(axis=1).astype(bool)\n",
    "contracts_df[['equity', 'cash', 'other']] = False\n",
    "\n",
    "empty_subcategories = {\n",
    "'holding_types': ['Other'],\n",
    "'countries': ['Unidentified'], \n",
    "'currencies': ['<NoCurrency>'],\n",
    "'industries': ['NonClassifiedEquity', 'NotClassified-NonEquity'],\n",
    "'top10': ['OtherAssets', 'AccountsPayable','AccountsReceivable','AccountsReceivable&Pay','AdministrationFees','CustodyFees','ManagementFees','OtherAssetsandLiabilities','OtherAssetslessLiabilities', 'OtherFees','OtherLiabilities','Tax','Tax--ManagementFees'],\n",
    "'debtors': ['OTHER'],\n",
    "'maturity': ['%MaturityOther'],\n",
    "'debt_type': ['%QualityNotAvailable', '%QualityNotRated'],\n",
    "}\n",
    "\n",
    "original_columns = contracts_df.columns\n",
    "columns_to_explode = ['profile', 'holding_types', 'top10', 'countries', 'fundamentals', 'industries', 'currencies', 'style', 'debtors', 'maturity', 'debt_type', 'lipper', 'dividends']\n",
    "\n",
    "columns_to_ignore = ['lipper', 'dividends']\n",
    "columns_to_explode = [col for col in columns_to_explode if col not in columns_to_ignore]\n",
    "\n",
    "non_percentage_columns = ['profile', 'style', 'lipper', 'fundamentals', 'dividends']\n",
    "percentage_columns = [col for col in columns_to_explode if col not in non_percentage_columns]\n",
    "for col in columns_to_explode:\n",
    "    print(col)\n",
    "    contracts_df[col] = contracts_df[col].fillna('[]')\n",
    "    contracts_df[col] = contracts_df[col].apply(evaluate_literal)\n",
    "\n",
    "    # Explode and create pivot_df\n",
    "    contracts_df = contracts_df.explode(col)\n",
    "    contracts_df[col] = contracts_df[col].apply(lambda x: (None, None) if pd.isna(x) else x)\n",
    "    contracts_df[['label', 'value']] = pd.DataFrame(contracts_df[col].tolist(), index=contracts_df.index)\n",
    "\n",
    "    pivot_df = contracts_df.pivot_table(index=contracts_df.index, columns='label', values='value', aggfunc='first')\n",
    "    pivot_df.rename(columns={label: f'{col}_{label}' for label in pivot_df.columns}, inplace=True)\n",
    "\n",
    "    # Drop unnecessary columns and align pivot_df with contracts_df\n",
    "    contracts_df = contracts_df.drop(columns=[col, 'label', 'value'], axis=1).drop_duplicates(subset='conId')\n",
    "    pivot_df = pivot_df.reindex(contracts_df.index)\n",
    "    \n",
    "    # Correct pivot_df values\n",
    "    if col in percentage_columns:\n",
    "        pivot_df = pivot_df.fillna(0.0).clip(lower=0)\n",
    "        columns_to_drop = [f'{col}_{label}' for label in empty_subcategories[col]]\n",
    "\n",
    "        # Scale value sum to be <= 1\n",
    "        pivot_cols_sum = pivot_df.sum(axis=1)\n",
    "        rows_greater_than = pivot_cols_sum > 1\n",
    "        pivot_df.loc[rows_greater_than] = pivot_df.loc[rows_greater_than].div(pivot_cols_sum[rows_greater_than], axis=0)\n",
    "\n",
    "        # Guarantee value sum == 1\n",
    "        pivot_cols_sum = pivot_df.sum(axis=1)\n",
    "        rows_less_than = pivot_cols_sum < 1\n",
    "        missing_value = (1 - pivot_cols_sum[rows_less_than])\n",
    "\n",
    "        empty_column = columns_to_drop[0]\n",
    "        if empty_column not in pivot_df.columns:\n",
    "                pivot_df[empty_column] = 0.0\n",
    "\n",
    "        pivot_df.loc[rows_less_than, empty_column] = pivot_df.loc[rows_less_than, empty_column] + missing_value\n",
    "\n",
    "        # Create variety columns\n",
    "        pivot_df[f'{col}_variety'] = pivot_df.pow(2).sum(axis=1)\n",
    "\n",
    "        # # To avoid multicollinearity\n",
    "        # pivot_df = pivot_df.drop(columns=columns_to_drop, axis=1, errors='ignore')\n",
    "\n",
    "        # Drop top10 company columns\n",
    "        if col == 'top10':\n",
    "            columns_to_drop = [column for column in pivot_df.columns if column != f'{col}_variety']\n",
    "            pivot_df = pivot_df.drop(columns=columns_to_drop, axis=1)\n",
    "\n",
    "    if col == 'dividends':\n",
    "        pivot_df = pivot_df.fillna(0.0).clip(lower=0)\n",
    "\n",
    "    contracts_df = pd.concat([contracts_df, pivot_df], axis=1)\n",
    "\n",
    "contracts_df = contracts_df[~contracts_df['profile_TotalNetAssets'].isna()]\n",
    "contracts_df = contracts_df.drop(columns=columns_to_ignore, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop ETF duplicates\n",
    "eur_exchanges = contracts_df[contracts_df['currency'] == 'EUR'].primaryExchange.unique()\n",
    "remaining_columns = [col for col in contracts_df.columns if col not in original_columns]\n",
    "og_len = len(contracts_df)\n",
    "\n",
    "contracts_df = (\n",
    "    contracts_df\n",
    "    .assign(currency_is_euro=contracts_df['currency'] == 'EUR')\n",
    "    .assign(exchange_is_european=contracts_df['exchange'].isin(eur_exchanges))\n",
    "    .assign(primary_is_european=contracts_df['primaryExchange'].isin(eur_exchanges))\n",
    "    .sort_values(by=['currency_is_euro','exchange_is_european', 'primary_is_european', 'tradable'], ascending=[False, False, False, False])\n",
    "    .drop_duplicates(subset=remaining_columns, keep='first')\n",
    "    .drop(columns=['currency_is_euro', 'exchange_is_european', 'primary_is_european'])\n",
    ")\n",
    "og_len - len(contracts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct profile total net assets and TER\n",
    "symbol_mapping = {\n",
    "    '$': 'USD',    # Default to USD\n",
    "    '￥': 'JPY',    # Japanese Yen\n",
    "    'Rs': 'INR',\n",
    "    'CNH': 'CNY',\n",
    "    '€': 'EUR',    # Euro\n",
    "    '¥': 'JPY',    # Alternative Yen symbol\n",
    "    '£': 'GBP',    # British Pound\n",
    "    'A$': 'AUD',   # Australian Dollar\n",
    "    'C$': 'CAD',   # Canadian Dollar\n",
    "    'HK$': 'HKD',  # Hong Kong Dollar\n",
    "}\n",
    "\n",
    "def standardize_currency(currency):\n",
    "    if pd.isna(currency):\n",
    "        return np.nan\n",
    "    if currency in symbol_mapping:\n",
    "        return symbol_mapping[currency]\n",
    "    if currency == '':\n",
    "        return ''\n",
    "    try:\n",
    "        if pycountry.currencies.get(alpha_3=currency):\n",
    "            return currency\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    return currency\n",
    "\n",
    "def clean_total_net_assets(value):\n",
    "    if pd.isna(value):\n",
    "        return np.nan, np.nan\n",
    "    value = re.sub(r'\\basof\\b.*', '', value, flags=re.IGNORECASE).strip()\n",
    "    match = re.match(r'([^0-9\\s]+)?\\s*([0-9.,]+)\\s*([kKmMbB]?)', value)\n",
    "    if not match:\n",
    "        return np.nan, np.nan\n",
    "    currency, num_str, unit = match.groups()\n",
    "    currency = currency if currency else ''\n",
    "    num = float(num_str.replace(',', ''))\n",
    "    unit = unit.lower() if unit else ''\n",
    "    if unit == 'k':\n",
    "        num *= 10**3\n",
    "    elif unit == 'm':\n",
    "        num *= 10**6\n",
    "    elif unit == 'b':\n",
    "        num *= 10**9\n",
    "    elif unit == 't':\n",
    "        num *= 10**12\n",
    "    return num, currency\n",
    "\n",
    "def get_exchange_rates(currencies, to_currency='USD'):\n",
    "    rates = {}\n",
    "    valid_currencies = []\n",
    "    for c in currencies:\n",
    "        if pd.notna(c) and pycountry.currencies.get(alpha_3=c) is not None:\n",
    "            valid_currencies.append(c)\n",
    "    if not valid_currencies:\n",
    "        return rates\n",
    "    try:\n",
    "        url = f\"https://open.er-api.com/v6/latest/{to_currency}\"\n",
    "        response = requests.get(url)\n",
    "        data = response.json()\n",
    "        if 'rates' in data:\n",
    "            for currency in valid_currencies:\n",
    "                if currency == 'USD':\n",
    "                    rates[currency] = 1.0\n",
    "                elif currency in data['rates']:\n",
    "                    rates[currency] = 1 / data['rates'][currency] if data['rates'][currency] != 0 else np.nan\n",
    "            # print(f\"Fetched rates: {rates}\")\n",
    "            return rates\n",
    "        else:\n",
    "            print(f\"Error fetching rates: {data.get('error', 'Unknown error')}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Exchange rate fetch failed: {e}\")\n",
    "    return rates\n",
    "\n",
    "def convert_to_usd(row, rates):\n",
    "    if pd.isna(row['profile_cap']) or pd.isna(row['profile_cap_currency']):\n",
    "        return np.nan\n",
    "    currency = row['profile_cap_currency']\n",
    "    if currency in rates:\n",
    "        return row['profile_cap'] * rates[currency]\n",
    "    print(f\"No rate available for {currency}\")\n",
    "    return np.nan\n",
    "\n",
    "contracts_df[['profile_cap', 'profile_cap_currency']] = contracts_df['profile_TotalNetAssets'].apply(lambda x: pd.Series(clean_total_net_assets(x)))\n",
    "contracts_df['profile_cap_currency'] = contracts_df['profile_cap_currency'].apply(standardize_currency)\n",
    "contracts_df['profile_cap_currency'] = np.where(contracts_df['profile_cap_currency'] == '', contracts_df['currency'], contracts_df['profile_cap_currency'])\n",
    "contracts_df['profile_cap_currency'] = contracts_df['profile_cap_currency'].apply(lambda x: x if (pd.isna(x) or pycountry.currencies.get(alpha_3=x) or x == '') else np.nan)\n",
    "\n",
    "exchange_rates = get_exchange_rates(contracts_df['profile_cap_currency'].unique())\n",
    "contracts_df['profile_cap_usd'] = contracts_df.apply(lambda row: convert_to_usd(row, exchange_rates),axis=1)\n",
    "contracts_df = contracts_df.drop(columns=['profile_TotalNetAssets', 'profile_cap', 'profile_cap_currency'], axis=1, errors='ignore')\n",
    "\n",
    "# TER\n",
    "contracts_df['profile_TotalExpenseRatio'] = contracts_df['profile_TotalExpenseRatio'].replace('', np.nan).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domicile dummies\n",
    "if 'profile_Domicile' in contracts_df.columns:\n",
    "    dummies = pd.get_dummies(contracts_df['profile_Domicile'], prefix='domicile').astype(int)\n",
    "    contracts_df = pd.concat([contracts_df, dummies], axis=1)\n",
    "    contracts_df.drop('profile_Domicile', axis=1, inplace=True)\n",
    "\n",
    "# Market cap dummies\n",
    "size_map = {\n",
    "        'Small-cap': 'small',\n",
    "        'Mid-cap': 'mid',\n",
    "        'BroadMarket': 'multi',\n",
    "        'Large-cap': 'large',\n",
    "    }\n",
    "contracts_df['profile_MarketCapFocus'] = contracts_df['profile_MarketCapFocus'].map(size_map)\n",
    "\n",
    "# Get size columns from MarketCapFocus and style cols\n",
    "dummies = pd.get_dummies(contracts_df['profile_MarketCapFocus'], dtype=int)\n",
    "contracts_df = pd.concat([contracts_df, dummies], axis=1)\n",
    "\n",
    "style_groups = {\n",
    "    'small': ['style_small-core', 'style_small-growth', 'style_small-value'],\n",
    "    'mid': ['style_mid-core', 'style_mid-growth', 'style_mid-value'],\n",
    "    'large': ['style_large-core', 'style_large-growth', 'style_large-value'],\n",
    "    'multi': ['style_multi-core', 'style_multi-growth', 'style_multi-value']\n",
    "}\n",
    "\n",
    "# Update each size column by OR-ing with the style columns\n",
    "for size, cols in style_groups.items():\n",
    "    contracts_df[size] = contracts_df[size] | contracts_df[cols].any(axis=1).astype(int)\n",
    "\n",
    "del dummies, size_map, style_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search exchange verification\n",
    "contracts_df['search_exchange'] = contracts_df['search_exchange'].str.extract(r'\\(([^()]*)\\)', expand=False)\n",
    "contracts_df['validExchanges'] = contracts_df['validExchanges'].apply(lambda x: x.split(','))\n",
    "\n",
    "def validate_search_exchange(row):\n",
    "    # if pd.isna(row['search_exchange']):\n",
    "    #     return np.nan\n",
    "    return 1 if row['search_exchange'] in row['validExchanges'] else 0\n",
    "\n",
    "contracts_df['valid_search_exchange'] = contracts_df.apply(validate_search_exchange, axis=1)\n",
    "contracts_df = contracts_df.drop(columns=['search_symbol', 'search_exchange', 'validExchanges'], axis=1, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refine original asset class classifications with fundamentals and holding_type columns\n",
    "bond_fundamental_cols = ['fundamentals_AverageCoupon', 'fundamentals_AverageQuality', 'fundamentals_YieldtoMaturity', 'fundamentals_NominalMaturity', 'fundamentals_EffectiveMaturity']\n",
    "equity_fundamental_cols = [col for col in contracts_df.columns if col.startswith('fundamentals_') if col not in bond_fundamental_cols]\n",
    "bond_mask = contracts_df[bond_fundamental_cols].notna().any(axis=1)\n",
    "equity_mask = contracts_df[equity_fundamental_cols].notna().any(axis=1)\n",
    "\n",
    "contracts_df['bond'] = contracts_df['bond'] | bond_mask\n",
    "contracts_df['other'] = ~(equity_mask | contracts_df['bond'])\n",
    "contracts_df['other'] = contracts_df['other'] | (equity_mask & contracts_df['bond'])\n",
    "contracts_df['equity'] = ~(contracts_df['bond'] | contracts_df['other'])\n",
    "\n",
    "contracts_df = contracts_df.rename(columns={'holding_types_FixedIncome': 'holding_types_bond',\n",
    "                                            'holding_types_Equity': 'holding_types_equity',\n",
    "                                            'holding_types_Cash': 'holding_types_cash',\n",
    "                                            'holding_types_Other': 'holding_types_other',\n",
    "                                            })\n",
    "\n",
    "def refine_classification(row):\n",
    "    holding_types = ['holding_types_bond', 'holding_types_equity', 'holding_types_cash', 'holding_types_other']\n",
    "    max_col = row[holding_types].idxmax()\n",
    "    if row[max_col] > 0.5:\n",
    "        type_name = max_col.replace('holding_types_', '')\n",
    "        result = pd.Series([False, False, False, False], index=['bond', 'equity', 'cash', 'other'])\n",
    "        result[type_name] = True\n",
    "        return result\n",
    "    else:\n",
    "        return row[['bond', 'equity', 'cash', 'other']]\n",
    "\n",
    "contracts_df[['bond', 'equity', 'cash', 'other']] = contracts_df.apply(refine_classification, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final cleaning before imputation\n",
    "rating_map = {\n",
    "    'AAA': 6,\n",
    "    'AA': 5,\n",
    "    'A': 4,\n",
    "    'BBB': 3,\n",
    "    'BB': 2,\n",
    "    'B': 1,\n",
    "}\n",
    "contracts_df['fundamentals_AverageQuality'] = contracts_df['fundamentals_AverageQuality'].replace(rating_map)\n",
    "\n",
    "# Convert bools to intbools\n",
    "bool_map = {\n",
    "    True: 1,\n",
    "    False: 0,\n",
    "    # np.nan: 0,\n",
    "}\n",
    "bool_cols = [col for col in contracts_df.columns if contracts_df[col].dtype == 'bool' or col.startswith('style_')]\n",
    "for col in list(set(bool_cols)):\n",
    "    contracts_df[col] = contracts_df[col].replace(bool_map).fillna(0) ## DONT FILLNA if you want to impute \n",
    "\n",
    "if 'stockType' in contracts_df.columns:\n",
    "    contracts_df.loc[contracts_df['stockType'] == 'ETC', 'industries_BasicMaterials'] = 1.0\n",
    "\n",
    "# Remove unnecessary qual or empty columns, only keep key identifiers\n",
    "qual_cols = ['primaryExchange', 'stockType', 'date_scraped', 'exchange_bug', 'exact_search', 'search_symbol', 'profile_MarketCapFocus', 'profile_MarketGeoFocus', 'profile_BenchmarkIndex', 'profile_FundCategory', 'dividends_PayoutRatio']\n",
    "contracts_df = contracts_df.drop(columns=qual_cols, axis=1, errors='ignore')\n",
    "\n",
    "# Clean remaining numerical columns\n",
    "identifier_cols = ['symbol', 'conId', 'longName', 'isin', 'exchange', 'currency', 'profile_MarketCapFocus']\n",
    "for col in [c for c in contracts_df.columns if c not in identifier_cols]:\n",
    "    temp_type = contracts_df[col].dtype\n",
    "    if temp_type == 'object':\n",
    "        contracts_df[col] = contracts_df[col].apply(lambda x: np.nan if isinstance(x, str) else x)\n",
    "\n",
    "# Assign corresponding fundamentals\n",
    "contracts_df.loc[contracts_df['holding_types_bond'] == 0, bond_fundamental_cols] = 0\n",
    "contracts_df.loc[contracts_df['holding_types_equity'] == 0, equity_fundamental_cols] = 0\n",
    "\n",
    "contracts_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check similarity between all top 10 columns\n",
    "from fuzzywuzzy import fuzz\n",
    "from itertools import combinations\n",
    "\n",
    "columns = [col.split('top10_')[-1] for col in contracts_df.columns if col.startswith('top10_') and col not in ['top10_variety', 'top10_']]\n",
    "\n",
    "similarity_threshold = 80\n",
    "similar_pairs = []\n",
    "# similarities = []\n",
    "n = len(columns)\n",
    "\n",
    "for col1, col2 in tqdm(combinations(columns, 2), total = n * (n - 1) // 2):\n",
    "    similarity = fuzz.token_set_ratio(col1, col2)\n",
    "    # similarities.append(similarity)\n",
    "    if similarity >= similarity_threshold:\n",
    "        similar_pairs.append((col1, col2, similarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge similar top10\n",
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz\n",
    "from itertools import combinations\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "\n",
    "if n:\n",
    "    # Find connected components \n",
    "    G = nx.Graph()\n",
    "    for col1_suffix, col2_suffix, _ in similar_pairs:\n",
    "        G.add_edge(col1_suffix, col2_suffix)\n",
    "\n",
    "    for suffix in columns:\n",
    "        G.add_node(suffix)\n",
    "\n",
    "    connected_components = list(nx.connected_components(G))\n",
    "\n",
    "    # 3. Prepare for DataFrame modification\n",
    "    new_column_data_map = {}\n",
    "    original_cols_involved_in_merging = set()\n",
    "    for group_of_suffixes in connected_components:\n",
    "        if len(group_of_suffixes) > 1:\n",
    "            sorted_suffixes_in_group = sorted(list(group_of_suffixes))\n",
    "            \n",
    "            # Determine the new representative column name (using the 'top10_' prefix)\n",
    "            representative_suffix = sorted_suffixes_in_group[0]\n",
    "            merged_full_col_name = f\"top10_{representative_suffix}\"\n",
    "            \n",
    "            # Identify all original full column names in this group\n",
    "            original_full_names_in_this_group = []\n",
    "            for suffix in sorted_suffixes_in_group:\n",
    "                full_name = f\"top10_{suffix}\"\n",
    "                original_full_names_in_this_group.append(full_name)\n",
    "                original_cols_involved_in_merging.add(full_name)\n",
    "                \n",
    "            # Sum the values of the original columns in this group\n",
    "            existing_cols_to_sum = [col for col in original_full_names_in_this_group if col in contracts_df.columns]\n",
    "            \n",
    "            if existing_cols_to_sum:\n",
    "                summed_series = contracts_df[existing_cols_to_sum].sum(axis=1)\n",
    "                new_column_data_map[merged_full_col_name] = summed_series\n",
    "            else:\n",
    "                print(f\"  Warning: No existing columns found for suffixes {group_of_suffixes} to sum.\")\n",
    "                \n",
    "    # 5. Update the DataFrame\n",
    "    contracts_df_merged = contracts_df.copy()\n",
    "\n",
    "    cols_to_drop_list = list(original_cols_involved_in_merging)\n",
    "    if cols_to_drop_list:\n",
    "        contracts_df_merged.drop(columns=cols_to_drop_list, inplace=True, errors='ignore')\n",
    "\n",
    "    if new_column_data_map:\n",
    "        for new_col_name, data_series in new_column_data_map.items():\n",
    "            contracts_df_merged[new_col_name] = data_series\n",
    "\n",
    "    contracts_df = contracts_df_merged\n",
    "\n",
    "    del contracts_df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Impute all values at once\n",
    "# from sklearn.impute import KNNImputer\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.pipeline import Pipeline\n",
    "\n",
    "# imputed_df = contracts_df.copy()\n",
    "\n",
    "# numerical_cols = [col for col in imputed_df.columns if imputed_df[col].dtype in [float, int] and col not in ['conId', 'valid_search_exchange']]\n",
    "# pipeline = Pipeline([\n",
    "#     ('scaler', MinMaxScaler()),\n",
    "#     ('imputer', KNNImputer())\n",
    "# ])\n",
    "\n",
    "# transformer = ColumnTransformer(\n",
    "#     transformers=[('num', pipeline, numerical_cols)],\n",
    "#     remainder='passthrough'\n",
    "# )\n",
    "# imputed_array = transformer.fit_transform(imputed_df)\n",
    "\n",
    "# scaler = transformer.named_transformers_['num'].named_steps['scaler']\n",
    "# numerical_imputed_scaled = imputed_array[:, :len(numerical_cols)]\n",
    "# numerical_imputed_original = scaler.inverse_transform(numerical_imputed_scaled)\n",
    "\n",
    "# imputed_array[:, :len(numerical_cols)] = numerical_imputed_original\n",
    "\n",
    "# remaining_cols = [col for col in imputed_df.columns if col not in numerical_cols]\n",
    "# output_cols = numerical_cols + remaining_cols\n",
    "\n",
    "# imputed_df = pd.DataFrame(imputed_array, columns=output_cols, index=imputed_df.index)\n",
    "\n",
    "# for col in numerical_cols:\n",
    "#     if imputed_df[col].dtype != contracts_df[col].dtype:\n",
    "#         imputed_df[col] = imputed_df[col].astype(contracts_df[col].dtype)\n",
    "\n",
    "# imputed_df = imputed_df.drop(columns=[col for col in imputed_df.columns if col.startswith('top10_') and col != 'top10_variety'], axis=1)\n",
    "# imputed_df = imputed_df.drop(columns=['dividends'], errors='ignore')\n",
    "\n",
    "# imputed_df # 12 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute values by asset class\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "imputed_df = contracts_df.copy()\n",
    "if input('Impute values (y/n)').lower() == 'y': \n",
    "\n",
    "    class_cols = ['equity', 'bond', 'cash', 'other']\n",
    "    cols_to_exclude = ['conId', 'valid_search_exchange'] + class_cols\n",
    "    numerical_cols_global = [\n",
    "        col for col in imputed_df.columns\n",
    "        if imputed_df[col].dtype in [float, np.float64, int, np.int64]\n",
    "        and col not in cols_to_exclude\n",
    "    ]\n",
    "\n",
    "    for asset_class in tqdm(class_cols, total=len(class_cols)):\n",
    "        mask = imputed_df[asset_class] == 1\n",
    "        if not mask.any():\n",
    "            continue\n",
    "\n",
    "        subset_numerical_data = imputed_df.loc[mask, numerical_cols_global]\n",
    "        if subset_numerical_data.empty:\n",
    "            continue\n",
    "\n",
    "        cols_for_imputation_in_subset = [col for col in subset_numerical_data.columns if subset_numerical_data[col].nunique(dropna=True) > 1]\n",
    "\n",
    "        if not cols_for_imputation_in_subset:\n",
    "            continue\n",
    "\n",
    "        data_to_process = subset_numerical_data[cols_for_imputation_in_subset]\n",
    "        pipeline = Pipeline([\n",
    "            ('scaler', MinMaxScaler()),\n",
    "            ('imputer', KNNImputer())\n",
    "        ])\n",
    "\n",
    "        imputed_scaled_subset = pipeline.fit_transform(data_to_process)\n",
    "        \n",
    "        scaler_fitted_on_subset = pipeline.named_steps['scaler']\n",
    "        imputed_original_scale_subset = scaler_fitted_on_subset.inverse_transform(imputed_scaled_subset)\n",
    "\n",
    "        imputed_df.loc[mask, cols_for_imputation_in_subset] = imputed_original_scale_subset\n",
    "\n",
    "    # Restore original data types for numerical columns where possible\n",
    "    for col in numerical_cols_global:\n",
    "        if col in imputed_df.columns and col in contracts_df.columns:\n",
    "            if imputed_df[col].dtype != contracts_df[col].dtype:\n",
    "                try:\n",
    "                    imputed_df[col] = imputed_df[col].astype(contracts_df[col].dtype)\n",
    "                except ValueError as e:\n",
    "                    print(f\"Warning: Could not convert column '{col}' to {contracts_df[col].dtype}. Error: {e}\")\n",
    "\n",
    "    # Drop specified columns\n",
    "    columns_to_drop = [col for col in imputed_df.columns if col.startswith('top10_') and col != 'top10_variety']\n",
    "    imputed_df = imputed_df.drop(columns=columns_to_drop, errors='ignore')\n",
    "    imputed_df = imputed_df.drop(columns=['dividends'], errors='ignore')\n",
    "\n",
    "    imputed_df = imputed_df.fillna(0.0)\n",
    "else:\n",
    "    emptiness = imputed_df.isna().mean().copy()\n",
    "    mean = emptiness.mean()\n",
    "    std = emptiness.std()\n",
    "\n",
    "    columns_to_drop = emptiness[emptiness > mean + 3*std].index.to_list()\n",
    "    imputed_df = imputed_df.drop(columns=columns_to_drop, errors='ignore').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "imputed_df.to_csv('data/fundamentals.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
