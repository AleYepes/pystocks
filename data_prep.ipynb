{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import ast\n",
    "import pycountry\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep functions\n",
    "def evaluate_literal(val):\n",
    "    try:\n",
    "        return ast.literal_eval(val)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return val\n",
    "    \n",
    "def load(path):\n",
    "    df = pd.read_csv(path)\n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].apply(evaluate_literal)\n",
    "    return df\n",
    "\n",
    "def save(df):\n",
    "    final_df = df[df.apply(is_row_valid, axis=1)]\n",
    "    final_df = clean_df(final_df)\n",
    "    try:\n",
    "        temp_df = load('data/contract_elaborated.csv')\n",
    "        temp_df = clean_df(temp_df)\n",
    "        final_df = pd.concat([final_df, temp_df]).drop_duplicates(subset=['symbol', 'exact_search', 'search_exchange', 'search_symbol'])\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "    # Filter out the duplicates with 'exact_search' is False\n",
    "    duplicates_df = final_df[final_df.duplicated(subset='symbol', keep=False)]\n",
    "    final_df = final_df.drop(duplicates_df[duplicates_df['exact_search'] == False].index)\n",
    "\n",
    "    final_df.to_csv('data/contract_elaborated.csv', index=False)\n",
    "\n",
    "def is_numerical(val):\n",
    "    try:\n",
    "        val = str(val).replace('%', '')\n",
    "        float(val)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def is_valid_tuple(tuple, column):\n",
    "    def extract_float(value):\n",
    "        match = re.match(r'[^0-9]*([0-9.,]+)', value)\n",
    "        if match:\n",
    "            return float(match.group(1).replace(',', ''))\n",
    "        return None\n",
    "    \n",
    "    label, value = tuple\n",
    "    if not isinstance(label, str): # keep\n",
    "        # if label != None: # Comment out for more rigid filter\n",
    "        return False\n",
    "    if value is None:\n",
    "        return True # Comment out for more rigid filter\n",
    "        return False \n",
    "    if is_numerical(value):\n",
    "        return True\n",
    "    \n",
    "    if column == 'profile':\n",
    "        # if value and label:\n",
    "        return True\n",
    "    if column == 'fundamentals':\n",
    "        if value.isupper():\n",
    "            return True\n",
    "    if column == 'dividends':\n",
    "        if value == 'Unknown':\n",
    "            return True\n",
    "        extract_float_value = extract_float(value)\n",
    "        if extract_float_value is not None:\n",
    "            return True\n",
    "    if column == 'style':\n",
    "        if isinstance(value, bool):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def is_row_valid(row):\n",
    "    for col in row.index:\n",
    "        if isinstance(row[col], list):\n",
    "            # if col == 'fundamentals':\n",
    "            #     if len(row[col]) not in [4,5,21,22,   23]: #4, 5, 21, 22 are the acceptable num of fund values, 23 is for little bugs\n",
    "            #         print(len(row[col]))\n",
    "            #         return False\n",
    "            for tuple in row[col]:\n",
    "                if not is_valid_tuple(tuple, col):\n",
    "                    print(tuple)\n",
    "                    return False\n",
    "    return True\n",
    "\n",
    "def has_bad_multiplier(long_name):\n",
    "    cleaned = long_name.replace('-', '').replace('+', '')\n",
    "    for word in cleaned.split():\n",
    "        if re.fullmatch(r'\\d+X', word):\n",
    "            if int(word[:-1]) > 1:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def get_remaining():\n",
    "    contract_details = load('data/contract_details.csv')\n",
    "    try:\n",
    "        final_df = load('data/contract_elaborated.csv')\n",
    "        final_df = final_df[final_df.apply(is_row_valid, axis=1)]\n",
    "\n",
    "        exclusion_condition = (final_df['exchange_bug'] == True) | (final_df['exact_search'] == True) | (~final_df['profile'].isna())\n",
    "        # exclusion_condition = (final_df['exchange_bug'] == True) | (final_df['exact_search'] == True)\n",
    "        symbols_to_exclude = final_df[exclusion_condition]['symbol']\n",
    "        remaining = contract_details[~contract_details['symbol'].isin(symbols_to_exclude)]\n",
    "\n",
    "        # # To debug invalid rows\n",
    "        # remaining = final_df.copy()\n",
    "        # remaining = remaining[~remaining.apply(is_row_valid, axis=1)]\n",
    "    except FileNotFoundError:\n",
    "        remaining = contract_details.copy()\n",
    "        \n",
    "    remaining = remaining[~remaining['longName'].apply(has_bad_multiplier)]\n",
    "    remaining = remaining[['symbol', 'exchange', 'primaryExchange', 'validExchanges', 'currency', 'conId', 'longName', 'stockType', 'isin']]\n",
    "    return remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep functions 2\n",
    "def evaluate_literal(val):\n",
    "    try:\n",
    "        return ast.literal_eval(val)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return val\n",
    "    \n",
    "def load(path):\n",
    "    df = pd.read_csv(path)\n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].apply(evaluate_literal)\n",
    "    return df\n",
    "\n",
    "def save(df):\n",
    "    final_df = df[df.apply(is_row_valid, axis=1)]\n",
    "    final_df = clean_df(final_df)\n",
    "    try:\n",
    "        temp_df = load('data/contract_elaborated.csv')\n",
    "        temp_df = clean_df(temp_df)\n",
    "        final_df = pd.concat([final_df, temp_df]).drop_duplicates(subset=['symbol', 'exact_search', 'search_exchange', 'search_symbol'])\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "    # Filter out the duplicates with 'exact_search' is False\n",
    "    duplicates_df = final_df[final_df.duplicated(subset='symbol', keep=False)]\n",
    "    final_df = final_df.drop(duplicates_df[duplicates_df['exact_search'] == False].index)\n",
    "\n",
    "    final_df.to_csv('data/contract_elaborated.csv', index=False)\n",
    "\n",
    "def is_numerical(val):\n",
    "    try:\n",
    "        val = str(val).replace('%', '')\n",
    "        float(val)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def is_valid_tuple(tuple, column):\n",
    "    def extract_float(value):\n",
    "        match = re.match(r'[^0-9]*([0-9.,]+)', value)\n",
    "        if match:\n",
    "            return float(match.group(1).replace(',', ''))\n",
    "        return None\n",
    "    \n",
    "    label, value = tuple\n",
    "    if not isinstance(label, str): # keep\n",
    "        # if label != None: # Comment out for more rigid filter\n",
    "        return False\n",
    "    if value is None:\n",
    "        return True # Comment out for more rigid filter\n",
    "        return False \n",
    "    if is_numerical(value):\n",
    "        return True\n",
    "    \n",
    "    if column == 'profile':\n",
    "        # if value and label:\n",
    "        return True\n",
    "    if column == 'fundamentals':\n",
    "        if value.isupper():\n",
    "            return True\n",
    "    if column == 'dividends':\n",
    "        if value == 'Unknown':\n",
    "            return True\n",
    "        extract_float_value = extract_float(value)\n",
    "        if extract_float_value is not None:\n",
    "            return True\n",
    "    if column == 'style':\n",
    "        if isinstance(value, bool):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def is_row_valid(row):\n",
    "    for col in row.index:\n",
    "        if isinstance(row[col], list):\n",
    "            # if col == 'fundamentals':\n",
    "            #     if len(row[col]) not in [4,5,21,22,   23]: #4, 5, 21, 22 are the acceptable num of fund values, 23 is for little bugs\n",
    "            #         print(len(row[col]))\n",
    "            #         return False\n",
    "            for tuple in row[col]:\n",
    "                if not is_valid_tuple(tuple, col):\n",
    "                    print(tuple)\n",
    "                    return False\n",
    "    return True\n",
    "\n",
    "def has_bad_multiplier(long_name):\n",
    "    cleaned = long_name.replace('-', '').replace('+', '')\n",
    "    for word in cleaned.split():\n",
    "        if re.fullmatch(r'\\d+X', word):\n",
    "            if int(word[:-1]) > 1:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def get_remaining():\n",
    "    contract_details = load('data/contract_details.csv')\n",
    "    try:\n",
    "        final_df = load('data/contract_elaborated.csv')\n",
    "        final_df = final_df[final_df.apply(is_row_valid, axis=1)]\n",
    "\n",
    "        exclusion_condition = (final_df['exchange_bug'] == True) | (final_df['exact_search'] == True) | (~final_df['profile'].isna())\n",
    "        # exclusion_condition = (final_df['exchange_bug'] == True) | (final_df['exact_search'] == True)\n",
    "        symbols_to_exclude = final_df[exclusion_condition]['symbol']\n",
    "        remaining = contract_details[~contract_details['symbol'].isin(symbols_to_exclude)]\n",
    "\n",
    "        # # To debug invalid rows\n",
    "        # remaining = final_df.copy()\n",
    "        # remaining = remaining[~remaining.apply(is_row_valid, axis=1)]\n",
    "    except FileNotFoundError:\n",
    "        remaining = contract_details.copy()\n",
    "        \n",
    "    remaining = remaining[~remaining['longName'].apply(has_bad_multiplier)]\n",
    "    remaining = remaining[['symbol', 'exchange', 'primaryExchange', 'validExchanges', 'currency', 'conId', 'longName', 'stockType', 'isin']]\n",
    "    return remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning functions\n",
    "def clean_labels(label, col):\n",
    "    if col == 'industries':\n",
    "        if isinstance(label, str):\n",
    "            if label.endswith('-Discontinuedeff09/19/2020'):\n",
    "                return label.split('-')[0]\n",
    "        return label\n",
    "    \n",
    "    elif col == 'holding_types':\n",
    "        if isinstance(label, str):\n",
    "            if label.startswith('■'):\n",
    "                return label[1:]\n",
    "            elif label.startswith('1'):\n",
    "                return label[1:]\n",
    "        return label\n",
    "    elif col == 'debtors':\n",
    "        if isinstance(label, str):\n",
    "            if ('（') in label:\n",
    "                return label.replace('（', '(')\n",
    "        return label\n",
    "    elif col == 'fundamentals':\n",
    "        if isinstance(label, str):\n",
    "            if label == 'LTDebt/ShareholdersEquity':\n",
    "                return 'LTDebt/Shareholders'\n",
    "        return label\n",
    "    return label\n",
    "    \n",
    "def correct_digit(value_str):\n",
    "    try:\n",
    "        digit = re.sub(r'[^\\d.-]', '', value_str).strip()\n",
    "        return float(digit)\n",
    "    except Exception:\n",
    "        return value_str\n",
    "\n",
    "def clean_values(value_str, col):\n",
    "    # print(value_str)\n",
    "    if col == 'profile':\n",
    "        return value_str\n",
    "    if isinstance(value_str, str):\n",
    "        if value_str.endswith('%'):\n",
    "            return correct_digit(value_str.replace('%',''))/100\n",
    "        try:\n",
    "            return correct_digit(value_str)\n",
    "        except Exception:\n",
    "            return value_str\n",
    "    return value_str\n",
    "\n",
    "def clean_df(df):\n",
    "    for col in df.columns:\n",
    "        # print(col)\n",
    "        df[col] = df[col].apply(evaluate_literal)\n",
    "        df[col] = df[col].apply(lambda x: [(clean_labels(item[0], col), item[1]) if isinstance(item, tuple) and len(item) == 2 else item for item in x] if isinstance(x, list) else x)\n",
    "        df[col] = df[col].apply(lambda x: [(item[0], clean_values(item[1], col)) if isinstance(item, tuple) and len(item) == 2 else item for item in x] if isinstance(x, list) else x)\n",
    "        df[col] = df[col].apply(lambda x: sorted(x, key=lambda item: item[0] if isinstance(item, tuple) and item[0] else '') if isinstance(x, list) else x)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning functions 2\n",
    "def clean_labels(label, col):\n",
    "    if col == 'industries':\n",
    "        if isinstance(label, str):\n",
    "            if label.endswith('-Discontinuedeff09/19/2020'):\n",
    "                return label.split('-')[0]\n",
    "        return label\n",
    "    \n",
    "    elif col == 'holding_types':\n",
    "        if isinstance(label, str):\n",
    "            if label.startswith('■'):\n",
    "                return label[1:]\n",
    "            elif label.startswith('1'):\n",
    "                return label[1:]\n",
    "        return label\n",
    "    elif col == 'debtors':\n",
    "        if isinstance(label, str):\n",
    "            if ('（') in label:\n",
    "                return label.replace('（', '(')\n",
    "        return label\n",
    "    elif col == 'fundamentals':\n",
    "        if isinstance(label, str):\n",
    "            if label == 'LTDebt/ShareholdersEquity':\n",
    "                return 'LTDebt/Shareholders'\n",
    "        return label\n",
    "    return label\n",
    "    \n",
    "def correct_digit(value_str):\n",
    "    try:\n",
    "        digit = re.sub(r'[^\\d.-]', '', value_str).strip()\n",
    "        return float(digit)\n",
    "    except Exception:\n",
    "        return value_str\n",
    "\n",
    "def clean_values(value_str, col):\n",
    "    # print(value_str)\n",
    "    if col == 'profile':\n",
    "        return value_str\n",
    "    if isinstance(value_str, str):\n",
    "        if value_str.endswith('%'):\n",
    "            return correct_digit(value_str.replace('%',''))/100\n",
    "        try:\n",
    "            return correct_digit(value_str)\n",
    "        except Exception:\n",
    "            return value_str\n",
    "    return value_str\n",
    "\n",
    "def clean_df(df):\n",
    "    for col in df.columns:\n",
    "        # print(col)\n",
    "        df[col] = df[col].apply(evaluate_literal)\n",
    "        df[col] = df[col].apply(lambda x: [(clean_labels(item[0], col), item[1]) if isinstance(item, tuple) and len(item) == 2 else item for item in x] if isinstance(x, list) else x)\n",
    "        df[col] = df[col].apply(lambda x: [(item[0], clean_values(item[1], col)) if isinstance(item, tuple) and len(item) == 2 else item for item in x] if isinstance(x, list) else x)\n",
    "        df[col] = df[col].apply(lambda x: sorted(x, key=lambda item: item[0] if isinstance(item, tuple) and item[0] else '') if isinstance(x, list) else x)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode columns\n",
    "contracts_df = load('data/contract_elaborated.csv')\n",
    "contracts_df = clean_df(contracts_df)\n",
    "contracts_df = contracts_df[contracts_df.apply(is_row_valid, axis=1)]\n",
    "\n",
    "contracts_df['bond'] = contracts_df[['debtors', 'maturity', 'debt_type']].notna().any(axis=1).astype(bool)\n",
    "# contracts_df['bond'].replace(0.0, np.nan, inplace=True)\n",
    "contracts_df[['equity', 'other']] = False\n",
    "\n",
    "empty_subcategories = {\n",
    "'holding_types': ['Other'],\n",
    "'countries': ['Unidentified'], \n",
    "'currencies': ['<NoCurrency>'],\n",
    "'industries': ['NonClassifiedEquity', 'NotClassified-NonEquity'],\n",
    "'top10': ['AccountsPayable','AccountsReceivable','AccountsReceivable&Pay','AdministrationFees','CustodyFees','ManagementFees','OtherAssets','OtherAssetsandLiabilities','OtherAssetslessLiabilities',\n",
    "          'OtherFees','OtherLiabilities','Tax','Tax--ManagementFees'],\n",
    "'debtors': ['OTHER'],\n",
    "'debt_type': ['%QualityNotAvailable', '%QualityNotRated'],\n",
    "'maturity': ['%MaturityOther'],\n",
    "}\n",
    "\n",
    "original_columns = contracts_df.columns\n",
    "columns_to_explode = ['profile', 'holding_types', 'style', 'lipper', 'fundamentals', 'countries', 'currencies', 'industries', 'top10', 'debtors', 'maturity', 'debt_type', 'dividends']\n",
    "percentage_columns = [col for col in ['holding_types', 'countries', 'currencies', 'industries', 'top10', 'debtors', 'maturity', 'debt_type'] if col in columns_to_explode]\n",
    "for col in columns_to_explode:\n",
    "    # print(col)\n",
    "    contracts_df[col] = contracts_df[col].fillna('[]')\n",
    "    contracts_df[col] = contracts_df[col].apply(evaluate_literal)\n",
    "\n",
    "    # Explode and create pivot_df\n",
    "    contracts_df = contracts_df.explode(col)\n",
    "    contracts_df[col] = contracts_df[col].apply(lambda x: (None, None) if pd.isna(x) else x)\n",
    "    contracts_df[['label', 'value']] = pd.DataFrame(contracts_df[col].tolist(), index=contracts_df.index)\n",
    "\n",
    "    pivot_df = contracts_df.pivot_table(index=contracts_df.index, columns='label', values='value', aggfunc='first')\n",
    "    pivot_df.rename(columns={label: f'{col}_{label}' for label in pivot_df.columns}, inplace=True)\n",
    "\n",
    "    # Correct pivot_vf values\n",
    "    if col in percentage_columns:\n",
    "        pivot_df = pivot_df.fillna(0.0).clip(lower=0)\n",
    "        columns_to_drop = [f'{col}_{label}' for label in empty_subcategories[col]]\n",
    "        pivot_cols = [pivot_col for pivot_col in pivot_df.columns if pivot_col not in columns_to_drop]\n",
    "\n",
    "        # Scale error values so all sum to 1\n",
    "        pivot_cols_sum = pivot_df.sum(axis=1)\n",
    "        mask = pivot_cols_sum > 1\n",
    "        pivot_df.loc[mask] = pivot_df.loc[mask].div(pivot_cols_sum[mask], axis=0)\n",
    "\n",
    "        # Create variety columns\n",
    "        pivot_df[f'{col}_variety'] = pivot_df.pow(2).sum(axis=1)\n",
    "        pivot_df[f'{col}_variety'] = pivot_df[f'{col}_variety'].astype(float).replace(0.0, np.nan)\n",
    "\n",
    "        # # To avoid multicollinearity\n",
    "        # pivot_df = pivot_df.drop(columns=columns_to_drop, axis=1, errors='ignore')\n",
    "\n",
    "        # Drop top10 company columns\n",
    "        if col == 'top10':\n",
    "            columns_to_drop = [column for column in pivot_df.columns if column != f'{col}_variety']\n",
    "            pivot_df = pivot_df.drop(columns=columns_to_drop, axis=1)\n",
    "\n",
    "    contracts_df = contracts_df.drop(columns=[col, 'label', 'value'], axis=1).drop_duplicates(subset='conId')\n",
    "    contracts_df = pd.concat([contracts_df, pivot_df], axis=1)\n",
    "\n",
    "contracts_df = contracts_df[~contracts_df['profile_TotalNetAssets'].isna()]\n",
    "\n",
    "# # Fill NaN values with 0.0 for percentage columns\n",
    "# for col in percentage_columns + [col for col in contracts_df.columns if col.startswith('dividends_')]:\n",
    "#     full_columns = [full_column for full_column in contracts_df.columns if full_column.startswith(col) and full_column != f'{col}_variety']\n",
    "#     contracts_df[full_columns] = contracts_df[full_columns].fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop ETF duplicates\n",
    "eur_exchanges = contracts_df[contracts_df['currency'] == 'EUR'].primaryExchange.unique()\n",
    "remaining_columns = [col for col in contracts_df.columns if col not in original_columns]\n",
    "og_len = len(contracts_df)\n",
    "\n",
    "contracts_df = (\n",
    "    contracts_df\n",
    "    .assign(currency_is_euro=contracts_df['currency'] == 'EUR')\n",
    "    .assign(exchange_is_european=contracts_df['exchange'].isin(eur_exchanges))\n",
    "    .assign(primary_is_european=contracts_df['primaryExchange'].isin(eur_exchanges))\n",
    "    .sort_values(by=['currency_is_euro','exchange_is_european', 'primary_is_european', 'tradable'], ascending=[False, False, False, False])\n",
    "    .drop_duplicates(subset=remaining_columns, keep='first')\n",
    "    .drop(columns=['currency_is_euro', 'exchange_is_european', 'primary_is_european'])\n",
    ")\n",
    "og_len - len(contracts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct profile total net assets and TER\n",
    "symbol_mapping = {\n",
    "    '$': 'USD',    # Default to USD\n",
    "    '￥': 'JPY',    # Japanese Yen\n",
    "    'Rs': 'INR',\n",
    "    'CNH': 'CNY',\n",
    "    '€': 'EUR',    # Euro\n",
    "    '¥': 'JPY',    # Alternative Yen symbol\n",
    "    '£': 'GBP',    # British Pound\n",
    "    'A$': 'AUD',   # Australian Dollar\n",
    "    'C$': 'CAD',   # Canadian Dollar\n",
    "    'HK$': 'HKD',  # Hong Kong Dollar\n",
    "}\n",
    "\n",
    "def standardize_currency(currency):\n",
    "    if pd.isna(currency):\n",
    "        return np.nan\n",
    "    if currency in symbol_mapping:\n",
    "        return symbol_mapping[currency]\n",
    "    if currency == '':\n",
    "        return ''\n",
    "    try:\n",
    "        if pycountry.currencies.get(alpha_3=currency):\n",
    "            return currency\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    return currency\n",
    "\n",
    "def clean_total_net_assets(value):\n",
    "    if pd.isna(value):\n",
    "        return np.nan, np.nan\n",
    "    value = re.sub(r'\\basof\\b.*', '', value, flags=re.IGNORECASE).strip()\n",
    "    match = re.match(r'([^0-9\\s]+)?\\s*([0-9.,]+)\\s*([kKmMbB]?)', value)\n",
    "    if not match:\n",
    "        return np.nan, np.nan\n",
    "    currency, num_str, unit = match.groups()\n",
    "    currency = currency if currency else ''\n",
    "    num = float(num_str.replace(',', ''))\n",
    "    unit = unit.lower() if unit else ''\n",
    "    if unit == 'k':\n",
    "        num *= 10**3\n",
    "    elif unit == 'm':\n",
    "        num *= 10**6\n",
    "    elif unit == 'b':\n",
    "        num *= 10**9\n",
    "    elif unit == 't':\n",
    "        num *= 10**12\n",
    "    return num, currency\n",
    "\n",
    "def get_exchange_rates(currencies, to_currency='USD'):\n",
    "    rates = {}\n",
    "    valid_currencies = []\n",
    "    for c in currencies:\n",
    "        if pd.notna(c) and pycountry.currencies.get(alpha_3=c) is not None:\n",
    "            valid_currencies.append(c)\n",
    "    if not valid_currencies:\n",
    "        return rates\n",
    "    try:\n",
    "        url = f\"https://open.er-api.com/v6/latest/{to_currency}\"\n",
    "        response = requests.get(url)\n",
    "        data = response.json()\n",
    "        if 'rates' in data:\n",
    "            for currency in valid_currencies:\n",
    "                if currency == 'USD':\n",
    "                    rates[currency] = 1.0\n",
    "                elif currency in data['rates']:\n",
    "                    rates[currency] = 1 / data['rates'][currency] if data['rates'][currency] != 0 else np.nan\n",
    "            # print(f\"Fetched rates: {rates}\")\n",
    "            return rates\n",
    "        else:\n",
    "            print(f\"Error fetching rates: {data.get('error', 'Unknown error')}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Exchange rate fetch failed: {e}\")\n",
    "    return rates\n",
    "\n",
    "def convert_to_usd(row, rates):\n",
    "    if pd.isna(row['profile_cap']) or pd.isna(row['profile_cap_currency']):\n",
    "        return np.nan\n",
    "    currency = row['profile_cap_currency']\n",
    "    if currency in rates:\n",
    "        return row['profile_cap'] * rates[currency]\n",
    "    print(f\"No rate available for {currency}\")\n",
    "    return np.nan\n",
    "\n",
    "contracts_df[['profile_cap', 'profile_cap_currency']] = contracts_df['profile_TotalNetAssets'].apply(lambda x: pd.Series(clean_total_net_assets(x)))\n",
    "contracts_df['profile_cap_currency'] = contracts_df['profile_cap_currency'].apply(standardize_currency)\n",
    "contracts_df['profile_cap_currency'] = np.where(contracts_df['profile_cap_currency'] == '', contracts_df['currency'], contracts_df['profile_cap_currency'])\n",
    "contracts_df['profile_cap_currency'] = contracts_df['profile_cap_currency'].apply(lambda x: x if (pd.isna(x) or pycountry.currencies.get(alpha_3=x) or x == '') else np.nan)\n",
    "\n",
    "exchange_rates = get_exchange_rates(contracts_df['profile_cap_currency'].unique())\n",
    "contracts_df['profile_cap_usd'] = contracts_df.apply(lambda row: convert_to_usd(row, exchange_rates),axis=1)\n",
    "contracts_df = contracts_df.drop(columns=['profile_TotalNetAssets', 'profile_cap', 'profile_cap_currency'], axis=1, errors='ignore')\n",
    "\n",
    "# TER\n",
    "contracts_df['profile_TotalExpenseRatio'] = contracts_df['profile_TotalExpenseRatio'].replace('', np.nan).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domicile dummies\n",
    "if 'profile_Domicile' in contracts_df.columns:\n",
    "    dummies = pd.get_dummies(contracts_df['profile_Domicile'], prefix='domicile').astype(int)\n",
    "    contracts_df = pd.concat([contracts_df, dummies], axis=1)\n",
    "    contracts_df.drop('profile_Domicile', axis=1, inplace=True)\n",
    "\n",
    "# Market cap dummies\n",
    "size_map = {\n",
    "        'Small-cap': 'small',\n",
    "        'Mid-cap': 'mid',\n",
    "        'BroadMarket': 'multi',\n",
    "        'Large-cap': 'large',\n",
    "    }\n",
    "contracts_df['profile_MarketCapFocus'] = contracts_df['profile_MarketCapFocus'].map(size_map)\n",
    "\n",
    "dummies = pd.get_dummies(contracts_df['profile_MarketCapFocus'], dtype=int)\n",
    "contracts_df = pd.concat([contracts_df, dummies], axis=1)\n",
    "\n",
    "# Define mapping from size to related style columns\n",
    "style_groups = {\n",
    "    'small': ['style_small-core', 'style_small-growth', 'style_small-value'],\n",
    "    'mid': ['style_mid-core', 'style_mid-growth', 'style_mid-value'],\n",
    "    'large': ['style_large-core', 'style_large-growth', 'style_large-value'],\n",
    "    'multi': ['style_multi-core', 'style_multi-growth', 'style_multi-value']\n",
    "}\n",
    "\n",
    "# Update each size column by OR-ing with the style columns\n",
    "for size, cols in style_groups.items():\n",
    "    contracts_df[size] = contracts_df[size] | contracts_df[cols].any(axis=1).astype(int)\n",
    "\n",
    "del dummies, size_map, style_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search exchange verification\n",
    "contracts_df['search_exchange'] = contracts_df['search_exchange'].str.extract(r'\\(([^()]*)\\)', expand=False)\n",
    "contracts_df['validExchanges'] = contracts_df['validExchanges'].apply(lambda x: x.split(','))\n",
    "\n",
    "def validate_search_exchange(row):\n",
    "    # if pd.isna(row['search_exchange']):\n",
    "    #     return np.nan\n",
    "    return 1 if row['search_exchange'] in row['validExchanges'] else 0\n",
    "\n",
    "contracts_df['valid_search_exchange'] = contracts_df.apply(validate_search_exchange, axis=1)\n",
    "contracts_df = contracts_df.drop(columns=['search_symbol', 'search_exchange', 'validExchanges'], axis=1, errors='ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final cleaning\n",
    "fill_dict = {\n",
    "    'holding_types_Equity': 0,\n",
    "    'holding_types_FixedIncome': 0,\n",
    "    'holding_types_Cash': 0,\n",
    "    'holding_types_Other': 1,\n",
    "    'holding_types_variety': 1,\n",
    "}\n",
    "contracts_df = contracts_df.fillna(fill_dict)\n",
    "\n",
    "# rating encoding\n",
    "rating_map = {\n",
    "    'AAA': 6,\n",
    "    'AA': 5,\n",
    "    'A': 4,\n",
    "    'BBB': 3,\n",
    "    'BB': 2,\n",
    "    'B': 1,\n",
    "}\n",
    "contracts_df['fundamentals_AverageQuality'] = contracts_df['fundamentals_AverageQuality'].map(rating_map)\n",
    "\n",
    "# # bool rename\n",
    "# bool_map = {\n",
    "#     True: 1,\n",
    "#     False: 0,\n",
    "#     np.nan: 0,\n",
    "# }\n",
    "# bool_cols = [col for col in contracts_df.columns if col.startswith('style_')] + ['tradable']\n",
    "# for col in bool_cols:\n",
    "#     contracts_df[col] = contracts_df[col].map(bool_map)\n",
    "\n",
    "contracts_df.loc[contracts_df['stockType'] == 'ETC', 'industries_BasicMaterials'] = 1.0\n",
    "\n",
    "# Remove unnecessary qual or empty columns, only keep key identifiers\n",
    "qual_cols = ['primaryExchange', 'stockType', 'date_scraped', 'exchange_bug', 'exact_search', 'search_symbol', 'profile_MarketCapFocus', 'profile_MarketGeoFocus', 'profile_BenchmarkIndex', 'profile_FundCategory', 'dividends_PayoutRatio']\n",
    "contracts_df = contracts_df.drop(columns=qual_cols, axis=1, errors='ignore')\n",
    "\n",
    "# Clean remaining numerical columns\n",
    "identifier_cols = ['symbol', 'conId', 'longName', 'isin', 'exchange', 'currency', 'profile_MarketCapFocus']\n",
    "for col in [c for c in contracts_df.columns if c not in identifier_cols]:\n",
    "    temp_type = contracts_df[col].dtype\n",
    "    if temp_type == 'object':\n",
    "        contracts_df[col] = contracts_df[col].apply(lambda x: np.nan if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation & Initial Setup:\n",
    "stylebox_cols = [col for col in contracts_df.columns if col.startswith('style_')]\n",
    "\n",
    "value_metric_cols = ['fundamentals_Price/Book', 'fundamentals_Price/Sales',\n",
    "                     'fundamentals_Price/Cash', 'fundamentals_Price/Earnings']\n",
    "                     # Optional: 'fundamentals_LTDebt/Shareholders', 'fundamentals_TotalDebt/TotalEquity'\n",
    "growth_metric_cols = ['fundamentals_EPSGrowth-1yr', 'fundamentals_EPS_growth_3yr',\n",
    "                      'fundamentals_EPS_growth_5yr']\n",
    "                      # Optional: 'fundamentals_ReturnonAssets1Yr', 'fundamentals_SalestoTotalAssets'\n",
    "\n",
    "bond_fundamental_cols = ['fundamentals_AverageCoupon', 'fundamentals_AverageQuality', 'fundamentals_YieldtoMaturity', 'fundamentals_NominalMaturity', 'fundamentals_EffectiveMaturity']\n",
    "equity_fundamental_cols = [col for col in contracts_df.columns if col.startswith('fundamentals_') if col not in bond_fundamental_cols]\n",
    "\n",
    "numerical_cols = [col for col in contracts_df.columns if contracts_df[col].dtype in [np.int64, np.float64] and col not in ['conId']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split by asset class\n",
    "bond_mask = contracts_df[bond_fundamental_cols].notna().any(axis=1)\n",
    "equity_mask = contracts_df[equity_fundamental_cols].notna().any(axis=1)\n",
    "contracts_df['bond'] = contracts_df['bond'] | bond_mask\n",
    "contracts_df['other'] = ~(equity_mask | contracts_df['bond'])\n",
    "contracts_df['other'] = contracts_df['other'] | (equity_mask & contracts_df['bond'])\n",
    "contracts_df['equity'] = ~(contracts_df['bond'] | contracts_df['other'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contracts_df[contracts_df.equity][[col for col in contracts_df if col.startswith('holding_types')]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contracts_df[contracts_df.bond][[col for col in contracts_df if col.startswith('holding_types')]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contracts_df['holding_types_other'] = contracts_df['holding_types_Cash'] + contracts_df['holding_types_Other']\n",
    "\n",
    "bond_threshold = contracts_df[contracts_df.bond]['holding_types_FixedIncome'].median() - contracts_df[contracts_df.bond]['holding_types_FixedIncome'].std()\n",
    "equity_threshold = contracts_df[contracts_df.equity]['holding_types_Equity'].median() - contracts_df[contracts_df.equity]['holding_types_Equity'].std()\n",
    "other_threshold = contracts_df[contracts_df.other]['holding_types_other'].median() - contracts_df[contracts_df.other]['holding_types_other'].std()\n",
    "\n",
    "np.array([bond_threshold, equity_threshold, other_threshold]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(contracts_df[contracts_df.bond]['holding_types_other'].describe())\n",
    "display(contracts_df[contracts_df.bond]['holding_types_Equity'].describe())\n",
    "display(contracts_df[contracts_df.bond]['holding_types_FixedIncome'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contracts_df['holding_types_other'] = contracts_df['holding_types_Cash'] + contracts_df['holding_types_Other']\n",
    "contracts_df = contracts_df.rename(columns={'holding_types_FixedIncome': 'holding_types_bond',\n",
    "                                            'holding_types_Equity': 'holding_types_equity'})\n",
    "\n",
    "holding_types = {'bond', 'equity', 'other'}\n",
    "lower_thresholds = []\n",
    "for holding_type in holding_types:\n",
    "    for other_type in holding_types - {holding_type}:\n",
    "        median = contracts_df[contracts_df[holding_type]][f'holding_types_{other_type}'].median()\n",
    "        std = contracts_df[contracts_df[holding_type]][f'holding_types_{other_type}'].std()\n",
    "        lower_thresholds.append(median + std)\n",
    "lower_threshold = np.array(lower_thresholds).mean()\n",
    "lower_threshold\n",
    "# bond_threshold = contracts_df[contracts_df.bond]['holding_types_FixedIncome'].mean()\n",
    "# equity_threshold = contracts_df[contracts_df.equity]['holding_types_Equity'].mean()\n",
    "# other_threshold = contracts_df[contracts_df.other]['holding_types_other'].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contracts_df['holding_types_other'] = contracts_df['holding_types_Cash'] + contracts_df['holding_types_Other']\n",
    "\n",
    "bond_mean = contracts_df[contracts_df.bond]['holding_types_FixedIncome'].mean()\n",
    "equity_mean = contracts_df[contracts_df.equity]['holding_types_Equity'].mean()\n",
    "other_mean = contracts_df[contracts_df.other]['holding_types_other'].mean()\n",
    "\n",
    "np.array([bond_mean, equity_mean, other_mean]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = contracts_df.iloc[0]\n",
    "\n",
    "row[['holding_types_bond', 'holding_types_equity', 'holding_types_other']].sort_values()#.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "contracts_df['holding_types_other'] = contracts_df['holding_types_Cash'] + contracts_df['holding_types_Other']\n",
    "contracts_df = contracts_df.rename(columns={'holding_types_FixedIncome': 'holding_types_bond', 'holding_types_Equity': 'holding_types_equity'})\n",
    "\n",
    "bond_threshold = contracts_df[contracts_df.bond]['holding_types_bond'].median() - contracts_df[contracts_df.bond]['holding_types_bond'].std()\n",
    "equity_threshold = contracts_df[contracts_df.equity]['holding_types_equity'].median() - contracts_df[contracts_df.equity]['holding_types_equity'].std()\n",
    "other_threshold = contracts_df[contracts_df.other]['holding_types_other'].median() - contracts_df[contracts_df.other]['holding_types_other'].std()\n",
    "upper_threshold = np.array([bond_threshold, equity_threshold, other_threshold]).mean()\n",
    "\n",
    "\n",
    "holding_types = {'bond', 'equity', 'other'}\n",
    "lower_thresholds = []\n",
    "for holding_type in holding_types:\n",
    "    for other_type in holding_types - {holding_type}:\n",
    "        median = contracts_df[contracts_df[holding_type]][f'holding_types_{other_type}'].median()\n",
    "        std = contracts_df[contracts_df[holding_type]][f'holding_types_{other_type}'].std()\n",
    "        lower_thresholds.append(median + std)\n",
    "lower_threshold = np.array(lower_thresholds).mean()\n",
    "\n",
    "\n",
    "def refine_classification(row):\n",
    "    bond = row['holding_types_bond']\n",
    "    equity = row['holding_types_equity']\n",
    "    other = row['holding_types_other']\n",
    "    max_holding = row[['holding_types_bond', 'holding_types_equity', 'holding_types_other']].sort_values().tail(1)\n",
    "    \n",
    "    if bond > upper_threshold:\n",
    "        return pd.Series([True, False, False], index=['bond', 'equity', 'other'])\n",
    "    elif equity > upper_threshold:\n",
    "        return pd.Series([False, True, False], index=['bond', 'equity', 'other'])\n",
    "    elif other > upper_threshold:\n",
    "        return pd.Series([False, False, True], index=['bond', 'equity', 'other'])\n",
    "    \n",
    "    elif bond < lower_threshold or equity < lower_threshold:\n",
    "        # return True for column corresponding to max_holding\n",
    "    else:\n",
    "        return pd.Series([False, False, True], index=['bond', 'equity', 'other'])\n",
    "\n",
    "contracts_df[['bond', 'equity', 'other']] = contracts_df.apply(refine_classification, axis=1)\n",
    "contracts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "contracts_df = contracts_df.rename(columns={'other': 'other_and_mixed'})\n",
    "contracts_df['holding_types_other'] = contracts_df['holding_types_Cash'] + contracts_df['holding_types_Other']\n",
    "\n",
    "bond_threshold = contracts_df[contracts_df['bond']]['holding_types_FixedIncome'].mean()\n",
    "equity_threshold = contracts_df[contracts_df['equity']]['holding_types_Equity'].mean()\n",
    "\n",
    "def refine_classification(row):\n",
    "    fixed_income = row['holding_types_FixedIncome']\n",
    "    equity = row['holding_types_Equity']\n",
    "    other = row['holding_types_other']\n",
    "    \n",
    "    max_holding = max(fixed_income, equity, other)\n",
    "    \n",
    "    if fixed_income == max_holding and fixed_income > bond_threshold:\n",
    "        return pd.Series([True, False, False], index=['bond', 'equity', 'other_and_mixed'])\n",
    "    # Classify as equity if Equity exceeds threshold and is dominant\n",
    "    elif equity == max_holding and equity > equity_threshold:\n",
    "        return pd.Series([False, True, False], index=['bond', 'equity', 'other_and_mixed'])\n",
    "    # Otherwise, classify as other_and_mixed (mixed or non-dominant)\n",
    "    else:\n",
    "        return pd.Series([False, False, True], index=['bond', 'equity', 'other_and_mixed'])\n",
    "\n",
    "# Step 5: Apply the refinement\n",
    "contracts_df[['bond', 'equity', 'other_and_mixed']] = contracts_df.apply(refine_classification, axis=1)\n",
    "\n",
    "# Step 6: Validate holding_types sum\n",
    "contracts_df['holding_sum'] = (contracts_df['holding_types_FixedIncome'] + \n",
    "                              contracts_df['holding_types_Equity'] + \n",
    "                              contracts_df['holding_types_other'])\n",
    "# Warn if sums deviate significantly from 1\n",
    "invalid_sums = contracts_df[(contracts_df['holding_sum'] < 0.9) | (contracts_df['holding_sum'] > 1.1)]\n",
    "if not invalid_sums.empty:\n",
    "    print(f\"Warning: {len(invalid_sums)} rows have holding_types sums outside [0.9, 1.1].\")\n",
    "\n",
    "# Step 7: Drop temporary column\n",
    "contracts_df = contracts_df.drop(columns=['holding_sum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute fundamental values\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Equity Fundamentals\n",
    "all_equity_cols = [col for col in numerical_cols if col not in bond_fundamental_cols]\n",
    "# equity_mask = contracts_df[equity_fundamental_cols].notna().any(axis=1)\n",
    "temp_df = contracts_df.loc[equity_mask, [col for col in numerical_cols if col not in bond_fundamental_cols]].copy()\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=10)\n",
    "imputed_values = imputer.fit_transform(temp_df)\n",
    "\n",
    "imputed_df = pd.DataFrame(imputed_values, columns=all_equity_cols, index=temp_df.index)\n",
    "contracts_df.loc[equity_mask, all_equity_cols] = imputed_df\n",
    "\n",
    "# Bond Fundamentals\n",
    "all_bond_cols = [col for col in numerical_cols if col not in equity_fundamental_cols]\n",
    "# bond_mask = contracts_df[bond_fundamental_cols].notna().any(axis=1)\n",
    "temp_df = contracts_df.loc[bond_mask, all_bond_cols].copy()\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=10)\n",
    "imputed_values = imputer.fit_transform(temp_df)\n",
    "\n",
    "imputed_df = pd.DataFrame(imputed_values, columns=all_bond_cols, index=temp_df.index)\n",
    "contracts_df.loc[bond_mask, all_bond_cols] = imputed_df\n",
    "\n",
    "del temp_df, imputed_df, imputed_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "# contracts_df.to_csv('data/fundamentals.csv', index=False)\n",
    "contracts_df#[~(equity_mask + bond_mask)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STYLEBOX MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://gemini.google.com/app/8cafe6813426884b\n",
    "\n",
    "Okay, I can provide you with a comprehensive Python script to implement the strategy we discussed for filling in missing Morningstar stylebox assignments.\n",
    "\n",
    "1. Set up a sample DataFrame similar to yours, including missing values.\n",
    "2. Preprocess the data:\n",
    "Identify style and fundamental columns.\n",
    "Impute missing fundamental values using KNNImputer.\n",
    "3. Determine the \"Size\" dimension:\n",
    "Calculate capitalization thresholds from non-missing data.\n",
    "Categorize ETFs into 'Large', 'Mid', or 'Small' cap.\n",
    "4. Determine the \"Style\" dimension (Value, Core, Growth):\n",
    "Engineer features: Normalize relevant fundamentals and calculate 'Value Score' and 'Growth Score'.\n",
    "Train a RandomForestClassifier on the subset of data with existing style assignments.\n",
    "Predict the style for ETFs with missing assignments.\n",
    "5. Combine Size and Style: Populate the original boolean stylebox columns based on the determined size and predicted style.\n",
    "\n",
    "This code provides a full pipeline.\n",
    "\n",
    "\n",
    "## Key things to note and potential refinements:\n",
    "\n",
    "1. Sample Data: The create_sample_df function generates data that tries to mimic your description. You'll replace this with loading your actual DataFrame.\n",
    "2. Imputation of Fundamentals: KNNImputer is used. You might experiment with other methods or parameters.\n",
    "3. Size Thresholds: The logic for small_max_cap and mid_max_cap is based on the distribution in your labeled data. This is a reasonable starting point. In practice, these thresholds are often defined by index providers (like Morningstar, Russell, S&P) and can change over time. If you have access to official breakpoints, using those would be more accurate.\n",
    "4. Feature Scaling: MinMaxScaler is used for normalizing fundamentals before calculating scores and for model input. This is crucial.\n",
    "5. Value and Growth Scores: The formulas you provided are implemented. The normalization ensures that for value metrics, lower raw values (like P/B) result in higher components of the Value Score, and for growth metrics, higher raw values (like EPS Growth) result in higher components of the Growth Score.\n",
    "6. Style Model: RandomForestClassifier is used. It's a good general-purpose classifier. You can experiment with others (Logistic Regression, Gradient Boosting, SVM). class_weight='balanced' is used to handle potential imbalances in Value/Core/Growth categories.\n",
    "7. \"Multi\" Categories: This script primarily focuses on assigning to the nine Large/Mid/Small x Value/Core/Growth boxes. The \"multi\" categories are more complex and would typically require analysis of the fund's holdings diversification across cap tiers. The current code ensures that if a L/M/S style is imputed, the multi-style columns for that ETF are set to 0.\n",
    "8. Error Handling & Edge Cases: Basic checks are included (e.g., if no labeled data is available). Real-world data can have many more quirks.\n",
    "9. Evaluation: The script prints a classification report for the style model on a test set. Thoroughly evaluating the quality of imputations is important (e.g., by comparing with a holdout set if you manually classify some, or by checking the logical consistency of results).\n",
    "10. Column Names: Ensure the column names in the script match exactly with your DataFrame.\n",
    "11. Fallback for Style Prediction: If the style model cannot be trained (e.g., too few distinct style labels), it currently defaults to predicting 'Core'. You might want a different strategy.\n",
    "To use this:\n",
    "\n",
    "\n",
    "## Details\n",
    "Adjust column name lists (style_cols, equity_fundamental_cols, etc.) if they differ.\n",
    "Run the script. The imputed_df will be your DataFrame with the stylebox NaNs filled according to the logic.\n",
    "The final part of the if __name__ == '__main__': block shows how you could add the calculated Value_Score_Display and Growth_Score_Display to the final DataFrame for easier inspection of the model's inputs.\n",
    "This should give you a very solid starting point!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def get_size_category_from_style_cols(row, style_cols_map):\n",
    "    \"\"\"Extracts size (Large, Mid, Small, Multi) from boolean style columns.\"\"\"\n",
    "    for style_col_name, assignments in style_cols_map.items():\n",
    "        if row[style_col_name] == 1:\n",
    "            return assignments['size']\n",
    "    return np.nan\n",
    "\n",
    "def get_value_growth_category_from_style_cols(row, style_cols_map):\n",
    "    \"\"\"Extracts style (Value, Growth, Core) from boolean style columns.\"\"\"\n",
    "    for style_col_name, assignments in style_cols_map.items():\n",
    "        if row[style_col_name] == 1:\n",
    "            return assignments['style']\n",
    "    return np.nan\n",
    "\n",
    "def impute_styleboxes(df_input):\n",
    "    \"\"\"\n",
    "    Fills missing Morningstar stylebox assignments using a hybrid approach.\n",
    "    \"\"\"\n",
    "    df = df_input.copy()\n",
    "\n",
    "    style_cols = ['style_large-core', 'style_large-growth', 'style_large-value', \n",
    "                  'style_mid-core', 'style_mid-growth', 'style_mid-value', \n",
    "                  'style_small-core', 'style_small-growth', 'style_small-value',\n",
    "                  'style_multi-core', 'style_multi-growth', 'style_multi-value']\n",
    "    \n",
    "    # Define a mapping for easier interpretation of style columns\n",
    "    # This helps in extracting existing size/style and later in reconstructing\n",
    "    style_cols_map = {\n",
    "        'style_large-value': {'size': 'Large', 'style': 'Value'},\n",
    "        'style_large-core': {'size': 'Large', 'style': 'Core'},\n",
    "        'style_large-growth': {'size': 'Large', 'style': 'Growth'},\n",
    "        'style_mid-value': {'size': 'Mid', 'style': 'Value'},\n",
    "        'style_mid-core': {'size': 'Mid', 'style': 'Core'},\n",
    "        'style_mid-growth': {'size': 'Mid', 'style': 'Growth'},\n",
    "        'style_small-value': {'size': 'Small', 'style': 'Value'},\n",
    "        'style_small-core': {'size': 'Small', 'style': 'Core'},\n",
    "        'style_small-growth': {'size': 'Small', 'style': 'Growth'},\n",
    "        # Multi-cap styles (handling these perfectly is complex, focusing on L/M/S)\n",
    "        'style_multi-value': {'size': 'Multi', 'style': 'Value'},\n",
    "        'style_multi-core': {'size': 'Multi', 'style': 'Core'},\n",
    "        'style_multi-growth': {'size': 'Multi', 'style': 'Growth'},\n",
    "    }\n",
    "\n",
    "    # Identify fundamental columns to be used for imputation and feature engineering\n",
    "    equity_fundamental_cols = [\n",
    "        'fundamentals_Price/Book', 'fundamentals_Price/Cash', \n",
    "        'fundamentals_Price/Earnings', 'fundamentals_Price/Sales',\n",
    "        'fundamentals_EPS_growth_1yr', 'fundamentals_EPS_growth_3yr',\n",
    "        'fundamentals_EPS_growth_5yr', 'fundamentals_ReturnonEquity1Yr',\n",
    "        'fundamentals_LTDebt/Shareholders'\n",
    "    ]\n",
    "\n",
    "    # --- 1. Preprocessing: Impute missing fundamental data ---\n",
    "    imputer = KNNImputer(keep_empty_features=True)\n",
    "    df[equity_fundamental_cols] = imputer.fit_transform(df[equity_fundamental_cols])\n",
    "\n",
    "    equity_fundamental_cols += ['profile_cap_usd']\n",
    "\n",
    "    # Identify rows where stylebox information is missing\n",
    "    df['style_is_missing'] = df[style_cols].isnull().all(axis=1)\n",
    "    df_labeled = df[~df['style_is_missing']].copy()\n",
    "    df_to_impute = df[df['style_is_missing']].copy()\n",
    "\n",
    "    if df_labeled.empty:\n",
    "        print(\"Error: No labeled data available to train models or derive thresholds. Cannot proceed.\")\n",
    "        return df_input\n",
    "\n",
    "    # --- 2. Determine Size Dimension (Large, Mid, Small) --- ################# WRONG use 'profile_MarketCapFocus' instead\n",
    "    df_labeled['size_category_actual'] = df_labeled.apply(\n",
    "        lambda row: get_size_category_from_style_cols(row, style_cols_map), axis=1\n",
    "    )\n",
    "    \n",
    "    # Define capitalization thresholds based on labeled data (excluding 'Multi')\n",
    "    cap_thresholds = {}\n",
    "    for size_cat in ['Small', 'Mid', 'Large']:\n",
    "        caps = df_labeled[df_labeled['size_category_actual'] == size_cat]['profile_cap_usd']\n",
    "        if not caps.empty:\n",
    "            cap_thresholds[f'{size_cat.lower()}_cap_min'] = caps.min()\n",
    "            cap_thresholds[f'{size_cat.lower()}_cap_max'] = caps.max()\n",
    "            cap_thresholds[f'{size_cat.lower()}_cap_median'] = caps.median()\n",
    "\n",
    "    # Define boundaries (example logic, can be refined)\n",
    "    small_max_cap = cap_thresholds.get('small_cap_max', df['profile_cap_usd'].quantile(0.33) if not df_labeled.empty else 0)\n",
    "    mid_max_cap = cap_thresholds.get('mid_cap_max', df['profile_cap_usd'].quantile(0.66) if not df_labeled.empty else 0)\n",
    "\n",
    "    if not cap_thresholds: # Fallback if no labeled data for some size categories\n",
    "        print(\"Warning: Not enough labeled data to define all cap thresholds robustly. Using quantiles as fallback.\")\n",
    "        small_max_cap = df['profile_cap_usd'].quantile(0.33)\n",
    "        mid_max_cap = df['profile_cap_usd'].quantile(0.66)\n",
    "\n",
    "\n",
    "    def assign_size_category(cap_usd):\n",
    "        if cap_usd <= small_max_cap:\n",
    "            return 'Small'\n",
    "        elif cap_usd <= mid_max_cap:\n",
    "            return 'Mid'\n",
    "        else:\n",
    "            return 'Large'\n",
    "\n",
    "    df_to_impute['size_category_imputed'] = df_to_impute['profile_cap_usd'].apply(assign_size_category)\n",
    "    print(\"Size dimension determined for missing rows.\\n\")\n",
    "\n",
    "    # --- 3. Determine Style Dimension (Value, Core, Growth) using a Model ---\n",
    "    print(\"Step 3: Determining style dimension (Value/Core/Growth) via classification model...\")\n",
    "    \n",
    "    # Feature Engineering for Style Model\n",
    "    # Normalize relevant fundamental ratios (0 to 1 range)\n",
    "    value_metric_cols = ['fundamentals_Price/Book', 'fundamentals_Price/Sales', \n",
    "                         'fundamentals_Price/Cash', 'fundamentals_Price/Earnings']\n",
    "    growth_metric_cols = ['fundamentals_EPS_growth_1yr', 'fundamentals_EPS_growth_3yr', \n",
    "                          'fundamentals_EPS_growth_5yr']\n",
    "    \n",
    "    # Use a copy for scaling to avoid changing original imputed values in df_labeled/df_to_impute directly for this step\n",
    "    df_scaled_features = df[equity_fundamental_cols].copy()\n",
    "    scaler = MinMaxScaler()\n",
    "    df_scaled_features[value_metric_cols + growth_metric_cols] = scaler.fit_transform(\n",
    "        df_scaled_features[value_metric_cols + growth_metric_cols]\n",
    "    )\n",
    "\n",
    "    # Calculate Value Score: lower P/B, P/S etc. is better for value.\n",
    "    # (1 - Normalized_Metric) makes higher scores better for value if Normalized_Metric is 0 (best) to 1 (worst)\n",
    "    # Our MinMaxScaler maps lowest raw value to 0 and highest to 1.\n",
    "    # So, for P/B, low P/B (good for value) -> 0. (1-0)=1. High P/B (bad for value) -> 1. (1-1)=0. This is correct.\n",
    "    df_scaled_features['Value_Score'] = df_scaled_features[value_metric_cols].apply(\n",
    "        lambda row: np.mean([1 - val for val in row]), axis=1\n",
    "    )\n",
    "    \n",
    "    # Calculate Growth Score: higher EPS growth is better.\n",
    "    # Normalized_Metric is 0 (worst) to 1 (best). This is correct.\n",
    "    df_scaled_features['Growth_Score'] = df_scaled_features[growth_metric_cols].mean(axis=1)\n",
    "\n",
    "    # Prepare data for style classification model\n",
    "    # Target variable: 'Value', 'Core', 'Growth'\n",
    "    df_labeled['style_vg_actual'] = df_labeled.apply(\n",
    "        lambda row: get_value_growth_category_from_style_cols(row, style_cols_map), axis=1\n",
    "    )\n",
    "    \n",
    "    # Filter out 'Multi' styles for training this specific V/G/C model, and also any rows where style_vg_actual is NaN\n",
    "    # (e.g. if a multi-cap was the only thing labeled, or if style_cols were all 0 but not NaN)\n",
    "    df_train_style = df_labeled[\n",
    "        df_labeled['size_category_actual'].isin(['Large', 'Mid', 'Small']) & \n",
    "        df_labeled['style_vg_actual'].notna()\n",
    "    ].copy()\n",
    "\n",
    "\n",
    "    if df_train_style.empty or df_train_style['style_vg_actual'].nunique() < 2:\n",
    "        print(\"Error: Not enough valid labeled data (L/M/S sizes and distinct V/G/C styles) to train style model. Skipping style imputation.\")\n",
    "        # If style model cannot be trained, we can only fill size, or return as is.\n",
    "        # For this example, we'll proceed to fill based on size only if style model fails.\n",
    "        # A more robust solution might involve other fallback logic for style.\n",
    "        predicted_styles = pd.Series(index=df_to_impute.index, dtype='object').fillna('Core') # Fallback to 'Core'\n",
    "    else:\n",
    "        # Features for the model\n",
    "        # We use the scaled features from df_scaled_features, aligning by index\n",
    "        feature_columns_model = ['Value_Score', 'Growth_Score'] + value_metric_cols + growth_metric_cols + ['fundamentals_ReturnonEquity1Yr', 'fundamentals_LTDebt/Shareholders']\n",
    "        \n",
    "        X = df_scaled_features.loc[df_train_style.index, feature_columns_model]\n",
    "        y = df_train_style['style_vg_actual']\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y if y.nunique() > 1 else None)\n",
    "\n",
    "        print(f\"  Training style model with {len(X_train)} samples.\")\n",
    "        style_model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "        style_model.fit(X_train, y_train)\n",
    "\n",
    "        print(\"  Style model training complete. Evaluating on test set:\")\n",
    "        y_pred_test = style_model.predict(X_test)\n",
    "        print(classification_report(y_test, y_pred_test, zero_division=0))\n",
    "\n",
    "        # Predict style for rows that need imputation\n",
    "        X_to_predict_style = df_scaled_features.loc[df_to_impute.index, feature_columns_model]\n",
    "        if not X_to_predict_style.empty:\n",
    "            predicted_styles = style_model.predict(X_to_predict_style)\n",
    "            df_to_impute['style_vg_imputed'] = predicted_styles\n",
    "        else:\n",
    "            df_to_impute['style_vg_imputed'] = 'Core' # Fallback if no rows to impute style for (should not happen if df_to_impute is not empty)\n",
    "        print(\"Style dimension (Value/Core/Growth) predicted for missing rows.\\n\")\n",
    "\n",
    "\n",
    "    # --- 4. Combine Size and Style & Populate Original Columns ---\n",
    "    print(\"Step 4: Combining imputed size and style to fill original stylebox columns...\")\n",
    "    imputed_count = 0\n",
    "    for idx, row in df_to_impute.iterrows():\n",
    "        size_cat = row['size_category_imputed']\n",
    "        # Use predicted style if available, otherwise fallback (e.g. 'Core')\n",
    "        style_cat = row.get('style_vg_imputed', 'Core') \n",
    "\n",
    "        imputed_flag = False\n",
    "        for s_col, assignments in style_cols_map.items():\n",
    "            # Focus on L/M/S for this imputation logic\n",
    "            if assignments['size'] in ['Large', 'Mid', 'Small']:\n",
    "                if assignments['size'] == size_cat and assignments['style'] == style_cat:\n",
    "                    df.loc[idx, s_col] = 1\n",
    "                    imputed_flag = True\n",
    "                else:\n",
    "                    # Ensure other L/M/S style columns for this ETF are 0\n",
    "                    if df.loc[idx, s_col] != 0 : # Check if it was NaN before\n",
    "                         df.loc[idx, s_col] = 0 \n",
    "            else: # For Multi-cap columns, ensure they are 0 if we imputed L/M/S\n",
    "                if df.loc[idx, s_col] != 0:\n",
    "                    df.loc[idx, s_col] = 0\n",
    "        \n",
    "        if imputed_flag:\n",
    "            imputed_count += 1\n",
    "        \n",
    "        # Mark that this row's style has been processed for imputation\n",
    "        df.loc[idx, 'style_is_missing'] = False \n",
    "\n",
    "\n",
    "    # Fill any remaining NaNs in style columns with 0 (e.g. if they were not part of the imputation logic)\n",
    "    df[style_cols] = df[style_cols].fillna(0)\n",
    "\n",
    "    print(f\"Imputed stylebox for {imputed_count} ETFs.\")\n",
    "    print(\"Stylebox imputation process complete.\\n\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# Create sample data\n",
    "raw_df = contracts_df.copy()\n",
    "print(\"Original DataFrame (sample):\")\n",
    "print(raw_df.head())\n",
    "print(f\"\\nMissing style assignments before imputation: {raw_df[style_cols].isnull().all(axis=1).sum()} rows\")\n",
    "print(f\"Missing values in 'fundamentals_Price/Book': {raw_df['fundamentals_Price/Book'].isnull().sum()} rows\\n\")\n",
    "\n",
    "# Perform imputation\n",
    "imputed_df = impute_styleboxes(raw_df)\n",
    "\n",
    "print(\"\\nDataFrame after stylebox imputation (sample):\")\n",
    "# Show some rows that were originally missing styles\n",
    "original_missing_indices = raw_df[raw_df[style_cols].isnull().all(axis=1)].index\n",
    "if not original_missing_indices.empty:\n",
    "    print(imputed_df.loc[original_missing_indices].head(10)[style_cols + ['profile_cap_usd', 'Value_Score', 'Growth_Score', 'size_category_imputed', 'style_vg_imputed']])\n",
    "else:\n",
    "    print(\"No rows had missing styles in the sample (or all were filled by other means).\")\n",
    "\n",
    "print(f\"\\nMissing style assignments after imputation: {imputed_df[style_cols].isnull().all(axis=1).sum()} rows\")\n",
    "# Verify that NaNs in style columns are filled (should be 0 or 1)\n",
    "print(f\"NaNs remaining in any style column: {imputed_df[style_cols].isnull().sum().sum()}\")\n",
    "\n",
    "# Check one imputed row in detail\n",
    "if not original_missing_indices.empty:\n",
    "    example_imputed_ticker = original_missing_indices[0]\n",
    "    print(f\"\\nDetails for an imputed ETF ({example_imputed_ticker}):\")\n",
    "    # Add imputed scores to the final df for inspection if they were calculated\n",
    "    # The scores are on df_scaled_features, need to merge them back if desired for final output\n",
    "    # For now, just show the relevant columns from imputed_df\n",
    "    print(imputed_df.loc[example_imputed_ticker, style_cols + ['profile_cap_usd']])\n",
    "    # To see Value_Score and Growth_Score, they would need to be added to the main 'df' inside 'impute_styleboxes'\n",
    "    # or merged back from 'df_scaled_features'.\n",
    "    # For simplicity in this script, they are intermediate.\n",
    "    # If you want them in the final output, you'd do:\n",
    "    # df = df.join(df_scaled_features[['Value_Score', 'Growth_Score']]) inside impute_styleboxes before returning.\n",
    "\n",
    "\n",
    "# --- Example of how to add scores to the final df for inspection ---\n",
    "# This part is illustrative and would typically be integrated into the main function\n",
    "# Re-run feature engineering part to get scores on the final imputed_df (if not already done within the function)\n",
    "\n",
    "temp_df_for_scores = imputed_df.copy()\n",
    "# Ensure fundamentals are imputed in this temp_df if they weren't already fully imputed on 'imputed_df'\n",
    "# (they should be from the KNNImputer step)\n",
    "\n",
    "equity_fundamental_cols_scores = [\n",
    "    'profile_cap_usd', 'fundamentals_Price/Book', 'fundamentals_Price/Cash', \n",
    "    'fundamentals_Price/Earnings', 'fundamentals_Price/Sales',\n",
    "    'fundamentals_EPS_growth_1yr', 'fundamentals_EPS_growth_3yr',\n",
    "    'fundamentals_EPS_growth_5yr', 'fundamentals_ReturnonEquity1Yr',\n",
    "    'fundamentals_LTDebt/Shareholders'\n",
    "]\n",
    "value_metric_cols_scores = ['fundamentals_Price/Book', 'fundamentals_Price/Sales', \n",
    "                        'fundamentals_Price/Cash', 'fundamentals_Price/Earnings']\n",
    "growth_metric_cols_scores = ['fundamentals_EPS_growth_1yr', 'fundamentals_EPS_growth_3yr', \n",
    "                        'fundamentals_EPS_growth_5yr']\n",
    "\n",
    "# Make sure all necessary columns exist and are numeric\n",
    "for col in equity_fundamental_cols_scores:\n",
    "    if col not in temp_df_for_scores.columns:\n",
    "        print(f\"Warning: Column {col} not found for score calculation display. Filling with 0.\")\n",
    "        temp_df_for_scores[col] = 0 \n",
    "    temp_df_for_scores[col] = pd.to_numeric(temp_df_for_scores[col], errors='coerce').fillna(0)\n",
    "\n",
    "\n",
    "scaler_display = MinMaxScaler()\n",
    "temp_df_for_scores[value_metric_cols_scores + growth_metric_cols_scores] = scaler_display.fit_transform(\n",
    "    temp_df_for_scores[value_metric_cols_scores + growth_metric_cols_scores]\n",
    ")\n",
    "temp_df_for_scores['Value_Score_Display'] = temp_df_for_scores[value_metric_cols_scores].apply(\n",
    "    lambda row: np.mean([1 - val for val in row]), axis=1\n",
    ")\n",
    "temp_df_for_scores['Growth_Score_Display'] = temp_df_for_scores[growth_metric_cols_scores].mean(axis=1)\n",
    "\n",
    "if not original_missing_indices.empty:\n",
    "    print(\"\\nImputed ETF with calculated scores (for display):\")\n",
    "    print(temp_df_for_scores.loc[original_missing_indices[0], style_cols + ['profile_cap_usd', 'Value_Score_Display', 'Growth_Score_Display']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contracts_df#.to_csv('data/fundamentals.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXTRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce to numerical columns\n",
    "basic_classification = [col for col in original_columns if col not in columns_to_explode]\n",
    "bond_fundamentals = ['fundamentals_AverageQuality', 'fundamentals_NominalMaturity', 'fundamentals_EffectiveMaturity', 'fundamentals_AverageCoupon', 'fundamentals_YieldtoMaturity']\n",
    "profile_classification = ['profile_Domicile', 'profile_MarketGeoFocus', 'profile_BenchmarkIndex', 'profile_FundCategory', 'profile_TotalExpenseRatio', 'profile_TotalNetAssets', 'profile_cap', 'profile_cap_currency', 'profile_MarketCapFocus']\n",
    "\n",
    "classification_columns = basic_classification + bond_fundamentals + profile_classification\n",
    "data_cols = contracts_df.columns[~contracts_df.columns.isin(classification_columns)]\n",
    "\n",
    "data = contracts_df[~contracts_df['fundamentals_Price/Book'].isna()].copy()\n",
    "data = data[~data['fundamentals_LTDebt/Shareholders'].isna()]\n",
    "data = data[~data['style_large-core'].isna()]\n",
    "\n",
    "fundamental_columns = [full_column for full_column in contracts_df.columns if full_column.startswith('fundamentals') and full_column not in bond_fundamentals]\n",
    "for col in fundamental_columns + ['profile_cap_usd']:\n",
    "    print(col)\n",
    "    data[col] = (data[col] - data[col].mean()) / data[col].std()\n",
    "\n",
    "data[data_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph correlations\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# drop columns with missing values\n",
    "corr_df = data[data_cols].corr()\n",
    "corr_df.dropna(axis=1, how='all', inplace=True)\n",
    "corr_df.dropna(axis=0, how='all', inplace=True)\n",
    "data_cols = corr_df.columns\n",
    "\n",
    "plt.figure(figsize=(50, 50))\n",
    "sns.heatmap(corr_df, cmap='coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Factor definition\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Academic factors\n",
    "Market beta\n",
    "SMB\n",
    "HML\n",
    "RMW\n",
    "\n",
    "\n",
    "# Variety factors\n",
    "approaches 1 => little variety\n",
    "approaches 0 => a lot of variety\n",
    "sum of country**2\n",
    "sum of currency**2\n",
    "sum of industry**2\n",
    "sum of top10**2\n",
    "\n",
    "Check if factors are orthogonal\n",
    "'''\n",
    "\n",
    "'''\n",
    "# Variety factors\n",
    "approaches 1 => little variety\n",
    "approaches 0 => a lot of variety\n",
    "sum of country**2\n",
    "sum of currency**2\n",
    "sum of industry**2\n",
    "sum of top10**\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in style columns\n",
    "fundamental_columns = [full_column for full_column in contracts_df.columns if full_column.startswith('fundamentals') and full_column not in bond_fundamentals]\n",
    "\n",
    "value_columns = ['fundamentals_Price/Book',  'fundamentals_Price/Cash', 'fundamentals_Price/Earnings', 'fundamentals_Price/Sales']#, 'fundamentals_LTDebt/Shareholders', 'fundamentals_TotalDebt/TotalCapital', 'fundamentals_TotalDebt/TotalEquity']#,  'fundamentals_TotalAssets/TotalEquity']\n",
    "growth_columns = ['fundamentals_EPSGrowth-1yr', 'fundamentals_EPS_growth_3yr', 'fundamentals_EPS_growth_5yr']#, 'fundamentals_ReturnonAssets', 'fundamentals_SalestoTotalAssets']\n",
    "output_columns = [full_column for full_column in contracts_df.columns if full_column.startswith('style')]\n",
    "\n",
    "'''\n",
    "growth score = 2 * [ N_P/B + N_P/E + N_P/Cash + N_P/Sales + N_EPS_growth_1yr + N_EPS_growth_3yr + N_EPS_growth_5yr + \n",
    "              N_ReturnonAssets1Yr + N_ReturnonAssets3Yr + N_ReturnonCapital + N_ReturnonCapital3Yr + \n",
    "              N_ReturnonEquity1Yr + N_ReturnonEquity3Yr + N_ReturnonInvestment1Yr + N_ReturnonInvestment3Yr + \n",
    "              N_SalestoTotalAssets + N_EBITtoInterest + N_RelativeStrength + \n",
    "              (1 - N_LTDebt/ShareholdersEquity) + (1 - N_TotalAssets/TotalEquity) + \n",
    "              (1 - N_TotalDebt/TotalCapital) + (1 - N_TotalDebt/TotalEquity) ] / 22 - 1\n",
    "\n",
    "Extreme Growth: If all growth indicators ≈ 1 and value indicators ≈ 0, then S = [18*1 + 4*1]/22 = 1, score = 2*1 - 1 = 1.\n",
    "Extreme Value: If all growth indicators ≈ 0 and value indicators ≈ 1, then S = [18*0 + 4*0]/22 = 0, score = 2*0 - 1 = -1.\n",
    "Neutral: If all ≈ 0.5, then S = [18*0.5 + 4*0.5]/22 = 0.5, score = 2*0.5 - 1 = 0.\n",
    "\n",
    "\n",
    "Step 4: Proposed Refined Model\n",
    "Balancing your suggestions with practicality and Morningstar’s framework, I recommend:\n",
    "Select Key Metrics: Use only the most relevant IBKR metrics.\n",
    "Equal Weighting Within Categories: Follow Morningstar’s approach for simplicity and grounding.\n",
    "Score Calculation: Compute a value-growth spectrum from -1 to 1.\n",
    "Refined Model\n",
    "Value Score = mean((1 - N_P/B) + (1 - N_P/Sales) + (1 - N_P/Cash) + (1 - N_P/E)) # Possibly add: LTDebt/ShareholdersEquity, TotalDebt/Equity\n",
    "Growth Score = mean(N_EPS_growth_1yr + N_EPS_growth_3yr + N_EPS_growth_5yr) # Possibly add: ReturnonAssets, SalestoTotalAssets\n",
    "\n",
    "Why This Works\n",
    "Relevance: Uses metrics tied to Morningstar’s historical measures and value investing principles.\n",
    "Simplicity: Equal weighting avoids overcomplication while mirroring industry practice.\n",
    "No Additional Standardization: Normalization suffices for comparability.\n",
    "Flexibility: Captures the spectrum effectively with available data.\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Clustering analysis\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distance correlation\n",
    "import dcor\n",
    "\n",
    "training_array = data[data_cols].values # Convert training matrix to numpy array\n",
    "symbol_list = data[data_cols].columns.tolist()\n",
    "num_symbols = len(symbol_list)\n",
    "corr_matrix = np.zeros((num_symbols, num_symbols)) # Pre-allocate numpy array for correlation\n",
    "cov_matrix = np.zeros((num_symbols, num_symbols))  # Pre-allocate numpy array for covariance\n",
    "\n",
    "for i, sym_i in tqdm(enumerate(symbol_list), total=num_symbols, desc=f\"Calculating distance stats sqr\"):\n",
    "    for j, sym_j in enumerate(symbol_list):\n",
    "        if i <= j:  # Compute only for upper triangle (including diagonal)\n",
    "            stats = dcor.distance_stats(training_array[:, i], training_array[:, j])\n",
    "            corr_value = stats.correlation_xy\n",
    "            cov_value = stats.covariance_xy\n",
    "\n",
    "            corr_matrix[i, j] = corr_value\n",
    "            corr_matrix[j, i] = corr_value  # Fill symmetric value\n",
    "\n",
    "            cov_matrix[i, j] = cov_value\n",
    "            cov_matrix[j, i] = cov_value  # Fill symmetric value\n",
    "\n",
    "corr_df = pd.DataFrame(corr_matrix, index=symbol_list, columns=symbol_list) # Convert numpy array back to df for output\n",
    "cov_df = pd.DataFrame(cov_matrix, index=symbol_list, columns=symbol_list)   # Convert numpy array back to df for output\n",
    "\n",
    "\n",
    "# drop columns with missing values\n",
    "corr_df = data[data_cols].corr()#.values\n",
    "corr_df = corr_df.dropna(axis=1, how='all')\n",
    "corr_df.dropna(axis=0, how='all', inplace=True)\n",
    "corr_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create distance matrix\n",
    "symbol_list = corr_df.columns\n",
    "\n",
    "symbol2index = dict(zip(corr_df.columns, corr_df.index))\n",
    "index2symbol = dict(zip(corr_df.index, corr_df.columns))\n",
    "corr_df.rename(columns=symbol2index, inplace=True)\n",
    "# cov_df.rename(columns=symbol2index, inplace=True)\n",
    "\n",
    "distance_matrix = (1 - corr_df).to_numpy()\n",
    "np.fill_diagonal(distance_matrix, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thresholds / cluster_num graphs\n",
    "methods = ['single', 'ward', 'average', 'complete', 'weighted', 'centroid', 'median']\n",
    "methods = ['ward']\n",
    "for method in methods:\n",
    "    linked = sch.linkage(squareform(distance_matrix), method=method)\n",
    "    \n",
    "    num_clusters = range(len(corr_df), 1, -1)\n",
    "    thresholds = linked[:, 2]\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(num_clusters, thresholds, marker='o')\n",
    "    plt.title(f\"Threshold/Num ({method})\")\n",
    "    plt.xlabel('Number of Clusters')\n",
    "    plt.ylabel('Threshold (Distance)')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "# Silhouettes and dendrograms\n",
    "def product(row):\n",
    "    product = 1\n",
    "    for value in row.values():\n",
    "        product *= value\n",
    "    return product\n",
    "\n",
    "methods = ['single', 'ward', 'average', 'complete', 'weighted', 'centroid', 'median']\n",
    "methods = ['ward']\n",
    "for method in methods:\n",
    "\n",
    "    ks = []\n",
    "    scores = []\n",
    "    counts = []\n",
    "    for k in range(2, min(len(distance_matrix), 9)):\n",
    "        clusters = AgglomerativeClustering(n_clusters=k, linkage=method).fit_predict(distance_matrix)\n",
    "        score = silhouette_score(distance_matrix, clusters, metric='precomputed')\n",
    "        ks.append(k)\n",
    "        scores.append(score)\n",
    "        unique_clusters, label_counts = np.unique(clusters, return_counts=True)\n",
    "        label_counts_dict = dict(zip(unique_clusters, label_counts))\n",
    "        counts.append(label_counts_dict)\n",
    "\n",
    "    silhouettes = pd.DataFrame({\n",
    "        'k': ks,\n",
    "        'score': scores,\n",
    "        'counts': counts\n",
    "    })\n",
    "    silhouettes['combitions'] = silhouettes['counts'].apply(product)\n",
    "    silhouettes = silhouettes.sort_values(by='score', ascending=False)\n",
    "    best_k = silhouettes.k.iloc[0]\n",
    "    display(silhouettes)\n",
    "\n",
    "    linked = sch.linkage(squareform(distance_matrix), method=method)\n",
    "    plt.figure(figsize=(40, 15))\n",
    "    sch.dendrogram(linked, labels=corr_df.index, leaf_rotation=90)\n",
    "    plt.title(f\"Method {method}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "# Silhouettes and dendrograms\n",
    "def product(row):\n",
    "    product = 1\n",
    "    for value in row.values():\n",
    "        product *= value\n",
    "    return product\n",
    "\n",
    "methods = ['single', 'ward', 'average', 'complete', 'weighted', 'centroid', 'median']\n",
    "methods = ['ward', 'complete']\n",
    "for method in methods:\n",
    "\n",
    "    ks = []\n",
    "    scores = []\n",
    "    counts = []\n",
    "    for k in range(2, min(len(distance_matrix), 9)):\n",
    "        clusters = AgglomerativeClustering(n_clusters=k, linkage=method).fit_predict(distance_matrix)\n",
    "        score = silhouette_score(distance_matrix, clusters, metric='precomputed')\n",
    "        ks.append(k)\n",
    "        scores.append(score)\n",
    "        unique_clusters, label_counts = np.unique(clusters, return_counts=True)\n",
    "        label_counts_dict = dict(zip(unique_clusters, label_counts))\n",
    "        counts.append(label_counts_dict)\n",
    "\n",
    "    silhouettes = pd.DataFrame({\n",
    "        'k': ks,\n",
    "        'score': scores,\n",
    "        'counts': counts\n",
    "    })\n",
    "    silhouettes['combitions'] = silhouettes['counts'].apply(product)\n",
    "    silhouettes = silhouettes.sort_values(by='score', ascending=False)\n",
    "    best_k = silhouettes.k.iloc[0]\n",
    "    display(silhouettes)\n",
    "\n",
    "    linked = sch.linkage(squareform(distance_matrix), method=method)\n",
    "    plt.figure(figsize=(40, 15))\n",
    "    sch.dendrogram(linked, labels=corr_df.index, leaf_rotation=90)\n",
    "    plt.title(f\"Method {method}\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
