{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ib_async import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "import math\n",
    "import re\n",
    "\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kind = 'midpoint'\n",
    "# kind = 'trades'\n",
    "# kind = 'indices'\n",
    "\n",
    "if kind == 'midpoint':\n",
    "    root = 'data/daily-midpoint/'\n",
    "elif kind == 'trades':\n",
    "    root = 'data/daily-trades/'\n",
    "elif kind == 'indices':\n",
    "    root = 'data/indices/'\n",
    "\n",
    "data_path = root + 'series/'\n",
    "verified_path = root + 'verified_files.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to ibkr\n",
    "util.startLoop()\n",
    "\n",
    "ib = IB()\n",
    "ib.connect('127.0.0.1', 7497, clientId=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_historical(symbol, exchange, currency, duration='40 Y', kind=None):\n",
    "    contract = Stock(symbol, exchange, currency)\n",
    "    if kind == 'midpoint':\n",
    "        data = ib.reqHistoricalData(\n",
    "            contract, \n",
    "            endDateTime='',\n",
    "            durationStr=duration,\n",
    "            barSizeSetting='1 day', \n",
    "            whatToShow='MIDPOINT', \n",
    "            useRTH=True,\n",
    "        )\n",
    "    elif kind == 'trades' or kind == 'indices':\n",
    "        data = ib.reqHistoricalData(\n",
    "            contract, \n",
    "            endDateTime='',\n",
    "            durationStr=duration,\n",
    "            barSizeSetting='1 day', \n",
    "            whatToShow='TRADES', \n",
    "            useRTH=True,\n",
    "        )\n",
    "    length = len(data) - 1 if data and exchange == 'SMART' else len(data)\n",
    "    return data, length, exchange\n",
    "\n",
    "def save_data(data_path, data, symbol, exchange, currency):\n",
    "    if data:\n",
    "        data_df = util.df(data)\n",
    "        data_df['date'] = pd.to_datetime(data_df['date'])\n",
    "        data_df = data_df.sort_values(by='date').reset_index(drop=True)\n",
    "        data_df.to_csv(f'{data_path}{symbol}-{exchange}-{currency}.csv', index=False)\n",
    "        # print(f'{symbol} saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get missing historical series\n",
    "if kind == 'indices':\n",
    "    raise Exception('Incorrect kind for this. Needs to be trades or midpoint)')\n",
    "\n",
    "years = ['40 Y', '20 Y', '10 Y', '5 Y', '3 Y', '2 Y', '1 Y']\n",
    "# years = ['1 Y']\n",
    "for duration in years:\n",
    "    contracts_df = pd.read_csv('data/contract_elaborated.csv')\n",
    "    \n",
    "    contracts_df['search_exchange'] = contracts_df['search_exchange'].str.extract(r'\\((.*?)\\)').fillna('')\n",
    "    file_list = os.listdir(data_path)\n",
    "    file_list = [name.split('-')[0] for name in file_list]\n",
    "\n",
    "    missing_symbols = contracts_df[~contracts_df['symbol'].isin(file_list)].copy()\n",
    "    count = 0\n",
    "    for _, row in tqdm(missing_symbols.iterrows(), total=len(missing_symbols), desc=f\"Getting {duration} series\"):\n",
    "        symbol = row['symbol']\n",
    "        search_exchange = row['search_exchange']\n",
    "        suggested_exchange = row['exchange']\n",
    "        primary_exchange = row['primaryExchange']\n",
    "        currency = row['currency']\n",
    "        \n",
    "        results = []\n",
    "        if search_exchange:\n",
    "            results.append(get_historical(symbol, search_exchange, currency, duration=duration, kind=kind))\n",
    "            if suggested_exchange != search_exchange:\n",
    "                results.append(get_historical(symbol, suggested_exchange, currency, duration=duration, kind=kind))\n",
    "            if primary_exchange != suggested_exchange and primary_exchange != search_exchange:\n",
    "                results.append(get_historical(symbol, primary_exchange, currency, duration=duration, kind=kind))\n",
    "        else:\n",
    "            results.append(get_historical(symbol, suggested_exchange, currency, duration=duration, kind=kind))\n",
    "            if primary_exchange != suggested_exchange:\n",
    "                results.append(get_historical(symbol, primary_exchange, currency, duration=duration, kind=kind))\n",
    "        results.append(get_historical(symbol, 'SMART', currency, duration=duration, kind=kind))\n",
    "        results = sorted(results, key=lambda x: x[1], reverse=True)\n",
    "        if results[0][1]:\n",
    "            save_data(data_path, results[0][0], symbol, results[0][2], currency)\n",
    "            count +=1\n",
    "\n",
    "    print(f'{duration}: {count} scraped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check scraping differences\n",
    "# def get_csv_lengths(directory):\n",
    "#     \"\"\"Returns a dictionary mapping (symbol, exchange, currency) to their row counts.\"\"\"\n",
    "#     csv_lengths = {}\n",
    "#     for file in os.listdir(directory):\n",
    "#         if file.endswith(\".csv\"):\n",
    "#             file_path = os.path.join(directory, file)\n",
    "#             try:\n",
    "#                 df = pd.read_csv(file_path)\n",
    "#                 file_key = os.path.splitext(file)[0]  # Remove .csv extension\n",
    "#                 parts = file_key.split('-')\n",
    "#                 if len(parts) == 3:\n",
    "#                     symbol, exchange, currency = parts\n",
    "#                     csv_lengths[(symbol, exchange, currency)] = len(df)\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error reading {file_path}: {e}\")\n",
    "#                 csv_lengths[(symbol, exchange, currency)] = None\n",
    "#     return csv_lengths\n",
    "\n",
    "# def main(dir1, dir2, dir3):\n",
    "#     \"\"\"Generates a DataFrame with symbol, exchange, currency, and row counts from three directories.\"\"\"\n",
    "#     lengths1 = get_csv_lengths(dir1)\n",
    "#     lengths2 = get_csv_lengths(dir2)\n",
    "#     lengths3 = get_csv_lengths(dir3)\n",
    "    \n",
    "#     # Collect all unique (symbol, exchange, currency) combinations\n",
    "#     all_keys = set(lengths1.keys()) | set(lengths2.keys()) | set(lengths3.keys())\n",
    "    \n",
    "#     data = []\n",
    "#     for key in sorted(all_keys):\n",
    "#         symbol, exchange, currency = key\n",
    "#         data.append([symbol, exchange, currency, lengths1.get(key, 'N/A'), lengths2.get(key, 'N/A'), lengths3.get(key, 'N/A')])\n",
    "    \n",
    "#     return pd.DataFrame(data, columns=[\"symbol\", \"exchange\", \"currency\", dir1, dir2, dir3])\n",
    "\n",
    "# # Example usage\n",
    "# dir1 = \"data/indices/series/\"\n",
    "# dir2 = \"data/daily-midpoint/series/\"\n",
    "# dir3 = \"data/daily-trades/series/\"\n",
    "# df = main(dir1, dir2, dir3)\n",
    "# # df[df.duplicated(subset='symbol', keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get indices\n",
    "# '''\n",
    "# SPY - SnP 500 -- US centric\n",
    "# VTI - Vanguard Total Stock Market -- US centric\n",
    "# VEU - Vanguard All-World Ex-US -- Global\n",
    "# VXUS - Vanguard Total International -- Global\n",
    "# BND - Vanguard Total Bond -- US\n",
    "# BNDX - Vanguard Total International Bond -- Global\n",
    "# '''\n",
    "# indices = [Stock('SPY', 'SMART', 'USD'), Stock('VTI', 'SMART', 'USD'), Stock('VEU', 'SMART', 'USD'), Stock('VXUS', 'SMART', 'USD'), Stock('BND', 'SMART', 'USD'), Stock('BNDX', 'SMART', 'USD')]\n",
    "# index_path = 'data/indices/series/'\n",
    "# for contract in tqdm(indices, total=len(indices), desc=f\"Getting index series\"):\n",
    "#     data,_,_ = get_historical(data, contract.symbol, contract.exchange, contract.currency, kind=kind)\n",
    "#     save_data(index_path, data, contract.symbol, contract.exchange, contract.currency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update historical series\n",
    "file_list = os.listdir(data_path)\n",
    "\n",
    "for file_name in tqdm(file_list, total=len(file_list), desc=f\"Updating {data_path}\"):\n",
    "    symbol, exchange, currency = file_name.replace('.csv', '').split('-')\n",
    "    \n",
    "    file_path = os.path.join(data_path, file_name)\n",
    "    data_df = pd.read_csv(file_path)\n",
    "    data_df['date'] = pd.to_datetime(data_df['date'])\n",
    "    last_date = data_df['date'].max()\n",
    "    time_missing = (datetime.now() - last_date).days\n",
    "    if time_missing > 364:\n",
    "        time_missing = math.ceil(time_missing / 364)\n",
    "        duration = f'{time_missing} Y'\n",
    "    else:\n",
    "        duration = f'{time_missing} D'\n",
    "    \n",
    "    if time_missing:\n",
    "        new_data,_,_ = get_historical(symbol, exchange, currency, duration=duration, kind=kind)\n",
    "        if new_data:\n",
    "            new_data_df = util.df(new_data)\n",
    "            new_data_df['date'] = pd.to_datetime(new_data_df['date'])\n",
    "            updated_data_df = pd.concat([new_data_df, data_df]).drop_duplicates(subset='date').sort_values(by='date').reset_index(drop=True)\n",
    "            updated_data_df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare indices\n",
    "def melt(data_df):\n",
    "    value_columns = ['open', 'close']\n",
    "    id_columns = [col for col in data_df.columns.to_list() if col not in value_columns]\n",
    "    melted_df = data_df.melt(id_vars=id_columns, value_vars=value_columns, var_name='kind', value_name='value')\n",
    "    return melted_df.sort_values(by=['date', 'kind'], ascending=[True, False]).reset_index(drop=True)\n",
    "\n",
    "# Load indices and merge them all into one df\n",
    "indices = {}\n",
    "file_list = os.listdir('data/indices/series/')\n",
    "for file in file_list:\n",
    "    symbol = os.path.splitext(file)[0].split('-')[0]\n",
    "    indices[symbol] = pd.read_csv('data/indices/series/' + file)\n",
    "\n",
    "# Melt indices, filters, and calc pct_change. ASSUMES that indices are sorted chronologically\n",
    "training_start_date = pd.to_datetime('2020-02-01')\n",
    "month_ago = datetime.today() - timedelta(days=31)\n",
    "\n",
    "day_gap = 6 # SET ACCEPTABLE DAY GAP\n",
    "\n",
    "melted_indices, index_returns = [], {}\n",
    "for symbol, df in tqdm(indices.items(), total=len(indices), desc=f'Melting and filtering {kind} indices'):\n",
    "    df = melt(df)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "    latest_date = df['date'].iloc[-1]\n",
    "    earliest_date = df['date'].iloc[0]\n",
    "    dates = df['date'].unique()\n",
    "    date_gaps = dates[1:] - dates[:-1]\n",
    "    df['symbol'] = symbol\n",
    "    df['pct_change'] = df['value'].pct_change()\n",
    "    index_returns[symbol] = df['pct_change'].mean()\n",
    "    melted_indices.append(df)\n",
    "print(f'Loaded {len(melted_indices)} out of {len(file_list)} series ({round(len(melted_indices)/len(file_list)*100, 4)}%)')\n",
    "\n",
    "# Concatenate and pivot data\n",
    "index_df = pd.concat(melted_indices, ignore_index=True)\n",
    "index_df = index_df.pivot(index=['date', 'kind'], columns='symbol', values='pct_change')\n",
    "index_df = index_df.sort_values(by=['date', 'kind'], ascending=[True, False]).reset_index()#.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define verified files\n",
    "try:\n",
    "    with open(verified_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        verified_files = [line.strip() for line in lines]\n",
    "    file_list = os.listdir(data_path)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    util.startLoop()\n",
    "    ib = IB()\n",
    "    ib.connect('127.0.0.1', 7497, clientId=1)\n",
    "\n",
    "    file_list = os.listdir(data_path)\n",
    "    contracts_df = pd.read_csv('data/contract_elaborated.csv')\n",
    "\n",
    "    verified_files = []\n",
    "    for file_name in tqdm(file_list, total=len(file_list)):\n",
    "        symbol, exchange, currency = file_name.replace('.csv', '').split('-')\n",
    "        try:\n",
    "            contract_details = ib.reqContractDetails(Stock(symbol, exchange, currency))\n",
    "            if not contract_details:\n",
    "                continue\n",
    "            id = contract_details[0].secIdList[0].value\n",
    "\n",
    "            if contracts_df[contracts_df['symbol'] == symbol]['isin'].iloc[0] == id:\n",
    "                instrument_name = contracts_df[contracts_df['symbol'] == symbol]['longName'].iloc[0]\n",
    "                instrument_name = instrument_name.replace('-', '').replace('+', '')\n",
    "                for word in instrument_name.split():\n",
    "                    if re.fullmatch(r'\\d+X', word):\n",
    "                        if int(word[:-1]) > 1:\n",
    "                            continue\n",
    "                        if word.startswith(('LV', 'LEV')):\n",
    "                            print(f'    {instrument_name}')\n",
    "                            \n",
    "                verified_files.append(file_name.split('-')[0])\n",
    "        except Exception as e:\n",
    "            # if e.args and len(e.args) > 0 and e.args[0] != 'open orders request timed out':\n",
    "            print(e)\n",
    "\n",
    "    with open(verified_path, 'w') as f:\n",
    "        for item in verified_files:\n",
    "            f.write(str(item) + '\\n')\n",
    "\n",
    "    ib.disconnect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
