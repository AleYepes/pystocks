{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import ast\n",
    "import pycountry\n",
    "import requests\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "from functions import *\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load files\n",
    "def load_files(root='data/raw/'):\n",
    "    dir_list = os.listdir(root)\n",
    "    dir_list = [file for file in dir_list if file.startswith('contract') and file.endswith('.csv')]\n",
    "    if dir_list:\n",
    "        fundamental_dfs = []\n",
    "        for file in dir_list:\n",
    "            print(file)\n",
    "            df = load(root + file)\n",
    "            df = df[df.apply(is_row_valid, axis=1)]\n",
    "            fundamental_dfs.append(df)\n",
    "        contracts_df = pd.concat(fundamental_dfs)\n",
    "        contracts_df = clean_df(contracts_df)\n",
    "        contracts_df = contracts_df[contracts_df.apply(is_row_valid, axis=1)]\n",
    "\n",
    "    contracts_df['date_scraped'] = pd.to_datetime(contracts_df['date_scraped'])\n",
    "    return contracts_df, root, dir_list\n",
    "\n",
    "contracts_df, root, dir_list = load_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode columns\n",
    "contracts_df['asset_bond'] = contracts_df[['debtors']].notna().any(axis=1).astype(bool)\n",
    "contracts_df[['asset_equity', 'asset_cash', 'asset_other']] = False\n",
    "\n",
    "empty_subcategories = {\n",
    "'holding_types': ['Other'],\n",
    "'countries': ['Unidentified'], \n",
    "'currencies': ['<NoCurrency>'],\n",
    "'industries': ['NonClassifiedEquity', 'NotClassified-NonEquity'],\n",
    "'top10': ['OtherAssets', 'AccountsPayable','AccountsReceivable','AccountsReceivable&Pay','AdministrationFees','CustodyFees','ManagementFees','OtherAssetsandLiabilities','OtherAssetslessLiabilities', 'OtherFees','OtherLiabilities','Tax','Tax--ManagementFees'],\n",
    "'debtors': ['OTHER'],\n",
    "'maturity': ['%MaturityOther'],\n",
    "'debt_type': ['%QualityNotAvailable', '%QualityNotRated'],\n",
    "}\n",
    "\n",
    "original_columns = contracts_df.columns\n",
    "columns_to_explode = ['holding_types', 'profile', 'countries', 'fundamentals', 'industries', 'currencies', 'style', 'debtors', 'maturity', 'debt_type', 'lipper', 'top10']\n",
    "\n",
    "columns_to_ignore = []#'lipper', 'debtors', 'debt_type', 'maturity']# 'top10', 'currencies', ] # little data or leads to high multicollinearity\n",
    "columns_to_explode = [col for col in columns_to_explode if col not in columns_to_ignore]\n",
    "\n",
    "non_percentage_columns = ['profile', 'style', 'lipper', 'fundamentals']\n",
    "percentage_columns = [col for col in columns_to_explode if col not in non_percentage_columns]\n",
    "for col in tqdm(columns_to_explode, total=len(columns_to_explode)):\n",
    "    contracts_df[col] = contracts_df[col].fillna('[]')\n",
    "    contracts_df[col] = contracts_df[col].apply(evaluate_literal)\n",
    "\n",
    "    # Explode and create pivot_df\n",
    "    contracts_df = contracts_df.explode(col)\n",
    "    contracts_df[col] = contracts_df[col].apply(lambda x: (None, None) if pd.isna(x) else x)\n",
    "    contracts_df[['label', 'value']] = pd.DataFrame(contracts_df[col].tolist(), index=contracts_df.index)\n",
    "\n",
    "    pivot_df = contracts_df.pivot_table(index=contracts_df.index, columns='label', values='value', aggfunc='first')\n",
    "    pivot_df.rename(columns={label: f'{col}_{label}' for label in pivot_df.columns}, inplace=True)\n",
    "\n",
    "    # Drop unnecessary columns and align pivot_df with contracts_df\n",
    "    contracts_df = contracts_df.drop(columns=[col, 'label', 'value'], axis=1).drop_duplicates(subset=['conId', 'funds_date'])\n",
    "    pivot_df = pivot_df.reindex(contracts_df.index)\n",
    "    \n",
    "    # Correct pivot_df values\n",
    "    if col in percentage_columns:\n",
    "        pivot_df = pivot_df.fillna(0.0).clip(lower=0)\n",
    "        columns_to_drop = [f'{col}_{label}' for label in empty_subcategories[col]]\n",
    "\n",
    "        # Scale value sum to be <= 1\n",
    "        pivot_cols_sum = pivot_df.sum(axis=1)\n",
    "        rows_greater_than = pivot_cols_sum > 1\n",
    "        pivot_df.loc[rows_greater_than] = pivot_df.loc[rows_greater_than].div(pivot_cols_sum[rows_greater_than], axis=0)\n",
    "\n",
    "        # Guarantee value sum == 1\n",
    "        pivot_cols_sum = pivot_df.sum(axis=1)\n",
    "        rows_less_than = pivot_cols_sum < 1\n",
    "        missing_value = (1 - pivot_cols_sum[rows_less_than])\n",
    "\n",
    "        empty_column = columns_to_drop[0]\n",
    "        if empty_column not in pivot_df.columns:\n",
    "                pivot_df[empty_column] = 0.0\n",
    "\n",
    "        pivot_df.loc[rows_less_than, empty_column] = pivot_df.loc[rows_less_than, empty_column] + missing_value\n",
    "\n",
    "        # # Create variety columns\n",
    "        # pivot_df[f'{col}_variety'] = pivot_df.pow(2).sum(axis=1)\n",
    "\n",
    "        # # Drop top10 company columns\n",
    "        # if col == 'top10':\n",
    "        #     columns_to_drop = [column for column in pivot_df.columns if column != f'{col}_variety']\n",
    "        #     pivot_df = pivot_df.drop(columns=columns_to_drop, axis=1)\n",
    "\n",
    "    contracts_df = pd.concat([contracts_df, pivot_df], axis=1)\n",
    "\n",
    "contracts_df = contracts_df[~contracts_df['profile_TotalNetAssets'].isna()]\n",
    "contracts_df = contracts_df.drop(columns=columns_to_ignore, errors='ignore')\n",
    "\n",
    "columns_to_move = ['asset_bond', 'asset_equity', 'asset_cash', 'asset_other', 'tradable']\n",
    "remaining_columns = [col for col in contracts_df.columns if col not in columns_to_move]\n",
    "\n",
    "new_column_order = remaining_columns + columns_to_move\n",
    "contracts_df = contracts_df[new_column_order]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### MANUAL post explosion check\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_columns = [col for col in contracts_df.columns if col not in original_columns]\n",
    "\n",
    "# # Remove bad string scrapes\n",
    "# contracts_df['temp_length'] = contracts_df['search_exchange'].apply(lambda x: len(x) if isinstance(x, str) else 0)\n",
    "# bad_rows = contracts_df[contracts_df['temp_length'] == 1]['conId'].tolist()\n",
    "# contracts_df[contracts_df['conId'].isin(bad_rows)]\n",
    "\n",
    "# Remove identical data\n",
    "bad_rows = contracts_df[contracts_df.duplicated(subset=remaining_columns, keep=False)]['conId'].tolist()\n",
    "contracts_df[contracts_df.duplicated(subset=remaining_columns, keep=False)]\n",
    "    \n",
    "if bad_rows and input(f\"Sure you want to drop {len(bad_rows)} rows? (y/n)\").lower().strip() == 'y':\n",
    "    raw_df, root, dir_list = load_files()\n",
    "    raw_df = raw_df[~raw_df['conId'].isin(bad_rows)]\n",
    "    for period, month_df in raw_df.groupby(raw_df['date_scraped'].dt.to_period('M')):\n",
    "        output_filename = f\"data/filtered/contract_scraped_{period.year - 2000}-{period.month:02d}.csv\"\n",
    "        month_df.to_csv(output_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contracts_df[contracts_df.duplicated(subset=remaining_columns, keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop ETF duplicates\n",
    "_, eur_exchanges = sort_by_eur_exchanges(contracts_df)\n",
    "\n",
    "remaining_columns = [col for col in contracts_df.columns if col not in original_columns and not col.startswith('top10_')]\n",
    "\n",
    "contracts_df = (\n",
    "    contracts_df\n",
    "    .assign(currency_is_euro=contracts_df['currency'] == 'EUR')\n",
    "    .assign(exchange_is_european=contracts_df['exchange'].isin(eur_exchanges))\n",
    "    .assign(primary_is_european=contracts_df['primaryExchange'].isin(eur_exchanges))\n",
    "    .sort_values(by=['currency_is_euro','exchange_is_european', 'primary_is_european', 'tradable'], ascending=[False, False, False, False])\n",
    "    .drop_duplicates(subset=remaining_columns + ['funds_date'], keep='first')\n",
    "    .drop(columns=['currency_is_euro', 'exchange_is_european', 'primary_is_european'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct profile total net assets and TER\n",
    "symbol_mapping = {\n",
    "    '$': 'USD',    # Default to USD\n",
    "    '￥': 'JPY',    # Japanese Yen\n",
    "    'Rs': 'INR',\n",
    "    'CNH': 'CNY',\n",
    "    '€': 'EUR',    # Euro\n",
    "    '¥': 'JPY',    # Alternative Yen symbol\n",
    "    '£': 'GBP',    # British Pound\n",
    "    'A$': 'AUD',   # Australian Dollar\n",
    "    'C$': 'CAD',   # Canadian Dollar\n",
    "    'HK$': 'HKD',  # Hong Kong Dollar\n",
    "}\n",
    "\n",
    "domicile2currency_map = {\n",
    "    'Luxembourg': 'EUR',\n",
    "    'Ireland': 'EUR',\n",
    "    'USA': 'USD',\n",
    "    'Canada': 'CAD',\n",
    "    'SaudiArabia': 'SAR',\n",
    "    'France': 'EUR',\n",
    "    'Netherlands': 'EUR',\n",
    "    'Jersey': 'GBP', # Jersey uses its own pound, but is pegged to GBP\n",
    "    'Mexico': 'MXN',\n",
    "    'Switzerland': 'CHF',\n",
    "    'HongKong': 'HKD',\n",
    "    'Germany': 'EUR',\n",
    "    'Guernsey': 'GBP', # Guernsey uses its own pound, but is pegged to GBP\n",
    "    'Spain': 'EUR',\n",
    "    'Sweden': 'SEK',\n",
    "    'Israel': 'ILS',\n",
    "    'Hungary': 'HUF',\n",
    "    'Singapore': 'SGD',\n",
    "    'Australia': 'AUD',\n",
    "}\n",
    "\n",
    "def standardize_currency(currency):\n",
    "    if pd.isna(currency):\n",
    "        return np.nan\n",
    "    if currency in symbol_mapping:\n",
    "        return symbol_mapping[currency]\n",
    "    if currency == '':\n",
    "        return ''\n",
    "    try:\n",
    "        if pycountry.currencies.get(alpha_3=currency):\n",
    "            return currency\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    return currency\n",
    "\n",
    "def clean_total_net_assets(value):\n",
    "    if pd.isna(value):\n",
    "        return np.nan, np.nan\n",
    "    value = re.sub(r'\\basof\\b.*', '', value, flags=re.IGNORECASE).strip()\n",
    "    match = re.match(r'([^0-9\\s]+)?\\s*([0-9.,]+)\\s*([kKmMbB]?)', value)\n",
    "    if not match:\n",
    "        return np.nan, np.nan\n",
    "    currency, num_str, unit = match.groups()\n",
    "    currency = currency if currency else ''\n",
    "    num = float(num_str.replace(',', ''))\n",
    "    unit = unit.lower() if unit else ''\n",
    "    if unit == 'k':\n",
    "        num *= 10**3\n",
    "    elif unit == 'm':\n",
    "        num *= 10**6\n",
    "    elif unit == 'b':\n",
    "        num *= 10**9\n",
    "    elif unit == 't':\n",
    "        num *= 10**12\n",
    "    return num, currency\n",
    "\n",
    "def extract_profile_net_assets(value):\n",
    "    if pd.isna(value):\n",
    "        return np.nan, np.nan, np.nan\n",
    "    \n",
    "    # match = re.match(r'([^0-9\\s]+)?\\s*([0-9.,]+)\\s*([kKmMbB]?)?\\s*(?:asof\\s*)?(\\d{4}[-/]\\d{2}[-/]\\d{2})?', value, flags=re.IGNORECASE)\n",
    "    match = re.match(r'([^0-9\\s]+)?\\s*([0-9.,]+)\\s*([kKmMbB]?)?\\s*(?:[aA][sS][o0][fF]\\s*)?(\\d{4}[-/]\\d{2}[-/]\\d{2})?', value, flags=re.IGNORECASE)\n",
    "    if not match:\n",
    "        return np.nan, np.nan, np.nan\n",
    "    \n",
    "    currency, num_str, unit, date_str = match.groups()\n",
    "    currency = currency if currency else ''\n",
    "    num = float(num_str.replace(',', ''))\n",
    "    unit = unit.lower() if unit else ''\n",
    "    \n",
    "    if unit == 'k':\n",
    "        num *= 10**3\n",
    "    elif unit == 'm':\n",
    "        num *= 10**6\n",
    "    elif unit == 'b':\n",
    "        num *= 10**9\n",
    "    elif unit == 't':\n",
    "        num *= 10**12\n",
    "            \n",
    "    parsed_date = np.nan\n",
    "    if date_str:\n",
    "        try:\n",
    "            parsed_date = datetime.strptime(date_str, '%Y/%m/%d')\n",
    "        except ValueError:\n",
    "            try:\n",
    "                parsed_date = datetime.strptime(date_str, '%Y-%m-%d')\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "    return num, currency, parsed_date\n",
    "\n",
    "def get_exchange_rates(currencies, to_currency='USD'):\n",
    "    rates = {}\n",
    "    valid_currencies = []\n",
    "    for c in currencies:\n",
    "        if pd.notna(c) and pycountry.currencies.get(alpha_3=c) is not None:\n",
    "            valid_currencies.append(c)\n",
    "    if not valid_currencies:\n",
    "        return rates\n",
    "    try:\n",
    "        url = f\"https://open.er-api.com/v6/latest/{to_currency}\"\n",
    "        response = requests.get(url)\n",
    "        data = response.json()\n",
    "        if 'rates' in data:\n",
    "            for currency in valid_currencies:\n",
    "                if currency == 'USD':\n",
    "                    rates[currency] = 1.0\n",
    "                elif currency in data['rates']:\n",
    "                    rates[currency] = 1 / data['rates'][currency] if data['rates'][currency] != 0 else np.nan\n",
    "            # print(f\"Fetched rates: {rates}\")\n",
    "            return rates\n",
    "        else:\n",
    "            print(f\"Error fetching rates: {data.get('error', 'Unknown error')}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Exchange rate fetch failed: {e}\")\n",
    "    return rates\n",
    "\n",
    "def convert_to_usd(row, rates):\n",
    "    if pd.isna(row['profile_cap']) or pd.isna(row['profile_cap_currency']):\n",
    "        return np.nan\n",
    "    currency = row['profile_cap_currency']\n",
    "    if currency in rates:\n",
    "        return row['profile_cap'] * rates[currency]\n",
    "    print(f\"No rate available for {currency}\")\n",
    "    return np.nan\n",
    "\n",
    "# Extract data from profile_TotalNetAssets\n",
    "contracts_df[['profile_cap', 'profile_cap_currency', 'profile_funds_date']] = contracts_df['profile_TotalNetAssets'].apply(lambda x: pd.Series(extract_profile_net_assets(x)))\n",
    "\n",
    "# Standardize currency\n",
    "contracts_df['profile_cap_currency'] = contracts_df['profile_cap_currency'].apply(standardize_currency)\n",
    "contracts_df['profile_cap_currency'] = np.where(\n",
    "    contracts_df['profile_cap_currency'] == '',\n",
    "    contracts_df['profile_Domicile'].map(domicile2currency_map).fillna(contracts_df['currency']),\n",
    "    contracts_df['profile_cap_currency']\n",
    ")\n",
    "contracts_df['profile_cap_currency'] = contracts_df['profile_cap_currency'].apply(lambda x: x if (pd.isna(x) or pycountry.currencies.get(alpha_3=x) or x == '') else np.nan)\n",
    "\n",
    "# Get exchange rates and convert to USD\n",
    "exchange_rates = get_exchange_rates(contracts_df['profile_cap_currency'].unique())\n",
    "contracts_df['profile_cap_usd'] = contracts_df.apply(lambda row: convert_to_usd(row, exchange_rates),axis=1)\n",
    "# contracts_df = contracts_df.drop(columns=['profile_TotalNetAssets', 'profile_cap', 'profile_cap_currency'], axis=1, errors='ignore')\n",
    "\n",
    "# Clean TER\n",
    "contracts_df['profile_TotalExpenseRatio'] = contracts_df['profile_TotalExpenseRatio'].replace('', np.nan).astype(float)\n",
    "\n",
    "# Check match for funds_date\n",
    "contracts_df['funds_date'] = pd.to_datetime(contracts_df['funds_date'], errors='coerce')\n",
    "contracts_df['date_match'] = (contracts_df['profile_funds_date'] == contracts_df['funds_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contracts_df#[['conId', 'symbol', 'exchange', 'longName', 'profile_TotalNetAssets', 'profile_funds_date', 'funds_date', 'date_scraped', 'date_match', 'fundamentals_ReturnonInvestment1Yr', 'fundamentals_NominalMaturity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy columns\n",
    "# Domicile dummies\n",
    "if 'profile_Domicile' in contracts_df.columns:\n",
    "    dummies = pd.get_dummies(contracts_df['profile_Domicile'], prefix='domicile').astype(int)\n",
    "    contracts_df = pd.concat([contracts_df, dummies], axis=1)\n",
    "    contracts_df.drop('profile_Domicile', axis=1, inplace=True)\n",
    "\n",
    "# Market cap dummies\n",
    "size_map = {\n",
    "        'Small-cap': 'marketcap_small',\n",
    "        'Mid-cap': 'marketcap_mid',\n",
    "        'BroadMarket': 'marketcap_multi',\n",
    "        'Large-cap': 'marketcap_large',\n",
    "    }\n",
    "contracts_df['profile_MarketCapFocus'] = contracts_df['profile_MarketCapFocus'].map(size_map)\n",
    "\n",
    "# Get size columns from MarketCapFocus and style cols\n",
    "dummies = pd.get_dummies(contracts_df['profile_MarketCapFocus'], dtype=int)\n",
    "contracts_df = pd.concat([contracts_df, dummies], axis=1)\n",
    "\n",
    "style_groups = {\n",
    "    'marketcap_small': ['style_small-core', 'style_small-growth', 'style_small-value'],\n",
    "    'marketcap_mid': ['style_mid-core', 'style_mid-growth', 'style_mid-value'],\n",
    "    'marketcap_large': ['style_large-core', 'style_large-growth', 'style_large-value'],\n",
    "    'marketcap_multi': ['style_multi-core', 'style_multi-growth', 'style_multi-value']\n",
    "}\n",
    "\n",
    "# Update each size column by OR-ing with the style columns\n",
    "for size, cols in style_groups.items():\n",
    "    contracts_df[size] = contracts_df[size] | contracts_df[cols].any(axis=1).astype(int)\n",
    "\n",
    "del dummies, size_map, style_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Search exchange verification\n",
    "# contracts_df['search_exchange'] = contracts_df['search_exchange'].str.extract(r'\\(([^()]*)\\)', expand=False)\n",
    "# contracts_df['validExchanges'] = contracts_df['validExchanges'].apply(lambda x: x.split(','))\n",
    "# contracts_df['validExchanges'] = contracts_df.apply(lambda x: x['validExchanges'] + [x['primaryExchange']], axis=1)\n",
    "\n",
    "# def validate_search_exchange(row):\n",
    "#     # if pd.isna(row['search_exchange']):\n",
    "#     #     return np.nan\n",
    "#     return True if row['search_exchange'] in row['validExchanges'] else False\n",
    "\n",
    "# contracts_df['valid_search_exchange'] = contracts_df.apply(validate_search_exchange, axis=1)\n",
    "contracts_df = contracts_df.drop(columns=['search_symbol', 'validExchanges', 'search_exchange'], axis=1, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refine original asset class classifications with fundamentals and holding_type columns\n",
    "bond_fundamental_cols = ['fundamentals_AverageCoupon', 'fundamentals_AverageQuality', 'fundamentals_YieldtoMaturity', 'fundamentals_NominalMaturity', 'fundamentals_EffectiveMaturity']\n",
    "equity_fundamental_cols = [col for col in contracts_df.columns if col.startswith('fundamentals_') if col not in bond_fundamental_cols]\n",
    "bond_mask = contracts_df[bond_fundamental_cols].notna().any(axis=1)\n",
    "equity_mask = contracts_df[equity_fundamental_cols].notna().any(axis=1)\n",
    "\n",
    "contracts_df['asset_bond'] = contracts_df['asset_bond'] | bond_mask\n",
    "contracts_df['asset_other'] = ~(equity_mask | contracts_df['asset_bond'])\n",
    "contracts_df['asset_other'] = contracts_df['asset_other'] | (equity_mask & contracts_df['asset_bond'])\n",
    "contracts_df['asset_equity'] = ~(contracts_df['asset_bond'] | contracts_df['asset_other'])\n",
    "\n",
    "contracts_df = contracts_df.rename(columns={'holding_types_FixedIncome': 'holding_types_bond',\n",
    "                                            'holding_types_Equity': 'holding_types_equity',\n",
    "                                            'holding_types_Cash': 'holding_types_cash',\n",
    "                                            'holding_types_Other': 'holding_types_other',\n",
    "                                            })\n",
    "\n",
    "def refine_classification(row):\n",
    "    holding_types = ['holding_types_bond', 'holding_types_equity', 'holding_types_cash', 'holding_types_other']\n",
    "    max_col = row[holding_types].idxmax()\n",
    "    if row[max_col] > 0.5:\n",
    "        type_name = max_col.replace('holding_types_', '')\n",
    "        type_name = f'asset_{type_name}'\n",
    "        result = pd.Series([False, False, False, False], index=['asset_bond', 'asset_equity', 'asset_cash', 'asset_other'])\n",
    "        result[type_name] = True\n",
    "        return result\n",
    "    else:\n",
    "        return row[['asset_bond', 'asset_equity', 'asset_cash', 'asset_other']]\n",
    "\n",
    "contracts_df[['asset_bond', 'asset_equity', 'asset_cash', 'asset_other']] = contracts_df.apply(refine_classification, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final cleaning before imputation\n",
    "rating_map = {\n",
    "    'AAA': 6,\n",
    "    'AA': 5,\n",
    "    'A': 4,\n",
    "    'BBB': 3,\n",
    "    'BB': 2,\n",
    "    'B': 1,\n",
    "}\n",
    "contracts_df['fundamentals_AverageQuality'] = contracts_df['fundamentals_AverageQuality'].replace(rating_map)\n",
    "\n",
    "# Convert bools to intbools\n",
    "bool_map = {\n",
    "    True: 1,\n",
    "    False: 0,\n",
    "    # np.nan: 0,\n",
    "}\n",
    "bool_cols = [col for col in contracts_df.columns if contracts_df[col].dtype == 'bool' or col.startswith('style_')]\n",
    "for col in list(set(bool_cols)):\n",
    "    contracts_df[col] = contracts_df[col].replace(bool_map).fillna(0) ## DONT FILLNA if you want to impute \n",
    "\n",
    "if 'stockType' in contracts_df.columns:\n",
    "    contracts_df.loc[contracts_df['stockType'] == 'ETC', 'industries_BasicMaterials'] = 1.0\n",
    "\n",
    "# Remove unnecessary qual or empty columns, only keep key identifiers\n",
    "qual_cols = ['stockType', 'date_scraped', 'exchange_bug', 'exact_search', 'search_symbol', 'profile_MarketCapFocus', 'profile_MarketGeoFocus', 'profile_BenchmarkIndex', 'profile_FundCategory', 'dividends_PayoutRatio']\n",
    "contracts_df = contracts_df.drop(columns=qual_cols, axis=1, errors='ignore')\n",
    "\n",
    "# Clean remaining numerical columns\n",
    "identifier_cols = ['symbol', 'primaryExchange', 'searchExchange', 'conId', 'longName', 'isin', 'exchange', 'currency', 'profile_MarketCapFocus', 'funds_date']\n",
    "for col in [c for c in contracts_df.columns if c not in identifier_cols]:\n",
    "    temp_type = contracts_df[col].dtype\n",
    "    if temp_type == 'object':\n",
    "        contracts_df[col] = contracts_df[col].apply(lambda x: np.nan if isinstance(x, str) else x)\n",
    "\n",
    "# Assign corresponding fundamentals\n",
    "contracts_df.loc[contracts_df['holding_types_bond'] == 0, bond_fundamental_cols] = 0\n",
    "contracts_df.loc[contracts_df['holding_types_equity'] == 0, equity_fundamental_cols] = 0\n",
    "\n",
    "contracts_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check similarity between all top 10 columns\n",
    "from fuzzywuzzy import fuzz\n",
    "from itertools import combinations\n",
    "\n",
    "columns = [col.split('top10_')[-1] for col in contracts_df.columns if col.startswith('top10_') and col not in ['top10_variety', 'top10_']]\n",
    "\n",
    "SIMILARITY_THRESHOLD = 80\n",
    "similar_pairs = []\n",
    "n = len(columns)\n",
    "\n",
    "for col1, col2 in tqdm(combinations(columns, 2), total = n * (n - 1) // 2):\n",
    "    similarity = fuzz.token_set_ratio(col1, col2)\n",
    "    if similarity >= SIMILARITY_THRESHOLD:\n",
    "        similar_pairs.append((col1, col2, similarity))\n",
    "\n",
    "# Merge similar top10\n",
    "import networkx as nx\n",
    "\n",
    "if n:\n",
    "    # Find connected components \n",
    "    G = nx.Graph()\n",
    "    for col1_suffix, col2_suffix, _ in similar_pairs:\n",
    "        G.add_edge(col1_suffix, col2_suffix)\n",
    "\n",
    "    for suffix in columns:\n",
    "        G.add_node(suffix)\n",
    "\n",
    "    connected_components = list(nx.connected_components(G))\n",
    "\n",
    "    # 3. Prepare for DataFrame modification\n",
    "    new_column_data_map = {}\n",
    "    original_cols_involved_in_merging = set()\n",
    "    for group_of_suffixes in connected_components:\n",
    "        if len(group_of_suffixes) > 1:\n",
    "            sorted_suffixes_in_group = sorted(list(group_of_suffixes))\n",
    "            \n",
    "            # Determine the new representative column name (using the 'top10_' prefix)\n",
    "            representative_suffix = sorted_suffixes_in_group[0]\n",
    "            merged_full_col_name = f\"top10_{representative_suffix}\"\n",
    "            \n",
    "            # Identify all original full column names in this group\n",
    "            original_full_names_in_this_group = []\n",
    "            for suffix in sorted_suffixes_in_group:\n",
    "                full_name = f\"top10_{suffix}\"\n",
    "                original_full_names_in_this_group.append(full_name)\n",
    "                original_cols_involved_in_merging.add(full_name)\n",
    "                \n",
    "            # Sum the values of the original columns in this group\n",
    "            existing_cols_to_sum = [col for col in original_full_names_in_this_group if col in contracts_df.columns]\n",
    "            \n",
    "            if existing_cols_to_sum:\n",
    "                summed_series = contracts_df[existing_cols_to_sum].sum(axis=1)\n",
    "                new_column_data_map[merged_full_col_name] = summed_series\n",
    "            else:\n",
    "                print(f\"  Warning: No existing columns found for suffixes {group_of_suffixes} to sum.\")\n",
    "                \n",
    "    contracts_df_merged = contracts_df.copy()\n",
    "\n",
    "    cols_to_drop_list = list(original_cols_involved_in_merging)\n",
    "    if cols_to_drop_list:\n",
    "        contracts_df_merged.drop(columns=cols_to_drop_list, inplace=True, errors='ignore')\n",
    "\n",
    "    if new_column_data_map:\n",
    "        for new_col_name, data_series in new_column_data_map.items():\n",
    "            contracts_df_merged[new_col_name] = data_series\n",
    "\n",
    "    contracts_df = contracts_df_merged\n",
    "\n",
    "    del contracts_df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute values by asset class\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "imputed_df = contracts_df.copy()\n",
    "if input('Impute values (y/n)').lower() == 'y': \n",
    "\n",
    "    class_cols = ['asset_equity', 'asset_bond', 'asset_cash', 'asset_other']\n",
    "    cols_to_exclude = ['conId', 'valid_search_exchange'] + class_cols\n",
    "    numerical_cols_global = [\n",
    "        col for col in imputed_df.columns\n",
    "        if imputed_df[col].dtype in [float, np.float64, int, np.int64]\n",
    "        and col not in cols_to_exclude\n",
    "    ]\n",
    "\n",
    "    for asset_class in tqdm(class_cols, total=len(class_cols)):\n",
    "        mask = imputed_df[asset_class] == 1\n",
    "        if not mask.any():\n",
    "            continue\n",
    "\n",
    "        subset_numerical_data = imputed_df.loc[mask, numerical_cols_global]\n",
    "        if subset_numerical_data.empty:\n",
    "            continue\n",
    "\n",
    "        cols_for_imputation_in_subset = [col for col in subset_numerical_data.columns if subset_numerical_data[col].nunique(dropna=True) > 1]\n",
    "\n",
    "        if not cols_for_imputation_in_subset:\n",
    "            continue\n",
    "\n",
    "        data_to_process = subset_numerical_data[cols_for_imputation_in_subset]\n",
    "        pipeline = Pipeline([\n",
    "            ('scaler', MinMaxScaler()),\n",
    "            ('imputer', KNNImputer())\n",
    "        ])\n",
    "\n",
    "        imputed_scaled_subset = pipeline.fit_transform(data_to_process)\n",
    "        \n",
    "        scaler_fitted_on_subset = pipeline.named_steps['scaler']\n",
    "        imputed_original_scale_subset = scaler_fitted_on_subset.inverse_transform(imputed_scaled_subset)\n",
    "\n",
    "        imputed_df.loc[mask, cols_for_imputation_in_subset] = imputed_original_scale_subset\n",
    "\n",
    "    # Restore original data types for numerical columns where possible\n",
    "    for col in numerical_cols_global:\n",
    "        if col in imputed_df.columns and col in contracts_df.columns:\n",
    "            if imputed_df[col].dtype != contracts_df[col].dtype:\n",
    "                try:\n",
    "                    imputed_df[col] = imputed_df[col].astype(contracts_df[col].dtype)\n",
    "                except ValueError as e:\n",
    "                    print(f\"Warning: Could not convert column '{col}' to {contracts_df[col].dtype}. Error: {e}\")\n",
    "\n",
    "    # Drop specified columns\n",
    "    columns_to_drop = [col for col in imputed_df.columns if col.startswith('top10_') and col != 'top10_variety']\n",
    "    imputed_df = imputed_df.drop(columns=columns_to_drop, errors='ignore')\n",
    "    imputed_df = imputed_df.drop(columns=['dividends'], errors='ignore')\n",
    "\n",
    "    # imputed_df = imputed_df.fillna(0.0)\n",
    "else:\n",
    "    emptiness = imputed_df.isna().mean()\n",
    "    mean = emptiness.mean()\n",
    "    std = emptiness.std()\n",
    "\n",
    "    columns_to_drop = emptiness[emptiness > mean + 3*std].index.to_list()\n",
    "    imputed_df = imputed_df.drop(columns=columns_to_drop, errors='ignore').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_df.fundamentals_EBITtoInterest.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_df.isna().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove low variance, low range columns\n",
    "numerical_cols = [col for col in imputed_df.columns if imputed_df[col].dtype in [np.int64, np.float64] and col not in ['conId', 'profile_cap_usd', 'fundamentals_EBITtoInterest', 'fundamentals_ReturnonEquity1Yr'] and not col.startswith('fundamentals') and not col.endswith('variety')]\n",
    "\n",
    "vars = imputed_df[numerical_cols].var()\n",
    "differences = ((imputed_df[numerical_cols]).min() - imputed_df[numerical_cols].max()).abs()\n",
    "differences += vars\n",
    "cols_to_drop = differences[differences < differences.mean() - differences.std()].index\n",
    "\n",
    "final_df = imputed_df.drop(columns=cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check max n_components\n",
    "import umap\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "import numpy as np\n",
    "\n",
    "# Fit UMAP with a small n_components to get the graph\n",
    "reducer_temp = umap.UMAP(\n",
    "    n_components=2,\n",
    "    metric='cosine',\n",
    "    random_state=0,\n",
    "    angular_rp_forest=True\n",
    ")\n",
    "reducer_temp.fit(embeddings)\n",
    "graph = reducer_temp.graph_\n",
    "\n",
    "# Compute connected components\n",
    "n_components_cc, labels = connected_components(graph, directed=False)\n",
    "component_sizes = np.bincount(labels)\n",
    "min_component_size = component_sizes.min() - 2\n",
    "print(f\"Number of components: {n_components_cc}\")\n",
    "print(f\"Component sizes: {component_sizes}\")\n",
    "print(f\"Smallest component size: {min_component_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baysian optimization 4 params\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Integer\n",
    "from sklearn.metrics import adjusted_rand_score, davies_bouldin_score\n",
    "from itertools import combinations\n",
    "import umap\n",
    "import hdbscan\n",
    "import math\n",
    "import gc\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=ConvergenceWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "\n",
    "def progress_callback(res):\n",
    "    pbar.update(1)\n",
    "\n",
    "def calculate_stability(labels_series):\n",
    "    labels_list = labels_series.tolist()\n",
    "    if len(labels_list) < 2:\n",
    "        return 0\n",
    "    ari_scores = [adjusted_rand_score(l1, l2) for l1, l2 in combinations(labels_list, 2)]\n",
    "    return np.mean(ari_scores)\n",
    "\n",
    "def objective(params):\n",
    "    n_components, n_neighbors, min_cluster_size, min_samples = params\n",
    "    params_list = []\n",
    "    n_runs = 3\n",
    "    for run in range(n_runs):\n",
    "        reducer = umap.UMAP(\n",
    "            n_neighbors=n_neighbors,\n",
    "            n_components=n_components,\n",
    "            metric='cosine',\n",
    "            random_state=run,\n",
    "            angular_rp_forest=True\n",
    "        )\n",
    "        reduced = reducer.fit_transform(embeddings)\n",
    "        clusterer = hdbscan.HDBSCAN(\n",
    "            min_cluster_size=min_cluster_size,\n",
    "            min_samples=min_samples,\n",
    "            metric='euclidean',\n",
    "            gen_min_span_tree=True,\n",
    "            core_dist_n_jobs=-1\n",
    "        )\n",
    "        labels = clusterer.fit_predict(reduced)\n",
    "\n",
    "        dbcv = clusterer.relative_validity_\n",
    "        db_index = davies_bouldin_score(reduced, labels) if len(set(labels)) > 1 else np.inf\n",
    "        noise_points = np.sum(labels == -1)\n",
    "        params_list.append({\n",
    "            'labels': labels,\n",
    "            'dbcv': dbcv,\n",
    "            'db_index': db_index,\n",
    "            'noise_points': noise_points\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(params_list)\n",
    "    stability = calculate_stability(df['labels'])\n",
    "    result_dict = {\n",
    "        'n_components': n_components,\n",
    "        'n_neighbors': n_neighbors,\n",
    "        'min_cluster_size': min_cluster_size,\n",
    "        'min_samples': min_samples,\n",
    "        'dbcv': np.mean(df['dbcv']),\n",
    "        'db_index': np.mean(df['db_index']),\n",
    "        'noise_points': np.mean(df['noise_points']),\n",
    "        'stability': stability,\n",
    "    }\n",
    "    evaluated_results.append(result_dict)\n",
    "    \n",
    "    norm_db_index = 1 / (1 + np.mean(df['db_index']))\n",
    "    norm_noise = 1 - (np.mean(df['noise_points']) / len(embeddings))\n",
    "    score = np.mean(df['dbcv']) * norm_db_index * norm_noise * stability\n",
    "    \n",
    "    return -score\n",
    "\n",
    "\n",
    "n_calls = 100 #########\n",
    "\n",
    "if 'pbar' in globals():\n",
    "    pbar.close()\n",
    "pbar = tqdm(total=n_calls)\n",
    "\n",
    "total_data_points = len(embeddings)\n",
    "print(f'total embeddings: {total_data_points}')\n",
    "space = [Integer(3, min_component_size, name='n_components'),\n",
    "         Integer(int(math.ceil(math.log10(total_data_points) ** 2)),\n",
    "                 total_data_points//3,\n",
    "                 name='n_neighbors'),\n",
    "         Integer(int(math.ceil(math.log10(total_data_points) ** 2)),\n",
    "                 total_data_points//3,\n",
    "                 name='min_cluster_size'),\n",
    "]\n",
    "space += [Integer(space[-1].low,\n",
    "                  space[-1].high + round(math.log10(total_data_points) ** 2),\n",
    "                  name='min_samples'),\n",
    "]\n",
    "\n",
    "evaluated_results = []\n",
    "n_initial_points = (n_calls//2 + len(space)*3) // 2\n",
    "# n_initial_points = n_calls//2\n",
    "result = gp_minimize(\n",
    "    objective,\n",
    "    space,\n",
    "    n_calls=n_calls,\n",
    "    random_state=1,\n",
    "    initial_point_generator='lhs',\n",
    "    n_initial_points=n_initial_points,\n",
    "    callback=[progress_callback]\n",
    ")\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "final_df.to_csv('data/fundamentals.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
