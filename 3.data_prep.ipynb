{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import ast\n",
    "import pycountry\n",
    "import requests\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode columns\n",
    "contracts_df = load('data/contract_elaborated.csv')\n",
    "contracts_df = clean_df(contracts_df)\n",
    "contracts_df = contracts_df[contracts_df.apply(is_row_valid, axis=1)]\n",
    "\n",
    "contracts_df['asset_bond'] = contracts_df[['debtors']].notna().any(axis=1).astype(bool)\n",
    "contracts_df[['asset_equity', 'asset_cash', 'asset_other']] = False\n",
    "\n",
    "empty_subcategories = {\n",
    "'holding_types': ['Other'],\n",
    "'countries': ['Unidentified'], \n",
    "'currencies': ['<NoCurrency>'],\n",
    "'industries': ['NonClassifiedEquity', 'NotClassified-NonEquity'],\n",
    "'top10': ['OtherAssets', 'AccountsPayable','AccountsReceivable','AccountsReceivable&Pay','AdministrationFees','CustodyFees','ManagementFees','OtherAssetsandLiabilities','OtherAssetslessLiabilities', 'OtherFees','OtherLiabilities','Tax','Tax--ManagementFees'],\n",
    "'debtors': ['OTHER'],\n",
    "'maturity': ['%MaturityOther'],\n",
    "'debt_type': ['%QualityNotAvailable', '%QualityNotRated'],\n",
    "}\n",
    "\n",
    "original_columns = contracts_df.columns\n",
    "columns_to_explode = ['holding_types', 'profile', 'top10', 'countries', 'fundamentals', 'industries', 'currencies', 'style', 'debtors', 'maturity', 'debt_type', 'lipper', 'dividends']\n",
    "\n",
    "columns_to_ignore = ['lipper', 'dividends', 'currencies', 'debtors', 'debt_type', 'maturity'] # little data or leads to high multicollinearity\n",
    "columns_to_explode = [col for col in columns_to_explode if col not in columns_to_ignore]\n",
    "\n",
    "non_percentage_columns = ['profile', 'style', 'lipper', 'fundamentals', 'dividends']\n",
    "percentage_columns = [col for col in columns_to_explode if col not in non_percentage_columns]\n",
    "for col in tqdm(columns_to_explode, total=len(columns_to_explode)):\n",
    "    contracts_df[col] = contracts_df[col].fillna('[]')\n",
    "    contracts_df[col] = contracts_df[col].apply(evaluate_literal)\n",
    "\n",
    "    # Explode and create pivot_df\n",
    "    contracts_df = contracts_df.explode(col)\n",
    "    contracts_df[col] = contracts_df[col].apply(lambda x: (None, None) if pd.isna(x) else x)\n",
    "    contracts_df[['label', 'value']] = pd.DataFrame(contracts_df[col].tolist(), index=contracts_df.index)\n",
    "\n",
    "    pivot_df = contracts_df.pivot_table(index=contracts_df.index, columns='label', values='value', aggfunc='first')\n",
    "    pivot_df.rename(columns={label: f'{col}_{label}' for label in pivot_df.columns}, inplace=True)\n",
    "\n",
    "    # Drop unnecessary columns and align pivot_df with contracts_df\n",
    "    contracts_df = contracts_df.drop(columns=[col, 'label', 'value'], axis=1).drop_duplicates(subset=['conId', 'funds_date'])\n",
    "    pivot_df = pivot_df.reindex(contracts_df.index)\n",
    "    \n",
    "    # Correct pivot_df values\n",
    "    if col in percentage_columns:\n",
    "        pivot_df = pivot_df.fillna(0.0).clip(lower=0)\n",
    "        columns_to_drop = [f'{col}_{label}' for label in empty_subcategories[col]]\n",
    "\n",
    "        # Scale value sum to be <= 1\n",
    "        pivot_cols_sum = pivot_df.sum(axis=1)\n",
    "        rows_greater_than = pivot_cols_sum > 1\n",
    "        pivot_df.loc[rows_greater_than] = pivot_df.loc[rows_greater_than].div(pivot_cols_sum[rows_greater_than], axis=0)\n",
    "\n",
    "        # Guarantee value sum == 1\n",
    "        pivot_cols_sum = pivot_df.sum(axis=1)\n",
    "        rows_less_than = pivot_cols_sum < 1\n",
    "        missing_value = (1 - pivot_cols_sum[rows_less_than])\n",
    "\n",
    "        empty_column = columns_to_drop[0]\n",
    "        if empty_column not in pivot_df.columns:\n",
    "                pivot_df[empty_column] = 0.0\n",
    "\n",
    "        pivot_df.loc[rows_less_than, empty_column] = pivot_df.loc[rows_less_than, empty_column] + missing_value\n",
    "\n",
    "        # Create variety columns\n",
    "        pivot_df[f'{col}_variety'] = pivot_df.pow(2).sum(axis=1)\n",
    "\n",
    "        # Drop top10 company columns\n",
    "        if col == 'top10':\n",
    "            columns_to_drop = [column for column in pivot_df.columns if column != f'{col}_variety']\n",
    "            pivot_df = pivot_df.drop(columns=columns_to_drop, axis=1)\n",
    "\n",
    "    if col == 'dividends':\n",
    "        pivot_df = pivot_df.fillna(0.0).clip(lower=0)\n",
    "\n",
    "    contracts_df = pd.concat([contracts_df, pivot_df], axis=1)\n",
    "\n",
    "contracts_df = contracts_df[~contracts_df['profile_TotalNetAssets'].isna()]\n",
    "contracts_df = contracts_df.drop(columns=columns_to_ignore, errors='ignore')\n",
    "\n",
    "columns_to_move = ['asset_bond', 'asset_equity', 'asset_cash', 'asset_other', 'tradable']\n",
    "remaining_columns = [col for col in contracts_df.columns if col not in columns_to_move]\n",
    "\n",
    "new_column_order = remaining_columns + columns_to_move\n",
    "contracts_df = contracts_df[new_column_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop ETF duplicates\n",
    "eur_exchanges = contracts_df[contracts_df['currency'] == 'EUR'].primaryExchange.unique()\n",
    "remaining_columns = [col for col in contracts_df.columns if col not in original_columns]\n",
    "og_len = len(contracts_df)\n",
    "\n",
    "contracts_df = (\n",
    "    contracts_df\n",
    "    .assign(currency_is_euro=contracts_df['currency'] == 'EUR')\n",
    "    .assign(exchange_is_european=contracts_df['exchange'].isin(eur_exchanges))\n",
    "    .assign(primary_is_european=contracts_df['primaryExchange'].isin(eur_exchanges))\n",
    "    .sort_values(by=['currency_is_euro','exchange_is_european', 'primary_is_european', 'tradable'], ascending=[False, False, False, False])\n",
    "    .drop_duplicates(subset=remaining_columns, keep='first')\n",
    "    .drop(columns=['currency_is_euro', 'exchange_is_european', 'primary_is_european'])\n",
    ")\n",
    "og_len - len(contracts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct profile total net assets and TER\n",
    "symbol_mapping = {\n",
    "    '$': 'USD',    # Default to USD\n",
    "    '￥': 'JPY',    # Japanese Yen\n",
    "    'Rs': 'INR',\n",
    "    'CNH': 'CNY',\n",
    "    '€': 'EUR',    # Euro\n",
    "    '¥': 'JPY',    # Alternative Yen symbol\n",
    "    '£': 'GBP',    # British Pound\n",
    "    'A$': 'AUD',   # Australian Dollar\n",
    "    'C$': 'CAD',   # Canadian Dollar\n",
    "    'HK$': 'HKD',  # Hong Kong Dollar\n",
    "}\n",
    "\n",
    "def standardize_currency(currency):\n",
    "    if pd.isna(currency):\n",
    "        return np.nan\n",
    "    if currency in symbol_mapping:\n",
    "        return symbol_mapping[currency]\n",
    "    if currency == '':\n",
    "        return ''\n",
    "    try:\n",
    "        if pycountry.currencies.get(alpha_3=currency):\n",
    "            return currency\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    return currency\n",
    "\n",
    "def clean_total_net_assets(value):\n",
    "    if pd.isna(value):\n",
    "        return np.nan, np.nan\n",
    "    value = re.sub(r'\\basof\\b.*', '', value, flags=re.IGNORECASE).strip()\n",
    "    match = re.match(r'([^0-9\\s]+)?\\s*([0-9.,]+)\\s*([kKmMbB]?)', value)\n",
    "    if not match:\n",
    "        return np.nan, np.nan\n",
    "    currency, num_str, unit = match.groups()\n",
    "    currency = currency if currency else ''\n",
    "    num = float(num_str.replace(',', ''))\n",
    "    unit = unit.lower() if unit else ''\n",
    "    if unit == 'k':\n",
    "        num *= 10**3\n",
    "    elif unit == 'm':\n",
    "        num *= 10**6\n",
    "    elif unit == 'b':\n",
    "        num *= 10**9\n",
    "    elif unit == 't':\n",
    "        num *= 10**12\n",
    "    return num, currency\n",
    "\n",
    "def get_exchange_rates(currencies, to_currency='USD'):\n",
    "    rates = {}\n",
    "    valid_currencies = []\n",
    "    for c in currencies:\n",
    "        if pd.notna(c) and pycountry.currencies.get(alpha_3=c) is not None:\n",
    "            valid_currencies.append(c)\n",
    "    if not valid_currencies:\n",
    "        return rates\n",
    "    try:\n",
    "        url = f\"https://open.er-api.com/v6/latest/{to_currency}\"\n",
    "        response = requests.get(url)\n",
    "        data = response.json()\n",
    "        if 'rates' in data:\n",
    "            for currency in valid_currencies:\n",
    "                if currency == 'USD':\n",
    "                    rates[currency] = 1.0\n",
    "                elif currency in data['rates']:\n",
    "                    rates[currency] = 1 / data['rates'][currency] if data['rates'][currency] != 0 else np.nan\n",
    "            # print(f\"Fetched rates: {rates}\")\n",
    "            return rates\n",
    "        else:\n",
    "            print(f\"Error fetching rates: {data.get('error', 'Unknown error')}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Exchange rate fetch failed: {e}\")\n",
    "    return rates\n",
    "\n",
    "def convert_to_usd(row, rates):\n",
    "    if pd.isna(row['profile_cap']) or pd.isna(row['profile_cap_currency']):\n",
    "        return np.nan\n",
    "    currency = row['profile_cap_currency']\n",
    "    if currency in rates:\n",
    "        return row['profile_cap'] * rates[currency]\n",
    "    print(f\"No rate available for {currency}\")\n",
    "    return np.nan\n",
    "\n",
    "contracts_df[['profile_cap', 'profile_cap_currency']] = contracts_df['profile_TotalNetAssets'].apply(lambda x: pd.Series(clean_total_net_assets(x)))\n",
    "contracts_df['profile_cap_currency'] = contracts_df['profile_cap_currency'].apply(standardize_currency)\n",
    "contracts_df['profile_cap_currency'] = np.where(contracts_df['profile_cap_currency'] == '', contracts_df['currency'], contracts_df['profile_cap_currency'])\n",
    "contracts_df['profile_cap_currency'] = contracts_df['profile_cap_currency'].apply(lambda x: x if (pd.isna(x) or pycountry.currencies.get(alpha_3=x) or x == '') else np.nan)\n",
    "\n",
    "exchange_rates = get_exchange_rates(contracts_df['profile_cap_currency'].unique())\n",
    "contracts_df['profile_cap_usd'] = contracts_df.apply(lambda row: convert_to_usd(row, exchange_rates),axis=1)\n",
    "contracts_df = contracts_df.drop(columns=['profile_TotalNetAssets', 'profile_cap', 'profile_cap_currency'], axis=1, errors='ignore')\n",
    "\n",
    "# TER\n",
    "contracts_df['profile_TotalExpenseRatio'] = contracts_df['profile_TotalExpenseRatio'].replace('', np.nan).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy columns\n",
    "# Domicile dummies\n",
    "if 'profile_Domicile' in contracts_df.columns:\n",
    "#     dummies = pd.get_dummies(contracts_df['profile_Domicile'], prefix='domicile').astype(int)\n",
    "#     contracts_df = pd.concat([contracts_df, dummies], axis=1)\n",
    "    contracts_df.drop('profile_Domicile', axis=1, inplace=True)\n",
    "\n",
    "# Market cap dummies\n",
    "size_map = {\n",
    "        'Small-cap': 'marketcap_small',\n",
    "        'Mid-cap': 'marketcap_mid',\n",
    "        'BroadMarket': 'marketcap_multi',\n",
    "        'Large-cap': 'marketcap_large',\n",
    "    }\n",
    "contracts_df['profile_MarketCapFocus'] = contracts_df['profile_MarketCapFocus'].map(size_map)\n",
    "\n",
    "# Get size columns from MarketCapFocus and style cols\n",
    "dummies = pd.get_dummies(contracts_df['profile_MarketCapFocus'], dtype=int)\n",
    "contracts_df = pd.concat([contracts_df, dummies], axis=1)\n",
    "\n",
    "style_groups = {\n",
    "    'marketcap_small': ['style_small-core', 'style_small-growth', 'style_small-value'],\n",
    "    'marketcap_mid': ['style_mid-core', 'style_mid-growth', 'style_mid-value'],\n",
    "    'marketcap_large': ['style_large-core', 'style_large-growth', 'style_large-value'],\n",
    "    'marketcap_multi': ['style_multi-core', 'style_multi-growth', 'style_multi-value']\n",
    "}\n",
    "\n",
    "# Update each size column by OR-ing with the style columns\n",
    "for size, cols in style_groups.items():\n",
    "    contracts_df[size] = contracts_df[size] | contracts_df[cols].any(axis=1).astype(int)\n",
    "\n",
    "del dummies, size_map, style_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Search exchange verification\n",
    "contracts_df['search_exchange'] = contracts_df['search_exchange'].str.extract(r'\\(([^()]*)\\)', expand=False)\n",
    "contracts_df['validExchanges'] = contracts_df['validExchanges'].apply(lambda x: x.split(','))\n",
    "contracts_df['validExchanges'] = contracts_df.apply(lambda x: x['validExchanges'] + [x['primaryExchange']], axis=1)\n",
    "\n",
    "def validate_search_exchange(row):\n",
    "    # if pd.isna(row['search_exchange']):\n",
    "    #     return np.nan\n",
    "    return True if row['search_exchange'] in row['validExchanges'] else False\n",
    "\n",
    "contracts_df['valid_search_exchange'] = contracts_df.apply(validate_search_exchange, axis=1)\n",
    "# contracts_df = contracts_df.drop(columns=['search_symbol', 'validExchanges'], axis=1, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refine original asset class classifications with fundamentals and holding_type columns\n",
    "bond_fundamental_cols = ['fundamentals_AverageCoupon', 'fundamentals_AverageQuality', 'fundamentals_YieldtoMaturity', 'fundamentals_NominalMaturity', 'fundamentals_EffectiveMaturity']\n",
    "equity_fundamental_cols = [col for col in contracts_df.columns if col.startswith('fundamentals_') if col not in bond_fundamental_cols]\n",
    "bond_mask = contracts_df[bond_fundamental_cols].notna().any(axis=1)\n",
    "equity_mask = contracts_df[equity_fundamental_cols].notna().any(axis=1)\n",
    "\n",
    "contracts_df['asset_bond'] = contracts_df['asset_bond'] | bond_mask\n",
    "contracts_df['asset_other'] = ~(equity_mask | contracts_df['asset_bond'])\n",
    "contracts_df['asset_other'] = contracts_df['asset_other'] | (equity_mask & contracts_df['asset_bond'])\n",
    "contracts_df['asset_equity'] = ~(contracts_df['asset_bond'] | contracts_df['asset_other'])\n",
    "\n",
    "contracts_df = contracts_df.rename(columns={'holding_types_FixedIncome': 'holding_types_bond',\n",
    "                                            'holding_types_Equity': 'holding_types_equity',\n",
    "                                            'holding_types_Cash': 'holding_types_cash',\n",
    "                                            'holding_types_Other': 'holding_types_other',\n",
    "                                            })\n",
    "\n",
    "def refine_classification(row):\n",
    "    holding_types = ['holding_types_bond', 'holding_types_equity', 'holding_types_cash', 'holding_types_other']\n",
    "    max_col = row[holding_types].idxmax()\n",
    "    if row[max_col] > 0.5:\n",
    "        type_name = max_col.replace('holding_types_', '')\n",
    "        type_name = f'asset_{type_name}'\n",
    "        result = pd.Series([False, False, False, False], index=['asset_bond', 'asset_equity', 'asset_cash', 'asset_other'])\n",
    "        result[type_name] = True\n",
    "        return result\n",
    "    else:\n",
    "        return row[['asset_bond', 'asset_equity', 'asset_cash', 'asset_other']]\n",
    "\n",
    "contracts_df[['asset_bond', 'asset_equity', 'asset_cash', 'asset_other']] = contracts_df.apply(refine_classification, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final cleaning before imputation\n",
    "rating_map = {\n",
    "    'AAA': 6,\n",
    "    'AA': 5,\n",
    "    'A': 4,\n",
    "    'BBB': 3,\n",
    "    'BB': 2,\n",
    "    'B': 1,\n",
    "}\n",
    "contracts_df['fundamentals_AverageQuality'] = contracts_df['fundamentals_AverageQuality'].replace(rating_map)\n",
    "\n",
    "# Convert bools to intbools\n",
    "bool_map = {\n",
    "    True: 1,\n",
    "    False: 0,\n",
    "    # np.nan: 0,\n",
    "}\n",
    "bool_cols = [col for col in contracts_df.columns if contracts_df[col].dtype == 'bool' or col.startswith('style_')]\n",
    "for col in list(set(bool_cols)):\n",
    "    contracts_df[col] = contracts_df[col].replace(bool_map).fillna(0) ## DONT FILLNA if you want to impute \n",
    "\n",
    "if 'stockType' in contracts_df.columns:\n",
    "    contracts_df.loc[contracts_df['stockType'] == 'ETC', 'industries_BasicMaterials'] = 1.0\n",
    "\n",
    "# Remove unnecessary qual or empty columns, only keep key identifiers\n",
    "qual_cols = ['stockType', 'date_scraped', 'exchange_bug', 'exact_search', 'search_symbol', 'profile_MarketCapFocus', 'profile_MarketGeoFocus', 'profile_BenchmarkIndex', 'profile_FundCategory', 'dividends_PayoutRatio']\n",
    "contracts_df = contracts_df.drop(columns=qual_cols, axis=1, errors='ignore')\n",
    "\n",
    "# Clean remaining numerical columns\n",
    "identifier_cols = ['symbol', 'primaryExchange', 'searchExchange', 'conId', 'longName', 'isin', 'exchange', 'currency', 'profile_MarketCapFocus', 'funds_date']\n",
    "for col in [c for c in contracts_df.columns if c not in identifier_cols]:\n",
    "    temp_type = contracts_df[col].dtype\n",
    "    if temp_type == 'object':\n",
    "        contracts_df[col] = contracts_df[col].apply(lambda x: np.nan if isinstance(x, str) else x)\n",
    "\n",
    "# Assign corresponding fundamentals\n",
    "contracts_df.loc[contracts_df['holding_types_bond'] == 0, bond_fundamental_cols] = 0\n",
    "contracts_df.loc[contracts_df['holding_types_equity'] == 0, equity_fundamental_cols] = 0\n",
    "\n",
    "contracts_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check similarity between all top 10 columns\n",
    "from fuzzywuzzy import fuzz\n",
    "from itertools import combinations\n",
    "\n",
    "columns = [col.split('top10_')[-1] for col in contracts_df.columns if col.startswith('top10_') and col not in ['top10_variety', 'top10_']]\n",
    "\n",
    "similarity_threshold = 80\n",
    "similar_pairs = []\n",
    "# similarities = []\n",
    "n = len(columns)\n",
    "\n",
    "for col1, col2 in tqdm(combinations(columns, 2), total = n * (n - 1) // 2):\n",
    "    similarity = fuzz.token_set_ratio(col1, col2)\n",
    "    # similarities.append(similarity)\n",
    "    if similarity >= similarity_threshold:\n",
    "        similar_pairs.append((col1, col2, similarity))\n",
    "\n",
    "# Merge similar top10\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "\n",
    "if n:\n",
    "    # Find connected components \n",
    "    G = nx.Graph()\n",
    "    for col1_suffix, col2_suffix, _ in similar_pairs:\n",
    "        G.add_edge(col1_suffix, col2_suffix)\n",
    "\n",
    "    for suffix in columns:\n",
    "        G.add_node(suffix)\n",
    "\n",
    "    connected_components = list(nx.connected_components(G))\n",
    "\n",
    "    # 3. Prepare for DataFrame modification\n",
    "    new_column_data_map = {}\n",
    "    original_cols_involved_in_merging = set()\n",
    "    for group_of_suffixes in connected_components:\n",
    "        if len(group_of_suffixes) > 1:\n",
    "            sorted_suffixes_in_group = sorted(list(group_of_suffixes))\n",
    "            \n",
    "            # Determine the new representative column name (using the 'top10_' prefix)\n",
    "            representative_suffix = sorted_suffixes_in_group[0]\n",
    "            merged_full_col_name = f\"top10_{representative_suffix}\"\n",
    "            \n",
    "            # Identify all original full column names in this group\n",
    "            original_full_names_in_this_group = []\n",
    "            for suffix in sorted_suffixes_in_group:\n",
    "                full_name = f\"top10_{suffix}\"\n",
    "                original_full_names_in_this_group.append(full_name)\n",
    "                original_cols_involved_in_merging.add(full_name)\n",
    "                \n",
    "            # Sum the values of the original columns in this group\n",
    "            existing_cols_to_sum = [col for col in original_full_names_in_this_group if col in contracts_df.columns]\n",
    "            \n",
    "            if existing_cols_to_sum:\n",
    "                summed_series = contracts_df[existing_cols_to_sum].sum(axis=1)\n",
    "                new_column_data_map[merged_full_col_name] = summed_series\n",
    "            else:\n",
    "                print(f\"  Warning: No existing columns found for suffixes {group_of_suffixes} to sum.\")\n",
    "                \n",
    "    contracts_df_merged = contracts_df.copy()\n",
    "\n",
    "    cols_to_drop_list = list(original_cols_involved_in_merging)\n",
    "    if cols_to_drop_list:\n",
    "        contracts_df_merged.drop(columns=cols_to_drop_list, inplace=True, errors='ignore')\n",
    "\n",
    "    if new_column_data_map:\n",
    "        for new_col_name, data_series in new_column_data_map.items():\n",
    "            contracts_df_merged[new_col_name] = data_series\n",
    "\n",
    "    contracts_df = contracts_df_merged\n",
    "\n",
    "    del contracts_df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute values by asset class\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "imputed_df = contracts_df.copy()\n",
    "if input('Impute values (y/n)').lower() == 'y': \n",
    "\n",
    "    class_cols = ['asset_equity', 'asset_bond', 'asset_cash', 'asset_other']\n",
    "    cols_to_exclude = ['conId', 'valid_search_exchange'] + class_cols\n",
    "    numerical_cols_global = [\n",
    "        col for col in imputed_df.columns\n",
    "        if imputed_df[col].dtype in [float, np.float64, int, np.int64]\n",
    "        and col not in cols_to_exclude\n",
    "    ]\n",
    "\n",
    "    for asset_class in tqdm(class_cols, total=len(class_cols)):\n",
    "        mask = imputed_df[asset_class] == 1\n",
    "        if not mask.any():\n",
    "            continue\n",
    "\n",
    "        subset_numerical_data = imputed_df.loc[mask, numerical_cols_global]\n",
    "        if subset_numerical_data.empty:\n",
    "            continue\n",
    "\n",
    "        cols_for_imputation_in_subset = [col for col in subset_numerical_data.columns if subset_numerical_data[col].nunique(dropna=True) > 1]\n",
    "\n",
    "        if not cols_for_imputation_in_subset:\n",
    "            continue\n",
    "\n",
    "        data_to_process = subset_numerical_data[cols_for_imputation_in_subset]\n",
    "        pipeline = Pipeline([\n",
    "            ('scaler', MinMaxScaler()),\n",
    "            ('imputer', KNNImputer())\n",
    "        ])\n",
    "\n",
    "        imputed_scaled_subset = pipeline.fit_transform(data_to_process)\n",
    "        \n",
    "        scaler_fitted_on_subset = pipeline.named_steps['scaler']\n",
    "        imputed_original_scale_subset = scaler_fitted_on_subset.inverse_transform(imputed_scaled_subset)\n",
    "\n",
    "        imputed_df.loc[mask, cols_for_imputation_in_subset] = imputed_original_scale_subset\n",
    "\n",
    "    # Restore original data types for numerical columns where possible\n",
    "    for col in numerical_cols_global:\n",
    "        if col in imputed_df.columns and col in contracts_df.columns:\n",
    "            if imputed_df[col].dtype != contracts_df[col].dtype:\n",
    "                try:\n",
    "                    imputed_df[col] = imputed_df[col].astype(contracts_df[col].dtype)\n",
    "                except ValueError as e:\n",
    "                    print(f\"Warning: Could not convert column '{col}' to {contracts_df[col].dtype}. Error: {e}\")\n",
    "\n",
    "    # Drop specified columns\n",
    "    columns_to_drop = [col for col in imputed_df.columns if col.startswith('top10_') and col != 'top10_variety']\n",
    "    imputed_df = imputed_df.drop(columns=columns_to_drop, errors='ignore')\n",
    "    imputed_df = imputed_df.drop(columns=['dividends'], errors='ignore')\n",
    "\n",
    "    imputed_df = imputed_df.fillna(0.0)\n",
    "else:\n",
    "    emptiness = imputed_df.isna().mean().copy()\n",
    "    mean = emptiness.mean()\n",
    "    std = emptiness.std()\n",
    "\n",
    "    columns_to_drop = emptiness[emptiness > mean + 3*std].index.to_list()\n",
    "    imputed_df = imputed_df.drop(columns=columns_to_drop, errors='ignore').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove low variance, low range columns\n",
    "numerical_cols = [col for col in imputed_df.columns if imputed_df[col].dtype in [np.int64, np.float64] and col not in ['conId', 'profile_cap_usd', 'fundamentals_EBITtoInterest', 'fundamentals_ReturnonEquity1Yr'] and not col.startswith('fundamentals') and not col.endswith('variety')]\n",
    "\n",
    "vars = imputed_df[numerical_cols].var()\n",
    "differences = ((imputed_df[numerical_cols]).min() - imputed_df[numerical_cols].max()).abs()\n",
    "differences += vars\n",
    "cols_to_drop = differences[differences < differences.mean() - differences.std()].index\n",
    "\n",
    "final_df = imputed_df.drop(columns=cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "final_df.to_csv('data/fundamentals.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
