{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import ast\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from ib_async import *\n",
    "import pandas_datareader.data as web\n",
    "import wbgapi as wb\n",
    "import country_converter as coco\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import gc\n",
    "import argparse\n",
    "\n",
    "from scipy.stats import skew\n",
    "from scipy.optimize import minimize_scalar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning\n",
    "MAX_STALE_DAYS = 5\n",
    "# Default params for detect_and_nullify_global_outliers\n",
    "Z_THRESHOLD_GLOBAL_DEFAULT = 120.0\n",
    "OUTLIER_WINDOW_DEFAULT = 5\n",
    "# Params for detect_and_nullify_global_outliers in the main loop\n",
    "Z_THRESHOLD_GLOBAL_LOOP = 50\n",
    "\n",
    "# Walk-Forward Analysis\n",
    "WALK_FORWARD_WINDOW_YEARS = range(3, 5)\n",
    "TRAINING_PERIOD_DAYS = 365\n",
    "MOMENTUM_PERIODS_DAYS = {\n",
    "    '1y':  TRAINING_PERIOD_DAYS,\n",
    "    '6mo': TRAINING_PERIOD_DAYS // 2,\n",
    "    '3mo': TRAINING_PERIOD_DAYS // 4,\n",
    "}\n",
    "\n",
    "# Asset Filtering\n",
    "MAX_GAP_LOG = 3.05\n",
    "MAX_PCT_MISSING = 0.3\n",
    "\n",
    "# Factor Construction\n",
    "FACTOR_SCALING_FACTOR = 0.6\n",
    "\n",
    "# Factor Screening\n",
    "CORRELATION_THRESHOLD = 0.95\n",
    "\n",
    "# Elastic Net Hyperparameters\n",
    "ENET_ALPHAS = np.logspace(-11, -4, 30)\n",
    "ENET_L1_RATIOS = [0.01, 0.05, 0.1, 0.3, 0.5, 0.7, 0.9, 0.95, 1]\n",
    "ENET_CV = 5\n",
    "ENET_TOL = 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_world_bank_data(all_country_codes, start_date, end_date, indicators):\n",
    "    valid_country_codes = {code for code in all_country_codes if code is not None}\n",
    "    try:\n",
    "        wb_economies = {e['id'] for e in wb.economy.list()}\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"FATAL: Failed to fetch economy list from World Bank API: {e}\")\n",
    "\n",
    "    final_economies = sorted([code for code in valid_country_codes if code in wb_economies])\n",
    "    unrecognized = valid_country_codes - set(final_economies)\n",
    "    if unrecognized:\n",
    "        print(f\"Info: The following economies were not recognized by the World Bank API and will be skipped: {unrecognized}\")\n",
    "    if not final_economies:\n",
    "        raise Exception(\"Error: No valid economies found to query the World Bank API.\")\n",
    "\n",
    "    all_data = []\n",
    "    chunk_size = 40\n",
    "    for i in range(0, len(final_economies), chunk_size):\n",
    "        chunk = final_economies[i:i + chunk_size]\n",
    "        try:\n",
    "            data_chunk = wb.data.DataFrame(list(indicators), chunk, time=range(start_date.year - 5, end_date.year + 1), labels=False)\n",
    "            all_data.append(data_chunk)\n",
    "        except wb.APIError as e:\n",
    "            print(f\"API Error fetching data for chunk {i//chunk_size + 1}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred fetching data for chunk {i//chunk_size + 1}: {e}\")\n",
    "\n",
    "    if not all_data:\n",
    "        raise Exception(\"Error: Failed to retrieve any data from the World Bank.\")\n",
    "\n",
    "    return pd.concat(all_data)\n",
    "\n",
    "def evaluate_literal(val):\n",
    "    try:\n",
    "        return ast.literal_eval(val)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return val\n",
    "    \n",
    "def load(path):\n",
    "    df = pd.read_csv(path)\n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].apply(evaluate_literal)\n",
    "    return df\n",
    "\n",
    "def verify_files(verified_path, data_path):\n",
    "    try:\n",
    "        return pd.read_csv(verified_path)\n",
    "    except FileNotFoundError:\n",
    "        util.startLoop()\n",
    "        ib = IB()\n",
    "        ib.connect('127.0.0.1', 7497, clientId=2)\n",
    "\n",
    "        file_list = os.listdir(data_path)\n",
    "        verified_files = []\n",
    "\n",
    "        for file_name in tqdm(file_list, total=len(file_list), desc=\"Verifying files\"):\n",
    "            if not file_name.endswith('.csv'):\n",
    "                continue\n",
    "            try:\n",
    "                symbol, exchange, currency = file_name.replace('.csv', '').split('-')\n",
    "                symbol_data = fund_df[(fund_df['symbol'] == symbol) & (fund_df['currency'] == currency)]\n",
    "                if symbol_data.empty:\n",
    "                    continue\n",
    "\n",
    "                contract_details = ib.reqContractDetails(Stock(symbol, exchange, currency))\n",
    "                if not contract_details:\n",
    "                    continue\n",
    "                isin = contract_details[0].secIdList[0].value\n",
    "\n",
    "                if symbol_data['isin'].iloc[0] != isin:\n",
    "                    continue\n",
    "\n",
    "                instrument_name = symbol_data['longName'].iloc[0].replace('-', '').replace('+', '')\n",
    "                leveraged = any(\n",
    "                    re.fullmatch(r'\\d+X', word) and int(word[:-1]) > 1 or word.lower().startswith(('lv', 'lev'))\n",
    "                    for word in instrument_name.split()\n",
    "                )\n",
    "                if leveraged:\n",
    "                    continue\n",
    "\n",
    "                verified_files.append({'symbol': symbol, 'currency': currency})\n",
    "            except ValueError as e:\n",
    "                print(f\"Invalid filename format {file_name}: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_name}: {e}\")\n",
    "\n",
    "        verified_df = pd.DataFrame(verified_files)\n",
    "        verified_df.to_csv(verified_path, index=False)\n",
    "\n",
    "        ib.disconnect()\n",
    "        util.stopLoop()\n",
    "        \n",
    "        return verified_df\n",
    "\n",
    "def ensure_series_types(df, price_col):\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.sort_values('date').reset_index(drop=True)\n",
    "    for col in ['volume', price_col]:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    return df\n",
    "\n",
    "def validate_raw_prices(df, price_col):\n",
    "    invalid_price_mask = df[price_col] <= 0\n",
    "    inconsistent_mask = pd.Series(False, index=df.index)\n",
    "    if 'low' in df.columns and 'high' in df.columns:\n",
    "        inconsistent_mask = (df['low'] > df['high'])\n",
    "    local_error_mask = invalid_price_mask | inconsistent_mask\n",
    "    df = df[~local_error_mask].copy()\n",
    "    return df\n",
    "\n",
    "def handle_stale_periods(df, price_col, max_stale_days=MAX_STALE_DAYS):\n",
    "    stale_groups = (df[price_col].diff() != 0).cumsum()\n",
    "    if stale_groups.empty:\n",
    "        return df\n",
    "    period_lengths = df.groupby(stale_groups)[price_col].transform('size')\n",
    "    long_stale_mask = period_lengths > max_stale_days\n",
    "    is_intermediate_stale_row = (stale_groups.duplicated(keep='first') & stale_groups.duplicated(keep='last'))\n",
    "    rows_to_drop_mask = long_stale_mask & is_intermediate_stale_row\n",
    "    df = df[~rows_to_drop_mask].copy()\n",
    "    return df\n",
    "\n",
    "def detect_and_nullify_global_outliers(meta_df, price_col, z_threshold=Z_THRESHOLD_GLOBAL_DEFAULT, window=OUTLIER_WINDOW_DEFAULT):\n",
    "    all_pct_changes = pd.concat(\n",
    "        [row['df']['pct_change'] for _, row in meta_df.iterrows()],\n",
    "        ignore_index=True\n",
    "    ).dropna()\n",
    "    all_pct_changes = all_pct_changes[~np.isinf(all_pct_changes) & (all_pct_changes != 0)]\n",
    "\n",
    "    global_median_return = all_pct_changes.median()\n",
    "    global_mad = (all_pct_changes - global_median_return).abs().median()\n",
    "\n",
    "    for idx, row in meta_df.iterrows():\n",
    "        df = row['df']\n",
    "        df = df.reset_index(drop=True)\n",
    "        if df['pct_change'].isnull().all():\n",
    "            continue\n",
    "        cols_to_null = [price_col, 'volume', 'high', 'low', 'pct_change']\n",
    "        cols_to_null = [c for c in cols_to_null if c in df.columns]\n",
    "\n",
    "        absolute_modified_z = (df['pct_change'] - global_median_return).abs() / global_mad\n",
    "        outlier_mask = absolute_modified_z > z_threshold\n",
    "\n",
    "        if outlier_mask.any():\n",
    "\n",
    "            candidate_indices = df.index[outlier_mask]\n",
    "            for df_idx in candidate_indices:\n",
    "                price_to_check_idx = df_idx - 1\n",
    "                price_to_check = df.loc[price_to_check_idx, price_col]\n",
    "                local_window_start = max(0, price_to_check_idx - window)\n",
    "                local_window = df.loc[local_window_start : price_to_check_idx - 1, price_col].dropna()\n",
    "                local_mean = local_window.mean()\n",
    "                local_std = local_window.std()\n",
    "                if local_std != 0: \n",
    "                    price_z_score = abs(price_to_check - local_mean) / local_std\n",
    "                    if price_z_score > z_threshold / 10:\n",
    "                        df.loc[price_to_check_idx, cols_to_null] = np.nan\n",
    "\n",
    "                price_to_check = df.loc[df_idx, price_col]\n",
    "                local_window_end = min(df_idx + window, df.index[outlier_mask].max())\n",
    "                local_window = df.loc[df_idx + 1: local_window_end, price_col].dropna()\n",
    "                local_mean = local_window.mean()\n",
    "                local_std = local_window.std()\n",
    "                if local_std != 0:\n",
    "                    price_z_score = abs(price_to_check - local_mean) / local_std\n",
    "                    if price_z_score > z_threshold / 10:\n",
    "                        df.loc[df_idx, cols_to_null] = np.nan\n",
    "\n",
    "            df['pct_change'] = df[price_col].pct_change(fill_method=None)\n",
    "            meta_df.at[idx, 'df'] = df\n",
    "\n",
    "def calculate_slope(value1, value2, time1, time2):\n",
    "    return (value1 - value2) / (time1 - time2)\n",
    "\n",
    "def get_return_stats(df, training_cutoff, momentum_cutoffs, risk_free_df):\n",
    "    training_df = df[df.index < training_cutoff]\n",
    "    training_rf = risk_free_df[risk_free_df.index < training_cutoff]\n",
    "\n",
    "    excess_returns = training_df['pct_change'] - training_rf['daily_nominal_rate']\n",
    "    sharpe = excess_returns.mean() / excess_returns.std()\n",
    "\n",
    "    momentum_3mo = training_df[training_df.index >= momentum_cutoffs['3mo']]['pct_change'].mean()\n",
    "    momentum_6mo = training_df[training_df.index >= momentum_cutoffs['6mo']]['pct_change'].mean()\n",
    "    momentum_1y  = training_df[training_df.index >= momentum_cutoffs['1y']]['pct_change'].mean()\n",
    "\n",
    "    return pd.Series(\n",
    "        [momentum_3mo, momentum_6mo, momentum_1y, sharpe],\n",
    "        index=['momentum_3mo', 'momentum_6mo', 'momentum_1y', 'stats_sharpe']\n",
    "    )\n",
    "\n",
    "def create_continent_map(standard_names):\n",
    "    continents = cc.convert(names=standard_names, to='continent', not_found=None)\n",
    "    return {name: (cont if cont is not None else 'Other')\n",
    "            for name, cont in zip(standard_names, continents)}\n",
    "\n",
    "def calculate_country_stats(wb_data_full, standard_names, end_year, window_size=3):\n",
    "    countries_in_window = [name for name in standard_names if name in wb_data_full.index.get_level_values('economy')]\n",
    "    if not countries_in_window:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    data = wb_data_full.loc[countries_in_window].dropna(axis=1)\n",
    "    available_years = [int(col.replace('YR', '')) for col in data.columns]\n",
    "\n",
    "    cols_to_keep = [col for col, year in zip(data.columns, available_years) if year <= end_year.year]\n",
    "    data = data[cols_to_keep].copy()\n",
    "    data.dropna(axis=1, inplace=True);\n",
    "\n",
    "    yoy_change = data.diff(axis=1)\n",
    "    first_div = yoy_change.T.rolling(window=window_size).mean().T\n",
    "\n",
    "    yoy_change_first_div = first_div.diff(axis=1)\n",
    "    second_div = yoy_change_first_div.T.rolling(window=window_size).mean().T\n",
    "\n",
    "    latest_year_col = data.columns[-1]\n",
    "    latest_first_div_col = first_div.columns[-1]\n",
    "    latest_second_div_col = second_div.columns[-1]\n",
    "\n",
    "    derivatives = pd.DataFrame(data[latest_year_col])\n",
    "    derivatives.rename(columns={latest_year_col: 'raw_value'}, inplace=True)\n",
    "    derivatives['1st_div'] = first_div[latest_first_div_col] / derivatives['raw_value']\n",
    "    derivatives['2nd_div'] = second_div[latest_second_div_col] / derivatives['raw_value']\n",
    "    \n",
    "    metric_df_reshaped = derivatives.unstack(level='series')\n",
    "    if isinstance(metric_df_reshaped.columns, pd.MultiIndex):\n",
    "         metric_df_final = metric_df_reshaped.swaplevel(0, 1, axis=1)\n",
    "         metric_df_final.sort_index(axis=1, level=0, inplace=True)\n",
    "    else:\n",
    "         metric_df_final = metric_df_reshaped\n",
    "\n",
    "    return metric_df_final\n",
    "\n",
    "def construct_long_short_factor_returns(full_meta_df, returns_df, long_symbols, short_symbols, factor_column=None):\n",
    "    long_df = full_meta_df[full_meta_df['conId'].isin(long_symbols)].set_index('conId')\n",
    "    long_weights = long_df['profile_cap_usd'].reindex(returns_df.columns).fillna(0)\n",
    "    if factor_column:\n",
    "        factor_weights = (full_meta_df[factor_column].max() - long_df[factor_column]) / (full_meta_df[factor_column].max() - full_meta_df[factor_column].min())\n",
    "        factor_weights = factor_weights.reindex(returns_df.columns).fillna(0)\n",
    "        if factor_weights.sum() != 0:\n",
    "            long_weights *= factor_weights\n",
    "\n",
    "    if long_weights.sum() != 0:\n",
    "        long_weights /= long_weights.sum()\n",
    "    long_returns = returns_df.dot(long_weights)\n",
    "    \n",
    "    short_df = full_meta_df[full_meta_df['conId'].isin(short_symbols)].set_index('conId')\n",
    "    short_weights = short_df['profile_cap_usd'].reindex(returns_df.columns).fillna(0)\n",
    "    if factor_column:\n",
    "        factor_weights = (short_df[factor_column] - full_meta_df[factor_column].min()) / (full_meta_df[factor_column].max() - full_meta_df[factor_column].min())\n",
    "        factor_weights = factor_weights.reindex(returns_df.columns).fillna(0)\n",
    "        if factor_weights.sum() != 0:\n",
    "            short_weights *= factor_weights\n",
    "\n",
    "    if short_weights.sum() != 0:\n",
    "        short_weights /= short_weights.sum()\n",
    "    short_returns = returns_df.dot(short_weights)\n",
    "    \n",
    "    factor_returns = long_returns - short_returns\n",
    "    return factor_returns\n",
    "\n",
    "def construct_factors(filtered_df, pct_changes, portfolio_dfs, risk_free_df, scaling_factor=FACTOR_SCALING_FACTOR):\n",
    "    factors = {}\n",
    "    # Market risk premium\n",
    "    factors['factor_market_premium'] = (portfolio_dfs['equity']['pct_change'] - risk_free_df['daily_nominal_rate'])\n",
    "\n",
    "    # SMB_ETF\n",
    "    small_symbols = filtered_df[filtered_df['marketcap_small'] == 1]['conId'].tolist()\n",
    "    large_symbols = filtered_df[filtered_df['marketcap_large'] == 1]['conId'].tolist()\n",
    "\n",
    "    intersection = set(small_symbols) & set(large_symbols)\n",
    "    small_symbols = [s for s in small_symbols if s not in intersection]\n",
    "    large_symbols = [s for s in large_symbols if s not in intersection]\n",
    "    smb_etf = construct_long_short_factor_returns(filtered_df, pct_changes, small_symbols, large_symbols)\n",
    "    factors['factor_smb'] = smb_etf\n",
    "\n",
    "    # # HML_ETF\n",
    "    # value_cols = [col for col in filtered_df.columns if col.startswith('style_') and col.endswith('value')]\n",
    "    # growth_cols = [col for col in filtered_df.columns if col.startswith('style_') and col.endswith('growth')]\n",
    "    # value_symbols = filtered_df[filtered_df[value_cols].ne(0).any(axis=1)]['conId'].tolist()\n",
    "    # growth_symbols = filtered_df[filtered_df[growth_cols].ne(0).any(axis=1)]['conId'].tolist()\n",
    "\n",
    "    # intersection = set(value_symbols) & set(growth_symbols)\n",
    "    # value_symbols = [s for s in value_symbols if s not in intersection]\n",
    "    # growth_symbols = [s for s in growth_symbols if s not in intersection]\n",
    "    # hml_etf = construct_long_short_factor_returns(filtered_df, pct_changes, value_symbols, growth_symbols)\n",
    "    # factors['factor_hml'] = hml_etf\n",
    "\n",
    "    # Metadata\n",
    "    excluded = ['style_', 'marketcap_', 'countries_', 'fundamentals_', 'momentum_']\n",
    "    numerical_cols = [col for col in filtered_df.columns if filtered_df[col].dtype in [np.int64, np.float64] and col not in ['conId']]\n",
    "    for col in numerical_cols:\n",
    "        if not any(col.startswith(prefix) for prefix in excluded):\n",
    "            try:\n",
    "                std = filtered_df[col].std()\n",
    "                mean = filtered_df[col].mean()\n",
    "\n",
    "                upper_boundary = min(filtered_df[col].max(), mean + (scaling_factor * std))\n",
    "                lower_boundary = max(filtered_df[col].min(), mean - (scaling_factor * std))\n",
    "\n",
    "                low_factor_symbols = filtered_df[filtered_df[col] <= lower_boundary]['conId'].tolist()\n",
    "                high_factor_symbols = filtered_df[filtered_df[col] >= upper_boundary]['conId'].tolist()\n",
    "                if col.endswith('variety'):\n",
    "                    var_etf = construct_long_short_factor_returns(filtered_df, pct_changes, low_factor_symbols, high_factor_symbols, factor_column=col)\n",
    "                else:\n",
    "                    var_etf = construct_long_short_factor_returns(filtered_df, pct_changes, high_factor_symbols, low_factor_symbols, factor_column=col)\n",
    "                factors[col] = var_etf\n",
    "\n",
    "            except Exception as e:\n",
    "                print(col)\n",
    "                print(e)\n",
    "                raise\n",
    "\n",
    "    return pd.DataFrame(factors)\n",
    "\n",
    "def prescreen_factors(factors_df, correlation_threshold=CORRELATION_THRESHOLD, drop_map=None):\n",
    "    if factors_df is None or factors_df.empty or factors_df.shape[1] == 0:\n",
    "        raise ValueError(\"factors_df must be a non-empty DataFrame with at least one column.\")\n",
    "    temp_factors_df = factors_df.copy()\n",
    "\n",
    "    corr_matrix = temp_factors_df.corr().abs()\n",
    "    corr_pairs = corr_matrix.where(np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)).stack()\n",
    "    corr_pairs = corr_pairs.sort_values(ascending=False)\n",
    "\n",
    "    if not drop_map:\n",
    "        drop_map = {}\n",
    "    col_order = list(temp_factors_df.columns)\n",
    "    for (col1, col2), corr_val in corr_pairs.items():\n",
    "        if corr_val < correlation_threshold:\n",
    "            break\n",
    "\n",
    "        already_dropped = {c for drops in drop_map.values() for c in drops}\n",
    "        if col1 in already_dropped or col2 in already_dropped:\n",
    "            continue\n",
    "\n",
    "        if col_order.index(col1) < col_order.index(col2):\n",
    "            keeper, to_drop = col1, col2\n",
    "        else:\n",
    "            keeper, to_drop = col2, col1\n",
    "\n",
    "        drop_map.setdefault(keeper, []).append(to_drop)\n",
    "\n",
    "    cols_to_drop = set(col for drops in drop_map.values() for col in drops)\n",
    "    temp_factors_df = temp_factors_df.drop(columns=cols_to_drop)\n",
    "    return temp_factors_df, drop_map\n",
    "\n",
    "def merge_drop_map(drop_map):\n",
    "    cols_to_drop = set(col for drops in drop_map.values() for col in drops)\n",
    "    final_drop_map = {}\n",
    "    for keeper, direct_drops in drop_map.items():\n",
    "        if keeper not in cols_to_drop:\n",
    "            cols_to_check = list(direct_drops) \n",
    "            all_related_drops = set(direct_drops)\n",
    "            while cols_to_check:\n",
    "                col = cols_to_check.pop(0)\n",
    "                if col in drop_map:\n",
    "                    new_drops = [d for d in drop_map[col] if d not in all_related_drops]\n",
    "                    cols_to_check.extend(new_drops)\n",
    "                    all_related_drops.update(new_drops)\n",
    "            \n",
    "            final_drop_map[keeper] = sorted(list(all_related_drops))\n",
    "    \n",
    "    return final_drop_map\n",
    "\n",
    "def run_regressions(distilled_factors):\n",
    "    results = []\n",
    "    for symbol in pct_changes.columns:\n",
    "        etf_excess = pct_changes[symbol] - risk_free_df['daily_nominal_rate']\n",
    "        data = pd.concat([etf_excess.rename('etf_excess'), distilled_factors], axis=1).dropna()\n",
    "\n",
    "        Y = data['etf_excess']\n",
    "        X = sm.add_constant(data.iloc[:, 1:])\n",
    "        model = sm.OLS(Y, X).fit()\n",
    "        result = {\n",
    "            'conId': symbol,\n",
    "            'nobs': model.nobs,\n",
    "            'r_squared': model.rsquared,\n",
    "            'r_squared_adj': model.rsquared_adj,\n",
    "            'f_statistic': model.fvalue,\n",
    "            'f_pvalue': model.f_pvalue,\n",
    "            'aic': model.aic,\n",
    "            'bic': model.bic,\n",
    "            'condition_number': model.condition_number,\n",
    "            'alpha': model.params['const'],\n",
    "            'alpha_pval': model.pvalues['const'],\n",
    "            'alpha_tval': model.tvalues['const'],\n",
    "            'alpha_bse': model.bse['const'],\n",
    "        }\n",
    "        for factor in distilled_factors.columns:\n",
    "            result[f'beta_{factor}'] = model.params[factor]\n",
    "            result[f'pval_beta_{factor}'] = model.pvalues[factor]\n",
    "            result[f'tval_beta_{factor}'] = model.tvalues[factor]\n",
    "            result[f'bse_beta_{factor}'] = model.bse[factor]\n",
    "        results.append(result)\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df\n",
    "\n",
    "def calculate_vif(df):\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"feature\"] = df.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]\n",
    "    return vif_data.sort_values(by='VIF', ascending=False)\n",
    "\n",
    "def run_elastic_net(\n",
    "                    factors_df,\n",
    "                    pct_changes,\n",
    "                    risk_free_df,\n",
    "                    training_cutoff,\n",
    "                    alphas=ENET_ALPHAS,\n",
    "                    l1_ratio=ENET_L1_RATIOS,\n",
    "                    cv=ENET_CV,\n",
    "                    tol=ENET_TOL,\n",
    "                    random_state=42):\n",
    "\n",
    "    data = data = (\n",
    "        factors_df.copy()\n",
    "        .join(pct_changes, how='inner')\n",
    "        .join(risk_free_df[['daily_nominal_rate']], how='inner')\n",
    "        .fillna(0)\n",
    "    )\n",
    "\n",
    "    train = data[data.index < training_cutoff]\n",
    "    test = data[data.index >= training_cutoff]\n",
    "\n",
    "    X_train = train[factors_df.columns].values\n",
    "    X_test = test[factors_df.columns].values\n",
    "    \n",
    "    metrics = []\n",
    "    for etf in tqdm(pct_changes.columns, total=len(pct_changes.columns), desc=\"Elastic Net Regression\"):\n",
    "        Y_train = train[etf].values - train['daily_nominal_rate'].values\n",
    "        Y_test = test[etf].values - test['daily_nominal_rate'].values\n",
    "\n",
    "        pipeline = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('enet', ElasticNetCV(alphas=alphas,\n",
    "                                l1_ratio=l1_ratio,\n",
    "                                cv=cv,\n",
    "                                random_state=random_state,\n",
    "                                max_iter=499999,\n",
    "                                tol=tol,\n",
    "                                fit_intercept=True,\n",
    "                                n_jobs=-1)),\n",
    "        ])\n",
    "\n",
    "        try:\n",
    "            pipeline.fit(X_train, Y_train)\n",
    "        except ValueError as e:\n",
    "            print(f\"Skipping {etf} due to error: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Unscale coefficients and intercept\n",
    "        enet = pipeline.named_steps['enet']\n",
    "        scaler = pipeline.named_steps['scaler']\n",
    "        betas_train = enet.coef_ / scaler.scale_\n",
    "        intercept = enet.intercept_ - np.dot(betas_train, scaler.mean_)\n",
    "\n",
    "        # out-of-sample stats\n",
    "        er_test = pipeline.predict(X_test)\n",
    "\n",
    "        # in-sample stats\n",
    "        er_train = pipeline.predict(X_train)\n",
    "\n",
    "        row = {\n",
    "            'conId': etf,\n",
    "            'jensens_alpha': intercept,\n",
    "            'enet_alpha': enet.alpha_,\n",
    "            'l1_ratio': enet.l1_ratio_,\n",
    "            'n_iter': enet.n_iter_,\n",
    "            'dual_gap': enet.dual_gap_,\n",
    "            'n_nonzero': np.sum(np.abs(betas_train) > 1e-6),\n",
    "            'cv_mse_best': np.min(enet.mse_path_.mean(axis=2)),\n",
    "            'cv_mse_average': np.mean(enet.mse_path_.mean(axis=2)),\n",
    "            'cv_mse_worst': np.max(enet.mse_path_.mean(axis=2)),\n",
    "            'mse_test' : mean_squared_error(Y_test, er_test),\n",
    "            'mse_train' : mean_squared_error(Y_train, er_train),\n",
    "            'r2_test' : r2_score(Y_test, er_test),\n",
    "            'r2_train' : r2_score(Y_train, er_train),\n",
    "        }\n",
    "\n",
    "        # Map back coefficients to factor names\n",
    "        for coef, fname in zip(betas_train, factors_df.columns):\n",
    "            row[f'{fname}_beta'] = coef\n",
    "\n",
    "        metrics.append(row)\n",
    "    \n",
    "    results_df = pd.DataFrame(metrics).set_index('conId')\n",
    "    return results_df\n",
    "\n",
    "def optimize_scalar(series):\n",
    "    def obj(s):\n",
    "        if s <= 0:\n",
    "            return np.inf\n",
    "        return skew(np.log1p(s * series))**2\n",
    "\n",
    "    result = minimize_scalar(obj, bounds=(1e-5, 1e20), method='bounded')\n",
    "    print(result.x)\n",
    "    return result.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kind = 'trades'\n",
    "root = 'data/daily-trades/'\n",
    "data_path = root + 'series/'\n",
    "verified_path = root + 'verified_files.csv'\n",
    "price_col = 'average'\n",
    "\n",
    "fund_df = load('data/fundamentals.csv')\n",
    "fund_df['funds_date'] = pd.to_datetime(fund_df['funds_date'])\n",
    "verified_df = verify_files(verified_path, data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load full historical price series\n",
    "if 'meta' not in globals():# or input('Reload CSVs? (y/n)').lower().strip() == 'y':\n",
    "    last_date = (datetime.now() - timedelta(days=365 * 99))\n",
    "    first_date = (datetime.now())\n",
    "    meta = []\n",
    "    file_list = os.listdir(data_path)\n",
    "    for file in tqdm(file_list, total=len(file_list), desc=\"Loading files\"):\n",
    "        if not file.endswith('.csv'):\n",
    "            continue\n",
    "        parts = os.path.splitext(file)[0].split('-')\n",
    "        symbol, exchange, currency = parts[0], parts[1], parts[2]\n",
    "        if not ((verified_df['symbol'] == symbol) & (verified_df['currency'] == currency)).any():\n",
    "            continue\n",
    "        try:\n",
    "            df = load(data_path + file)\n",
    "            df = ensure_series_types(df, price_col)\n",
    "            df = validate_raw_prices(df, price_col)\n",
    "            df = handle_stale_periods(df, price_col)\n",
    "            df['pct_change'] = df[price_col].pct_change()\n",
    "\n",
    "            if df['date'].max() > last_date:\n",
    "                last_date = df['date'].max()\n",
    "            if df['date'].min() < first_date:\n",
    "                first_date = df['date'].min()\n",
    "            \n",
    "            meta.append({\n",
    "                'symbol': symbol,\n",
    "                'currency': currency,\n",
    "                'exchange_api': exchange,\n",
    "                'df': df[['date', price_col, 'volume', 'pct_change']],\n",
    "            })\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"ERROR loading {file}: {e}\")\n",
    "\n",
    "    meta = pd.DataFrame(meta)\n",
    "    detect_and_nullify_global_outliers(meta, price_col=price_col, z_threshold=Z_THRESHOLD_GLOBAL_LOOP, window=OUTLIER_WINDOW_DEFAULT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download supplementary data\n",
    "if 'risk_free_df_full' not in globals():# or input('Redownload supplementary data? (y/n)').lower().strip() == 'y':\n",
    "    # Risk-free series calculation\n",
    "    tickers = {\n",
    "        'US': 'DTB3',\n",
    "        'Canada': 'IR3TIB01CAM156N',\n",
    "        'Germany': 'IR3TIB01DEM156N',\n",
    "        'UK': 'IR3TIB01GBM156N',\n",
    "        'France': 'IR3TIB01FRA156N',\n",
    "    }\n",
    "    bonds = {}\n",
    "    failed = []\n",
    "    for country, ticker in tickers.items():\n",
    "        try:\n",
    "            series = web.DataReader(ticker, 'fred', first_date, last_date)\n",
    "            bonds[country] = series / 100.0\n",
    "        except Exception:\n",
    "            try:\n",
    "                series = web.DataReader(ticker, 'oecd', first_date, last_date)\n",
    "                bonds[country] = series / 100.0\n",
    "            except Exception as oecd_err:\n",
    "                failed.append(country)\n",
    "\n",
    "    # Combine into a single DataFrame\n",
    "    df_bonds = pd.concat(bonds, axis=1)\n",
    "    df_bonds.columns = [c for c in tickers if c not in failed]\n",
    "    df_bonds = df_bonds.interpolate(method='akima').bfill().ffill()\n",
    "\n",
    "    risk_free_df_full = df_bonds.mean(axis=1).rename('nominal_rate')\n",
    "    business_days = pd.date_range(start=first_date, end=last_date, freq='B')\n",
    "    risk_free_df_full = risk_free_df_full.reindex(business_days, copy=False)\n",
    "\n",
    "    risk_free_df_full = pd.DataFrame(risk_free_df_full)\n",
    "    risk_free_df_full['daily_nominal_rate'] = risk_free_df_full['nominal_rate'] / 252\n",
    "\n",
    "    # Get country stats\n",
    "    indicator_name_map = {\n",
    "        'NY.GDP.PCAP.CD': 'gdp_pcap',\n",
    "        'SP.POP.TOTL': 'population',\n",
    "    }\n",
    "    cc = coco.CountryConverter()\n",
    "\n",
    "    all_country_cols = [col for col in fund_df.columns if col.startswith('countries') and not col.endswith('variety')]\n",
    "    all_possible_standard_names = set()\n",
    "    for col in all_country_cols:\n",
    "        raw_name = col.replace('countries_', '').replace(' ', '')\n",
    "        standard_name = cc.convert(names=raw_name, to='ISO3', not_found=None)\n",
    "        if standard_name:\n",
    "            all_possible_standard_names.add(standard_name)\n",
    "\n",
    "    world_bank_data_full = fetch_world_bank_data(all_possible_standard_names, first_date, last_date, indicator_name_map.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YEAR_RANGE = 5\n",
    "training_oldest = last_date - timedelta(days=365 * YEAR_RANGE)\n",
    "\n",
    "meta_window = meta.copy()\n",
    "meta_window['df'] = meta['df'].apply(lambda df: df.loc[df['date'].between(training_oldest, last_date)].copy())\n",
    "business_days = pd.date_range(start=training_oldest, end=last_date, freq='B')\n",
    "\n",
    "for idx, row in meta_window.iterrows():\n",
    "    df = row['df']\n",
    "    merged = pd.DataFrame({'date': business_days}).merge(df, on='date', how='left')\n",
    "    present = merged[price_col].notna()\n",
    "    present_idx = np.flatnonzero(present)\n",
    "    gaps = []\n",
    "    length = len(merged)\n",
    "    if present_idx.size > 0:\n",
    "        if present_idx[0] > 0:\n",
    "            gaps.append(present_idx[0])\n",
    "        if present_idx.size > 1:\n",
    "            internal_gaps = np.diff(present_idx) - 1\n",
    "            gaps.extend(gap for gap in internal_gaps if gap > 0)\n",
    "        if present_idx[-1] < length - 1:\n",
    "            gaps.append(length - 1 - present_idx[-1])\n",
    "    else:\n",
    "        gaps = [length]\n",
    "    gaps = np.array(gaps, dtype=int)\n",
    "    gaps = gaps[gaps > 0]\n",
    "    max_gap = float(gaps.max()) if gaps.size > 0 else 0.0\n",
    "    std_gap = float(gaps.std()) if gaps.size > 0 else 0.0\n",
    "    missing = length - present.sum()\n",
    "    pct_missing = missing / length\n",
    "    meta_window.at[idx, 'df'] = merged\n",
    "    meta_window.at[idx, 'max_gap'] = max_gap\n",
    "    meta_window.at[idx, 'missing'] = missing\n",
    "    meta_window.at[idx, 'pct_missing'] = pct_missing\n",
    "meta_window['max_gap_log'] = np.log1p(meta_window['max_gap'])\n",
    "\n",
    "## static 3y window mean stats\n",
    "condition = ((meta_window['max_gap_log'] < MAX_GAP_LOG) & (meta_window['pct_missing'] < MAX_PCT_MISSING))\n",
    "filtered = meta_window[condition].copy()\n",
    "print(f'{len(filtered)} ETFs included')\n",
    "print(f'{len(meta_window) - len(filtered)} dropped')\n",
    "del meta_window\n",
    "\n",
    "for idx, row in filtered.iterrows():\n",
    "    df = row['df']\n",
    "    df[price_col] = df[price_col].interpolate(method='akima', limit_direction='both')\n",
    "    if df[price_col].isna().any():\n",
    "        df[price_col] = df[price_col].ffill()\n",
    "        df[price_col] = df[price_col].bfill()\n",
    "    df['pct_change'] = df[price_col].pct_change()\n",
    "    filtered.at[idx, 'df'] = df.set_index('date')\n",
    "\n",
    "# Isolate to one fundamental row per conId\n",
    "training_cutoff = last_date - pd.Timedelta(days=TRAINING_PERIOD_DAYS)\n",
    "\n",
    "before_training_end = fund_df[fund_df['funds_date'] <= training_cutoff]\n",
    "if not before_training_end.empty:\n",
    "    before_training_end = before_training_end.loc[before_training_end.groupby('conId')['funds_date'].idxmax()]\n",
    "else:\n",
    "    before_training_end = pd.DataFrame(columns=fund_df.columns)\n",
    "\n",
    "after_training_end = fund_df[fund_df['funds_date'] > training_cutoff]\n",
    "if not after_training_end.empty:\n",
    "    after_training_end = after_training_end.loc[after_training_end.groupby('conId')['funds_date'].idxmin()]\n",
    "else:\n",
    "    after_training_end = pd.DataFrame(columns=fund_df.columns)\n",
    "\n",
    "if not before_training_end.empty and not after_training_end.empty:\n",
    "    after_training_end = after_training_end[~after_training_end['conId'].isin(before_training_end['conId'])]\n",
    "spliced_fund_df = pd.concat([before_training_end, after_training_end])\n",
    "\n",
    "filtered = pd.merge(filtered, spliced_fund_df, on=['symbol', 'currency'], how='inner').drop(['max_gap', 'missing', 'pct_missing', 'max_gap_log'], axis=1)\n",
    "numerical_cols = [col for col in filtered.columns if filtered[col].dtype in [np.int64, np.float64] and col not in ['conId']]\n",
    "pct_changes = pd.concat(\n",
    "        [row['df']['pct_change'].rename(row['conId']) \n",
    "        for _, row in filtered.iterrows()], axis=1\n",
    "    )\n",
    "\n",
    "# Remove uninformative cols for market portfolios \n",
    "uninformative_cols = [col for col in numerical_cols if filtered[col].nunique(dropna=True) <= 1]\n",
    "filtered = filtered.drop(columns=uninformative_cols)\n",
    "filtered = filtered.dropna(axis=1, how='all')\n",
    "\n",
    "# Add rate of change fundamentals\n",
    "rate_fundamentals = [('EPSGrowth-1yr', 'EPS_growth_3yr', 'EPS_growth_5yr'),\n",
    "                    ('ReturnonAssets1Yr', 'ReturnonAssets3Yr'),\n",
    "                    ('ReturnonCapital', 'ReturnonCapital3Yr'),\n",
    "                    ('ReturnonEquity1Yr', 'ReturnonEquity3Yr'),\n",
    "                    ('ReturnonInvestment1Yr', 'ReturnonInvestment3Yr')]\n",
    "\n",
    "for cols in rate_fundamentals:\n",
    "    base_name = cols[0].replace('-1yr', '').replace('1Yr', '')\n",
    "    slope_col = f'fundamentals_{base_name}_slope'\n",
    "    if len(cols) == 3:\n",
    "        col_1yr, col_3yr, col_5yr = cols\n",
    "        filtered[slope_col] = calculate_slope(filtered[f'fundamentals_{col_1yr}'], filtered[f'fundamentals_{col_5yr}'], 1, 5)\n",
    "        slope_1yr_3yr = calculate_slope(filtered[f'fundamentals_{col_1yr}'], filtered[f'fundamentals_{col_3yr}'], 1, 3)\n",
    "        slope_3yr_5yr = calculate_slope(filtered[f'fundamentals_{col_3yr}'], filtered[f'fundamentals_{col_5yr}'], 3, 5)\n",
    "        filtered[f'fundamentals_{base_name}_second_deriv'] = calculate_slope(slope_1yr_3yr, slope_3yr_5yr, 1, 3)\n",
    "    elif len(cols) == 2:\n",
    "        col_1yr, col_3yr = cols\n",
    "        filtered[slope_col] = calculate_slope(filtered[f'fundamentals_{col_1yr}'], filtered[f'fundamentals_{col_3yr}'], 1, 3)\n",
    "numerical_cols = [col for col in filtered.columns if filtered[col].dtype in [np.int64, np.float64] and col not in ['conId']]\n",
    "\n",
    "# Return stats and split training and tests sets\n",
    "momentum_cutoffs = {\n",
    "    '1y':  training_cutoff - pd.Timedelta(days=MOMENTUM_PERIODS_DAYS['1y']),\n",
    "    '6mo': training_cutoff - pd.Timedelta(days=MOMENTUM_PERIODS_DAYS['6mo']),\n",
    "    '3mo': training_cutoff - pd.Timedelta(days=MOMENTUM_PERIODS_DAYS['3mo']),\n",
    "}\n",
    "risk_free_df = risk_free_df_full.loc[business_days]\n",
    "filtered[['momentum_3mo', 'momentum_6mo', 'momentum_1y', 'stats_sharpe']] = filtered['df'].apply(lambda df: get_return_stats(df, training_cutoff, momentum_cutoffs, risk_free_df))\n",
    "\n",
    "holding_cols = [col for col in filtered.columns if col.startswith('holding_') and col != 'holding_types_variety'] + ['total']\n",
    "portfolio_dfs = {}\n",
    "\n",
    "for holding_col in holding_cols:\n",
    "    name = holding_col.split('_')[-1]\n",
    "    if holding_col == 'total':\n",
    "        weight = filtered['profile_cap_usd']\n",
    "    else:\n",
    "        weight = (filtered['profile_cap_usd'] * filtered[holding_col])\n",
    "\n",
    "    total_market_cap = (weight).sum()\n",
    "    filtered['weight'] = weight / total_market_cap\n",
    "    \n",
    "    weights = filtered.set_index('conId')['weight']\n",
    "    portfolio_return = pct_changes.dot(weights)\n",
    "    initial_price = 1\n",
    "    portfolio_price = initial_price * (1 + portfolio_return.fillna(0)).cumprod()\n",
    "\n",
    "    portfolio_df = pd.DataFrame({\n",
    "        'date': portfolio_price.index,\n",
    "        price_col: portfolio_price.values,\n",
    "        'pct_change': portfolio_return.values\n",
    "    }).set_index('date')\n",
    "\n",
    "    portfolio_dfs[name] = portfolio_df\n",
    "\n",
    "filtered.drop('weight', axis=1, inplace=True)\n",
    "\n",
    "# Avoid dummy trap\n",
    "empty_subcategories = {\n",
    "'holding_types': ['other'],\n",
    "'countries': ['Unidentified'], \n",
    "'currencies': ['<NoCurrency>'],\n",
    "'industries': ['NonClassifiedEquity', 'NotClassified-NonEquity'],\n",
    "'top10': ['OtherAssets', 'AccountsPayable','AccountsReceivable','AccountsReceivable&Pay','AdministrationFees','CustodyFees','ManagementFees','OtherAssetsandLiabilities','OtherAssetslessLiabilities', 'OtherFees','OtherLiabilities','Tax','Tax--ManagementFees'],\n",
    "'debtors': ['OTHER'],\n",
    "'maturity': ['%MaturityOther'],\n",
    "'debt_type': ['%QualityNotAvailable', '%QualityNotRated'],\n",
    "'manual': ['asset_other']\n",
    "}\n",
    "\n",
    "dummy_trap_cols = []\n",
    "for k, lst in empty_subcategories.items():\n",
    "    for i in lst:\n",
    "        if k == 'manual':\n",
    "            dummy_trap_cols.append(i)\n",
    "        else:\n",
    "            dummy_trap_cols.append(f'{k}_{i}')\n",
    "    \n",
    "filtered = filtered.drop(columns=dummy_trap_cols, axis=1, errors='ignore')\n",
    "numerical_cols = [col for col in filtered.columns if filtered[col].dtype in [np.int64, np.float64] and col not in ['conId']]\n",
    "\n",
    "# Select asset types to work on\n",
    "asset_conditions = {\n",
    "    'equity': (filtered['asset_equity'] == 1),\n",
    "    'cash': (filtered['asset_cash'] == 1),\n",
    "    'bond': (filtered['asset_bond'] == 1),\n",
    "    'other': (filtered['asset_equity'] == 0) & (filtered['asset_cash'] == 0) & (filtered['asset_bond'] == 0),\n",
    "}\n",
    "\n",
    "exclude_assets = ['bond', 'cash']\n",
    "asset_classes = list(asset_conditions.keys())\n",
    "\n",
    "include_assets = [asset for asset in asset_classes if asset not in exclude_assets]\n",
    "combined_condition = pd.Series(False, index=filtered.index)\n",
    "for asset in include_assets:\n",
    "    combined_condition |= asset_conditions[asset]\n",
    "\n",
    "filtered_df = filtered[combined_condition]\n",
    "numerical_cols = [col for col in filtered_df.columns if filtered_df[col].dtype in [np.int64, np.float64] and col not in ['conId']]\n",
    "\n",
    "single_value_columns = [col for col in filtered_df.columns if col in numerical_cols and filtered_df[col].nunique() == 1]\n",
    "asset_cols = [col for col in filtered_df if col.startswith('asset')]\n",
    "filtered_df = filtered_df.drop(columns=single_value_columns + asset_cols)\n",
    "numerical_cols = [col for col in filtered_df.columns if filtered_df[col].dtype in [np.int64, np.float64] and col not in ['conId']]\n",
    "\n",
    "pct_changes = pct_changes[filtered_df['conId']]\n",
    "del filtered\n",
    "\n",
    "# Standardize country column names\n",
    "cc = coco.CountryConverter()\n",
    "country_cols = [col for col in filtered_df.columns if col.startswith('countries') and not col.endswith('variety')]\n",
    "standard_names = set()\n",
    "rename_map = {}\n",
    "for col in country_cols:\n",
    "    if col == 'countries_Unidentified':\n",
    "        continue\n",
    "\n",
    "    raw_name = col.replace('countries_', '').replace(' ', '')\n",
    "    raw_name = ''.join([' ' + char if char.isupper() and i > 0 else char for i, char in enumerate(raw_name)]).strip()\n",
    "\n",
    "    standard_name = cc.convert(names=raw_name, to='ISO3', not_found=None)\n",
    "    standard_names.add(standard_name)\n",
    "    if standard_name:\n",
    "        rename_map[col] = f'countries_{standard_name}'\n",
    "    else:\n",
    "        print(f\"Could not standardize: '{raw_name}' (from column '{col}')\")\n",
    "filtered_df.rename(columns=rename_map, inplace=True)\n",
    "\n",
    "# Collapse country columns\n",
    "metric_df = calculate_country_stats(world_bank_data_full, standard_names, last_date, window_size=3)\n",
    "metric_suffixes = {\n",
    "    'raw_value': '_value',\n",
    "    '1st_div': '_growth',\n",
    "    '2nd_div': '_acceleration'\n",
    "}\n",
    "for ind_code, ind_name in indicator_name_map.items():\n",
    "    if ind_code in metric_df.columns.get_level_values(0):\n",
    "        for metric_col, suffix in metric_suffixes.items():\n",
    "            new_col_name = f'{ind_name}{suffix}'\n",
    "            filtered_df[new_col_name] = 0.0\n",
    "\n",
    "for std_name in standard_names:\n",
    "    country_weight_col = f'countries_{std_name}'\n",
    "    if country_weight_col not in filtered_df.columns:\n",
    "        continue\n",
    "\n",
    "    if std_name in metric_df.index:\n",
    "        for ind_code, ind_name in indicator_name_map.items():\n",
    "            if ind_code in metric_df.columns.get_level_values(0):\n",
    "                for metric_col, suffix in metric_suffixes.items():\n",
    "                    value = metric_df.loc[std_name, (ind_code, metric_col)]\n",
    "                    target_col = f'{ind_name}{suffix}'\n",
    "                    filtered_df[target_col] += filtered_df[country_weight_col] * value\n",
    "\n",
    "# Drop single unique value columns\n",
    "numerical_cols = [col for col in filtered_df.columns if filtered_df[col].dtype in [np.int64, np.float64] and col not in ['conId']]\n",
    "single_value_columns = [col for col in numerical_cols if filtered_df[col].nunique() == 1]\n",
    "filtered_df = filtered_df.drop(columns=single_value_columns, errors='ignore')\n",
    "numerical_cols = [col for col in filtered_df.columns if filtered_df[col].dtype in [np.int64, np.float64] and col not in ['conId']]\n",
    "\n",
    "fundamental_columns = [col for col in filtered_df.columns if col.startswith('fundamentals')]\n",
    "\n",
    "value_columns_inverted = [\n",
    "    'fundamentals_Price/Book',\n",
    "    'fundamentals_Price/Cash',\n",
    "    'fundamentals_Price/Earnings',\n",
    "    'fundamentals_Price/Sales',\n",
    "]\n",
    "leverage_columns_inverted = [\n",
    "    'fundamentals_LTDebt/Shareholders',\n",
    "    'fundamentals_TotalDebt/TotalCapital',\n",
    "    'fundamentals_TotalDebt/TotalEquity',\n",
    "    'fundamentals_TotalAssets/TotalEquity',\n",
    "]\n",
    "profitability_columns = [\n",
    "    'fundamentals_ReturnonAssets1Yr',\n",
    "    'fundamentals_ReturnonAssets3Yr',\n",
    "    'fundamentals_ReturnonCapital',\n",
    "    'fundamentals_ReturnonCapital3Yr',\n",
    "    'fundamentals_ReturnonEquity1Yr',\n",
    "    'fundamentals_ReturnonEquity3Yr',\n",
    "    'fundamentals_ReturnonInvestment1Yr',\n",
    "    'fundamentals_ReturnonInvestment3Yr',\n",
    "]\n",
    "momentum_columns = [\n",
    "    'momentum_3mo',\n",
    "    'momentum_6mo',\n",
    "    'momentum_1y',\n",
    "    'fundamentals_RelativeStrength'\n",
    "]\n",
    "columns_to_scale = value_columns_inverted + leverage_columns_inverted + profitability_columns + momentum_columns\n",
    "if any(x in filtered_df.columns for x in columns_to_scale):\n",
    "    scaler = MinMaxScaler()\n",
    "    filtered_df[columns_to_scale] = scaler.fit_transform(filtered_df[columns_to_scale])\n",
    "\n",
    "    filtered_df['factor_value'] = (1 - filtered_df[value_columns_inverted]).sum(axis=1)\n",
    "    filtered_df['factor_leverage'] = (1 - filtered_df[leverage_columns_inverted]).sum(axis=1)\n",
    "    filtered_df['factor_profitability'] = filtered_df[profitability_columns].sum(axis=1)\n",
    "    filtered_df['factor_momentum'] = filtered_df[momentum_columns].sum(axis=1)\n",
    "\n",
    "    filtered_df = filtered_df.drop(columns=columns_to_scale, errors='ignore')\n",
    "\n",
    "# Reorganize columns\n",
    "categories = ['factor', 'holding_types', 'stats', 'momentum', 'profile', 'top10', 'population', 'msci', 'gdp', 'continent', 'countries', 'fundamentals', 'industries', 'currencies', 'debtors', 'maturity', 'debt_type', 'lipper', 'dividends', 'marketcap', 'style', 'domicile', 'asset']\n",
    "numerical_cols = [col for col in filtered_df.columns if filtered_df[col].dtype in [np.int64, np.float64] and col not in ['conId']]\n",
    "non_numerical = [col for col in filtered_df.columns if col not in numerical_cols]\n",
    "for category in reversed(categories):\n",
    "    cat_cols = [col for col in numerical_cols if col.startswith(category)]\n",
    "    remaining = [col for col in numerical_cols if col not in cat_cols]\n",
    "    numerical_cols = cat_cols + remaining\n",
    "new_column_order = non_numerical + numerical_cols\n",
    "filtered_df = filtered_df[new_column_order]\n",
    "\n",
    "factors_df = construct_factors(filtered_df, pct_changes, portfolio_dfs, risk_free_df, scaling_factor=FACTOR_SCALING_FACTOR)\n",
    "\n",
    "# Custom drop\n",
    "low_absolute_beta = ['profile_cap_usd', 'holding_types_equity', 'industries_BasicMaterials', 'continent_Oceania_beta', 'holding_types_bond_beta']#, 'factor_smb_beta']\n",
    "frequently_lassoed = ['gdp_pcap_growth', 'gdp_pcap_acceleration', 'continent_Africa', 'population_value', 'industries_Financials_beta', 'stats_sharpe']\n",
    "walk_forward = ['industries_Healthcare_beta', 'continent_America_beta']#, 'factor_momentum_beta', 'factor_profitability_beta']\n",
    "custom_drop = low_absolute_beta + frequently_lassoed + walk_forward\n",
    "custom_drop = [c.split('_beta')[0] for c in custom_drop]\n",
    "factors_df = factors_df.drop(columns=custom_drop, errors='ignore')\n",
    "\n",
    "# Screen factors\n",
    "distilled_factors, drop_map = prescreen_factors(factors_df, correlation_threshold=CORRELATION_THRESHOLD)\n",
    "# corr_matrix = distilled_factors.corr()\n",
    "# vif_df = calculate_vif(distilled_factors.dropna(axis=0))\n",
    "# highest_vif = vif_df['VIF'].iloc[0]\n",
    "# if distilled_factors.shape[1] > 2:\n",
    "#     to_drop = vif_df['feature'].iloc[0]\n",
    "#     distilled_factors.drop(columns=[to_drop], inplace=True)\n",
    "\n",
    "# np.fill_diagonal(corr_matrix.values, 0)\n",
    "# keeper = corr_matrix[to_drop].sort_values(ascending=False).index[0]\n",
    "# drop_map.setdefault(keeper, []).append(to_drop)\n",
    "calculate_vif(distilled_factors.dropna(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Final factors\n",
    "drop_map = merge_drop_map(drop_map)\n",
    "if drop_map:\n",
    "    display(pd.Series(drop_map))\n",
    "print(distilled_factors.shape)\n",
    "\n",
    "categories = list(set([col.split('_')[0] for col in distilled_factors.columns]) | set(categories))\n",
    "print('\\n# Factors included:')\n",
    "for cat in categories:\n",
    "    if cat in ['tradable']:\n",
    "        cat_list = ['tradable']\n",
    "    else:\n",
    "        cat_list = [col.split(cat)[-1].strip('_').capitalize() for col in distilled_factors.columns if col.startswith(cat)]\n",
    "    if cat_list:\n",
    "        print(f'{(',  ').join(cat_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ElasticNet regression\n",
    "results_df = run_elastic_net(\n",
    "    factors_df=distilled_factors,\n",
    "    pct_changes=pct_changes,\n",
    "    risk_free_df=risk_free_df,\n",
    "    training_cutoff=training_cutoff,\n",
    "    alphas=ENET_ALPHAS,\n",
    "    l1_ratio=ENET_L1_RATIOS,\n",
    "    cv=ENET_CV,\n",
    "    tol=ENET_TOL,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-regression factor screening\n",
    "from scipy.stats import skew\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "beta_cols = [col for col in results_df if col.endswith('beta')]\n",
    "screening_df = pd.DataFrame(index=results_df.index)\n",
    "\n",
    "screening_df['r2_adj_test'] = results_df['r2_test'].max() - results_df['r2_test']\n",
    "screening_df['r2_adj_test'] = np.log1p(screening_df['r2_adj_test'] * optimize_scalar(screening_df['r2_adj_test']))\n",
    "screening_df['r2_adj_test'] = screening_df['r2_adj_test'].max() - screening_df['r2_adj_test']\n",
    "\n",
    "screening_df['cv_mse_std'] = results_df[['cv_mse_best','cv_mse_average','cv_mse_worst']].std(axis=1)\n",
    "screening_df['cv_mse_std'] = np.log1p(screening_df['cv_mse_std'] * optimize_scalar(screening_df['cv_mse_std']))\n",
    "\n",
    "screening_df['screening_score'] = screening_df['r2_adj_test'] / (1 + screening_df['cv_mse_std'])\n",
    "\n",
    "\n",
    "display_df = pd.DataFrame({\n",
    "    'mean_beta':  results_df[beta_cols].abs().mean(),\n",
    "    'mean_adj_beta': results_df[beta_cols].abs().multiply(screening_df['screening_score'], axis=0).mean(),\n",
    "    'non_zero_percentage': (results_df[beta_cols].abs() > 1e-6).sum() / len(results_df),\n",
    "})\n",
    "display_df.sort_values(by='non_zero_percentage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factor-based ER\n",
    "beta_cols = [col for col in results_df.columns if col.endswith('beta')]\n",
    "asset_betas = results_df[beta_cols]\n",
    "asset_betas.columns = [col.replace('_beta', '') for col in beta_cols]\n",
    "asset_betas = asset_betas[distilled_factors.columns]\n",
    "\n",
    "factor_premia = distilled_factors.mean()\n",
    "# factor_premia[factor_premia > 0] = 0\n",
    "# factor_premia *= -1\n",
    "\n",
    "systematic_returns = asset_betas.dot(factor_premia)\n",
    "factor_based_er = results_df['jensens_alpha'] + systematic_returns\n",
    "\n",
    "screening_df = pd.DataFrame(index=results_df.index)\n",
    "screening_df['expected_return'] = factor_based_er\n",
    "screening_df['expected_return'] -= screening_df['expected_return'].min()\n",
    "\n",
    "screening_df['r2_test'] = results_df['r2_test'].max() - results_df['r2_test']\n",
    "screening_df['r2_test'] = np.log1p(screening_df['r2_test'] * optimize_scalar(screening_df['r2_test']))\n",
    "screening_df['r2_test'] = screening_df['r2_test'].max() - screening_df['r2_test']\n",
    "\n",
    "screening_df['cv_mse_std'] = results_df[['cv_mse_best','cv_mse_average','cv_mse_worst']].std(axis=1)\n",
    "screening_df['cv_mse_std'] = np.log1p(screening_df['cv_mse_std'] * optimize_scalar(screening_df['cv_mse_std']))\n",
    "screening_df['cv_mse_std'] = screening_df['cv_mse_std'].max() - screening_df['cv_mse_std']\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "screening_df[['r2_test', 'cv_mse_std']] = scaler.fit_transform(screening_df[['r2_test', 'cv_mse_std']])\n",
    "screening_df['r2_adjusted_er'] = screening_df['expected_return'] * screening_df['r2_test'] * screening_df['cv_mse_std']\n",
    "screening_df['historical_er'] = pct_changes.mean()\n",
    "\n",
    "mu_utility = screening_df['r2_adjusted_er'] \n",
    "mu_historical = pct_changes.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factor-based COV\n",
    "factor_cov_matrix = distilled_factors.cov()\n",
    "idiosyncratic_variances = results_df['mse_train']\n",
    "D = np.diag(results_df['mse_train'])\n",
    "\n",
    "systematic_cov = asset_betas.values @ factor_cov_matrix.values @ asset_betas.values.T\n",
    "S = pd.DataFrame(\n",
    "    systematic_cov + D,\n",
    "    index=results_df.index,\n",
    "    columns=results_df.index\n",
    ")\n",
    "\n",
    "# Static Risk-free rate \n",
    "rf_rate = risk_free_df['daily_nominal_rate'].iloc[-10:].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Portfolio Mean-Variance + Factor Exposure Balance Optimization\n",
    "import cvxpy as cp\n",
    "from pypfopt import base_optimizer\n",
    "\n",
    "def portfolio_factor_dispersion(w, asset_betas):\n",
    "    portfolio_betas = w @ asset_betas\n",
    "    n_factors = asset_betas.shape[1]\n",
    "    demeaned_betas = portfolio_betas - (cp.sum(portfolio_betas) / n_factors)\n",
    "    return cp.norm(demeaned_betas, 2)\n",
    "\n",
    "def optimize_with_factor_balance(mu, S, asset_betas, lambda_dispersion, lambda_risk, solver='CLARABEL'):\n",
    "    n_assets = len(mu)\n",
    "    w = cp.Variable(n_assets)\n",
    "\n",
    "    expected_return = mu.values @ w\n",
    "    portfolio_risk = cp.quad_form(w, S)\n",
    "    factor_dispersion = portfolio_factor_dispersion(w, asset_betas.values)\n",
    "    \n",
    "    objective = cp.Maximize(\n",
    "        expected_return\n",
    "        - factor_dispersion * lambda_dispersion\n",
    "        - portfolio_risk * lambda_risk\n",
    "    )\n",
    "    \n",
    "    constraints = [cp.sum(w) == 1, w >= 0, w <= 1]\n",
    "    problem = cp.Problem(objective, constraints)\n",
    "    problem.solve(solver=solver)\n",
    "\n",
    "    if problem.status not in [\"optimal\", \"optimal_inaccurate\"]:\n",
    "        print(f\"Warning: Optimal solution not found. Status: {problem.status}\")\n",
    "        return None\n",
    "\n",
    "    weights = pd.Series(w.value, index=mu.index)\n",
    "    return weights\n",
    "\n",
    "def print_portfolio_stats(weights, mu, S, asset_betas):\n",
    "    df = weights[weights > 0].sort_values(ascending=False)\n",
    "    for k,v in df.items():\n",
    "        row = filtered_df[filtered_df['conId'] == k]\n",
    "        symbol = row['symbol'].iloc[0]\n",
    "        print(f'{symbol}: {round(v*100, 2)}%')\n",
    "\n",
    "    rf_rate = risk_free_df['daily_nominal_rate'].iloc[-10:].mean()\n",
    "    er, volatility, sharpe_ratio = base_optimizer.portfolio_performance(weights, mu, S, risk_free_rate=rf_rate)\n",
    "    final_portfolio_betas = weights @ asset_betas\n",
    "\n",
    "    print(f'num_etfs: {len(weights[weights > 0])}')\n",
    "    print(f'beta_std: {final_portfolio_betas.std()}')\n",
    "    print(f'er: {er}')\n",
    "    print(f'volatility: {volatility}')\n",
    "    print(f'sharpe: {sharpe_ratio}\\n')\n",
    "\n",
    "\n",
    "\n",
    "# Single optimization\n",
    "lambda_risk = 1\n",
    "lambda_dispersion = 1\n",
    "\n",
    "weights = optimize_with_factor_balance(\n",
    "    mu=mu_utility,\n",
    "    S=S,\n",
    "    asset_betas=asset_betas,\n",
    "    lambda_risk=lambda_risk, \n",
    "    lambda_dispersion=lambda_dispersion, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_with_cardinality(mu, S, asset_betas, lambda_dispersion, lambda_risk, upper_bounds, max_assets, solver='SCIP'):\n",
    "    n_assets = len(mu)\n",
    "    w = cp.Variable(n_assets)\n",
    "    z = cp.Variable(n_assets, boolean=True)\n",
    "\n",
    "    expected_return = mu.values @ w\n",
    "    portfolio_risk = cp.quad_form(w, S)\n",
    "    factor_dispersion = portfolio_factor_dispersion(w, asset_betas.values)\n",
    "    \n",
    "    objective = cp.Maximize(\n",
    "        expected_return\n",
    "        - factor_dispersion * lambda_dispersion\n",
    "        - portfolio_risk * lambda_risk\n",
    "    )\n",
    "    \n",
    "    constraints = [\n",
    "        cp.sum(w) == 1,\n",
    "        w >= 0,\n",
    "        cp.sum(z) <= max_assets,\n",
    "        w <= upper_bounds * z,\n",
    "    ]\n",
    "    \n",
    "    problem = cp.Problem(objective, constraints)\n",
    "    # problem.solve(solver=solver, verbose=True)\n",
    "    problem.solve(solver=solver)\n",
    "\n",
    "    if problem.status not in [\"optimal\", \"optimal_inaccurate\"]:\n",
    "        print(f\"Warning: Optimal solution not found. Status: {problem.status}\")\n",
    "        # return None\n",
    "\n",
    "    weights = pd.Series(w.value, index=mu.index)\n",
    "    return weights\n",
    "\n",
    "\n",
    "\n",
    "lambda_risk = 1\n",
    "lambda_dispersion = 1\n",
    "upper_bounds = 1\n",
    "\n",
    "first_weights = weights.sort_values(ascending=False).head(100)\n",
    "mu_post = mu_utility[first_weights.index]\n",
    "mu_historical_post = mu_historical[first_weights.index]\n",
    "S_post = S.loc[mu_post.index, mu_post.index]\n",
    "betas_post = asset_betas.loc[mu_post.index]\n",
    "\n",
    "evaluated_results = []\n",
    "test_range = range(2, 8)\n",
    "for max_assets in tqdm(test_range, total = len(test_range)):\n",
    "    start = datetime.now()\n",
    "    weights = optimize_with_cardinality(\n",
    "        mu=mu_post,\n",
    "        S=S_post,\n",
    "        asset_betas=betas_post,\n",
    "        lambda_dispersion=lambda_dispersion,\n",
    "        lambda_risk=lambda_risk,\n",
    "        upper_bounds=upper_bounds,\n",
    "        max_assets = max_assets,\n",
    "    )\n",
    "    print_portfolio_stats(weights, mu_post, S_post, betas_post)\n",
    "\n",
    "    num_assets = len(weights[weights > 0])\n",
    "    assert num_assets == max_assets\n",
    "\n",
    "    er_model, std_model, sharpe_model = base_optimizer.portfolio_performance(weights, mu_post, S_post, risk_free_rate=rf_rate)\n",
    "    er_hist, _, sharpe_hist = base_optimizer.portfolio_performance(weights, mu_historical_post, S_post, risk_free_rate=rf_rate)\n",
    "    final_portfolio_betas = weights @ betas_post\n",
    "\n",
    "    evaluated_results.append({\n",
    "        'sharpe_model': sharpe_model,\n",
    "        'sharpe_hist': sharpe_hist,\n",
    "        'er_model': er_model,\n",
    "        'er_hist': er_hist,\n",
    "        'volatility': std_model,\n",
    "        'factor_beta_std': final_portfolio_betas.std(),\n",
    "        'num_assets': num_assets,\n",
    "        'weights': weights[weights > 0],\n",
    "        'duration': datetime.now() - start\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old Bayesian optimization\n",
    "# from skopt import gp_minimize\n",
    "# from skopt.space import Real\n",
    "# from skopt.utils import use_named_args\n",
    "# from scipy.stats import norm\n",
    "\n",
    "# def progress_callback(res):\n",
    "#     pbar.update(1)\n",
    "\n",
    "# midpoint = 0.05\n",
    "# space  = [\n",
    "#     # Real(0, midpoint, prior='uniform', name='cuttoff_threshold'),\n",
    "#     # Real(midpoint+1e-15, .8, prior='uniform', name='upper_bounds'),\n",
    "#     Real(1e-9, 1, prior='log-uniform', name='lambda_risk'),\n",
    "#     Real(1e-9, 1, prior='log-uniform', name='lambda_dispersion'),\n",
    "#     # Real(1e-5, 1e5, prior='log-uniform', name='lambda_l1'),\n",
    "#     ]\n",
    "# @use_named_args(space)\n",
    "# def objective(**params):\n",
    "#     weights = optimize_with_factor_balance(\n",
    "#         mu=mu_utility,\n",
    "#         S=S,\n",
    "#         asset_betas=asset_betas,\n",
    "#         # cuttoff_threshold=params['cuttoff_threshold'],\n",
    "#         # upper_bounds=params['upper_bounds'],\n",
    "#         lambda_risk=params['lambda_risk'],\n",
    "#         lambda_dispersion=params['lambda_dispersion'],\n",
    "#         # lambda_l1=params['lambda_l1'],\n",
    "#     )\n",
    "#     if weights.isna().sum():\n",
    "#         score = 100\n",
    "#         print(f'Empty: {round(params['cuttoff_threshold'], 3)} - {round(params['upper_bounds'], 3)}  Score: {score}')\n",
    "#         return score\n",
    "\n",
    "#     num_assets = len(weights[weights > 0])\n",
    "#     final_portfolio_betas = weights @ asset_betas\n",
    "\n",
    "#     er_model, std_model, sharpe_model = base_optimizer.portfolio_performance(weights, mu_utility, S, risk_free_rate=rf_rate)\n",
    "#     er_hist, _, sharpe_hist = base_optimizer.portfolio_performance(weights, mu_historical, S, risk_free_rate=rf_rate)\n",
    "\n",
    "#     # score = (sharpe_model - params['cuttoff_threshold']) / (num_assets + 4) # +4 to avoid large marginal changes for small num_assets\n",
    "#     # score = (sharpe_model - params['cuttoff_threshold']) * (1 + normal_dist.pdf(num_assets)*5)\n",
    "#     score = sharpe_model\n",
    "\n",
    "#     evaluated_results.append({\n",
    "#         # 'cuttoff_threshold': params['cuttoff_threshold'],\n",
    "#         # 'upper_bounds': params['upper_bounds'],\n",
    "#         'lambda_risk': params['lambda_risk'],\n",
    "#         'lambda_dispersion': params['lambda_dispersion'],\n",
    "#         # 'lambda_l1': params['lambda_l1'],\n",
    "#         'score': score,\n",
    "#         'sharpe_model': sharpe_model,\n",
    "#         'sharpe_hist': sharpe_hist,\n",
    "#         'er_model': er_model,\n",
    "#         'er_hist': er_hist,\n",
    "#         'volatility': std_model,\n",
    "#         'factor_beta_std': final_portfolio_betas.std(),\n",
    "#         'num_assets': num_assets,\n",
    "#         'weights': weights[weights > 0],\n",
    "#     })\n",
    "#     return -score\n",
    "\n",
    "\n",
    "# MEAN = 5\n",
    "# STD = 2\n",
    "# normal_dist = norm(loc=MEAN, scale=STD)\n",
    "\n",
    "# evaluated_results = []\n",
    "# n_calls = 50\n",
    "# n_initial_points = n_calls//2\n",
    "# n_initial_points = 2**4\n",
    "# n_calls = n_initial_points # Delete later\n",
    "\n",
    "# if 'pbar' in globals():\n",
    "#     pbar.close()\n",
    "# pbar = tqdm(total=n_calls)\n",
    "\n",
    "# result = gp_minimize(\n",
    "#     func=objective,\n",
    "#     dimensions=space,\n",
    "#     n_calls=n_calls,\n",
    "#     # initial_point_generator='lhs',\n",
    "#     initial_point_generator='sobol',\n",
    "#     n_initial_points=n_initial_points,\n",
    "#     random_state=42,\n",
    "#     callback=[progress_callback],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot parameter exploration 2d\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "eval_df = pd.DataFrame(evaluated_results)\n",
    "\n",
    "x = 'lambda_l1'\n",
    "y = 'lambda_risk'\n",
    "\n",
    "plt.scatter(eval_df[x], eval_df[y], c=eval_df['score'], cmap='viridis')\n",
    "# plt.scatter(optimal_dict[x], optimal_dict[y], color='red', marker='D')\n",
    "plt.xlabel(x)\n",
    "plt.ylabel(y)\n",
    "plt.colorbar()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 3D\n",
    "import plotly.express as px\n",
    "\n",
    "def to_text(df):\n",
    "    lines = []\n",
    "    for k,v in df.items():\n",
    "        row = filtered_df[filtered_df['conId'] == k]\n",
    "        symbol = row['symbol'].iloc[0]\n",
    "        conid = int(row['conId'].iloc[0])\n",
    "        lines.append(f'{conid} - {symbol}: {round(v*100, 2)}%')\n",
    "    return '<br>' + '<br>'.join(lines)\n",
    "\n",
    "eval_df = pd.DataFrame(evaluated_results)\n",
    "eval_df['weights'] = eval_df['weights'].apply(lambda x: to_text(x.sort_values(ascending=False)))\n",
    "eval_df['lambda_dispersion'] = np.log(eval_df['lambda_dispersion'])\n",
    "eval_df['lambda_risk'] = np.log(eval_df['lambda_risk'])\n",
    "# eval_df['lambda_l1'] = np.log(eval_df['lambda_l1'])\n",
    "\n",
    "fig = px.scatter_3d(\n",
    "    eval_df,\n",
    "    x='score',\n",
    "    y='lambda_risk',\n",
    "    z='lambda_dispersion',\n",
    "    color='num_assets',\n",
    "    # size='num_assets',\n",
    "    color_continuous_scale=px.colors.sequential.Viridis,\n",
    "    hover_data={\n",
    "        'score': True,\n",
    "        'num_assets': True,\n",
    "        'sharpe_model': True,\n",
    "        'factor_beta_std': True,\n",
    "        # 'cuttoff_threshold': True,\n",
    "        'lambda_risk': True,\n",
    "        'lambda_dispersion': True,\n",
    "        'weights': True,\n",
    "        }\n",
    ")\n",
    "fig.update_layout(height=700)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df[filtered_df['conId'] == 311447467]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df['factor_dispersion'].iloc[0].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the heatmap\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "eval_df = pd.DataFrame(evaluated_results)\n",
    "eval_corr = eval_df.drop(['weights'], axis=1).corr()\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(eval_corr, annot=True, cmap='coolwarm', center=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypfopt import DiscreteAllocation\n",
    "latest_prices = ... # Get the most recent prices for your ETFs\n",
    "da = DiscreteAllocation(cleaned_weights, latest_prices, total_portfolio_value=25000)\n",
    "allocation, leftover = da.lp_portfolio()\n",
    "print(\"Discrete allocation:\", allocation)\n",
    "print(f\"Funds remaining: ${leftover:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
