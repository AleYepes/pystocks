{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ib_async import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm.auto import tqdm\n",
    "from time import sleep\n",
    "import math\n",
    "import re\n",
    "import traceback\n",
    "from functions import *\n",
    "\n",
    "from pathlib import Path\n",
    "os.chdir(Path.cwd().parents[0]) # Set path as if it was in root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kind = 'trades'\n",
    "root = 'data/'\n",
    "data_path = root + 'daily-trades/series/'\n",
    "verified_path = root + 'daily-trades/verified_files.csv'\n",
    "\n",
    "scraped_path = root + f'preprocessed/justetf_{datetime.now().strftime('%Y-%m')}.csv'\n",
    "scraped_df = load(scraped_path)\n",
    "physical_isins = scraped_df[scraped_df['replication'] == 'Physical']['isin']\n",
    "\n",
    "fund_path = root + f'preprocessed/fundamentals_{datetime.now().strftime('%Y-%m')}.csv'\n",
    "fund_df = load(fund_path)\n",
    "fund_df = fund_df[fund_df['isin'].isin(physical_isins)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = False\n",
    "try:\n",
    "    verified_df = pd.read_csv(verified_path)\n",
    "except FileNotFoundError:\n",
    "    error = True\n",
    "\n",
    "if error or input('Update verified files? (y/n)').lower().strip() == 'y':\n",
    "    util.startLoop()\n",
    "    ib = IB()\n",
    "    ib.connect('127.0.0.1', 7497, clientId=4)\n",
    "\n",
    "    file_list = os.listdir(data_path)\n",
    "    verified_files = []\n",
    "\n",
    "    for file_name in tqdm(file_list, total=len(file_list), desc=\"Verifying files\"):\n",
    "        if not file_name.endswith('.csv'):\n",
    "            continue\n",
    "\n",
    "        file_path = os.path.join(data_path, file_name)\n",
    "        try:\n",
    "            match = re.match(r'^(.*?)-([A-Z0-9]+)-([A-Z]{3})\\.csv$', file_name)\n",
    "            if not match:\n",
    "                print(f\"Deleting malformed filename: {file_name}\")\n",
    "                os.remove(file_path)\n",
    "                continue\n",
    "            symbol, exchange, currency = match.groups()\n",
    "            \n",
    "            symbol_data = fund_df[(fund_df['symbol'] == symbol) & (fund_df['currency'] == currency)]\n",
    "            if symbol_data.empty:\n",
    "                # print(f\"No fundamental data for {symbol}. Deleting file.\")\n",
    "                # os.remove(file_path)\n",
    "                continue\n",
    "\n",
    "            contract_details = ib.reqContractDetails(Stock(symbol, exchange, currency))\n",
    "            if not contract_details:\n",
    "                # print(f\"No contract details from IBKR for {symbol}. Deleting file.\")\n",
    "                # os.remove(file_path)\n",
    "                continue\n",
    "\n",
    "            conid = contract_details[0].contract.conId\n",
    "            if conid not in symbol_data['conId'].values:\n",
    "                # print(f\"conId mismatch\")\n",
    "                # os.remove(file_path)\n",
    "                continue\n",
    "\n",
    "            instrument_name = symbol_data['longName'].iloc[0]\n",
    "            if has_bad_multiplier(instrument_name):\n",
    "                # print(f\"Leveraged instrument detected: {instrument_name}. Deleting file.\")\n",
    "                # os.remove(file_path)\n",
    "                continue\n",
    "\n",
    "            verified_files.append({'symbol': symbol, 'currency': currency, 'exchange': exchange, 'conId': conid})\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_name}: {e}\")\n",
    "\n",
    "    if 'verified_df' in globals():\n",
    "        verified_df = pd.concat([verified_df, pd.DataFrame(verified_files)], ignore_index=True).drop_duplicates()\n",
    "    else:\n",
    "        verified_df = pd.DataFrame(verified_files)\n",
    "    verified_df.to_csv(verified_path, index=False)\n",
    "\n",
    "    ib.disconnect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_historical(symbol, exchange, currency, duration='40 Y', kind=None):\n",
    "    contract = Stock(symbol, exchange, currency)\n",
    "    if kind == 'midpoint':\n",
    "        data = ib.reqHistoricalData(\n",
    "            contract, \n",
    "            endDateTime='',\n",
    "            durationStr=duration,\n",
    "            barSizeSetting='1 day', \n",
    "            whatToShow='MIDPOINT', \n",
    "            useRTH=True,\n",
    "        )\n",
    "    elif kind == 'trades':\n",
    "        data = ib.reqHistoricalData(\n",
    "            contract, \n",
    "            endDateTime='',\n",
    "            durationStr=duration,\n",
    "            barSizeSetting='1 day', \n",
    "            whatToShow='TRADES', \n",
    "            useRTH=True,\n",
    "        )\n",
    "    else:\n",
    "        raise Exception('Incorrect kind')\n",
    "    \n",
    "    length = len(data) - 1 if data and exchange == 'SMART' else len(data)\n",
    "    return data, length, exchange\n",
    "\n",
    "def save_data(data_path, data, symbol, exchange, currency):\n",
    "    if data:\n",
    "        data_df = util.df(data)\n",
    "        data_df['date'] = pd.to_datetime(data_df['date']).dt.date\n",
    "        data_df = data_df.sort_values(by='date').reset_index(drop=True)\n",
    "        data_df.to_csv(f'{data_path}{symbol}-{exchange}-{currency}.csv', index=False)\n",
    "        # print(f'{symbol} saved')\n",
    "\n",
    "def fill_internal_gaps(df, symbol, exchange, currency):\n",
    "    if df.empty or 'date' not in df.columns:\n",
    "        return df\n",
    "\n",
    "    df['date'] = pd.to_datetime(df['date']).dt.date\n",
    "    df = df.set_index('date').sort_index()\n",
    "    attempted_gaps = set()\n",
    "\n",
    "    while True:\n",
    "        first_date, last_date = df.index.min(), df.index.max()\n",
    "        business_days = pd.date_range(start=first_date, end=last_date, freq='B')\n",
    "        full_df = df.reindex(business_days)\n",
    "        is_missing = full_df['open'].isnull()\n",
    "        if not is_missing.any():\n",
    "            break\n",
    "\n",
    "        full_df['gap_id'] = (is_missing.diff() != 0).cumsum()\n",
    "        missing_blocks = full_df[is_missing]\n",
    "        gap_info = missing_blocks.groupby('gap_id').agg(\n",
    "            gap_start=('gap_id', lambda x: x.index.min()),\n",
    "            gap_end=('gap_id', lambda x: x.index.max()),\n",
    "            gap_size=('gap_id', 'size')\n",
    "        )\n",
    "        significant_gaps = gap_info[gap_info['gap_size'] >= MIN_GAP_SIZE_TO_FILL].copy()\n",
    "        significant_gaps['gap_tuple'] = significant_gaps.apply(lambda row: (row['gap_start'], row['gap_end']), axis=1)\n",
    "        gaps_to_try = significant_gaps[~significant_gaps['gap_tuple'].isin(attempted_gaps)]\n",
    "        if gaps_to_try.empty:\n",
    "            break\n",
    "\n",
    "        current_gap = gaps_to_try.iloc[0]\n",
    "        attempted_gaps.add(current_gap['gap_tuple'])\n",
    "\n",
    "        start_fill_date = current_gap['gap_start'] - timedelta(days=1)\n",
    "        end_fill_date = current_gap['gap_end'] + timedelta(days=1)\n",
    "        days_to_request = (end_fill_date - start_fill_date).days + 1\n",
    "\n",
    "        duration_str = f'{math.ceil(days_to_request / 365)} Y' if days_to_request >= 365 else f'{days_to_request} D'\n",
    "        end_date_str = end_fill_date.strftime('%Y%m%d 00:00:00')\n",
    "\n",
    "        contract = Stock(symbol, exchange, currency)\n",
    "        gap_data = ib.reqHistoricalData(\n",
    "            contract,\n",
    "            endDateTime=end_date_str,\n",
    "            durationStr=duration_str,\n",
    "            barSizeSetting='1 day',\n",
    "            whatToShow=kind,\n",
    "            useRTH=True,\n",
    "            timeout=30 \n",
    "        )\n",
    "\n",
    "        if gap_data:\n",
    "            gap_df = util.df(gap_data)\n",
    "            gap_df['date'] = pd.to_datetime(gap_df['date']).dt.date\n",
    "            gap_df = gap_df.set_index('date')\n",
    "            df = df.combine_first(gap_df)\n",
    "        else:\n",
    "            print(f\"[{symbol}] API returned no data for gap. Marked as attempted, continuing scan.\")\n",
    "\n",
    "    return df.reset_index().sort_values(by='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.startLoop()\n",
    "ib = IB()\n",
    "ib.connect('127.0.0.1', 7497, clientId=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get missing historical series\n",
    "years = ['40 Y', '10 Y', '5 Y', '3 Y', '2 Y', '1 Y']\n",
    "# years = ['40 Y']\n",
    "for duration in years:\n",
    "    fund_df = pd.read_csv(f'data/preprocessed/fundamentals_{datetime.now().strftime('%Y-%m')}.csv')\n",
    "\n",
    "    # Create a set of (symbol, currency) tuples from existing files\n",
    "    file_list = os.listdir(data_path)\n",
    "    file_keys = set()\n",
    "    for name in file_list:\n",
    "        if name.endswith('.csv'):\n",
    "            try:\n",
    "                symbol, exchange, currency = name.replace('.csv', '').split('-')\n",
    "                file_keys.add((symbol, currency))\n",
    "            except ValueError:\n",
    "                print(f\"Skipping malformed filename: {name}\")\n",
    "\n",
    "    # Identify missing symbols based on symbol and currency\n",
    "    missing_symbols = fund_df[~fund_df.apply(lambda row: (row['symbol'], row['currency']) in file_keys, axis=1)].copy()\n",
    "\n",
    "    count = 0\n",
    "    for _, row in tqdm(missing_symbols.iterrows(), total=len(missing_symbols), desc=f\"Getting {duration} series\"):\n",
    "        symbol = row['symbol']\n",
    "        search_exchange = row['search_exchange']\n",
    "        suggested_exchange = row['exchange']\n",
    "        primary_exchange = row['primaryExchange']\n",
    "        currency = row['currency']\n",
    "        \n",
    "        results = []\n",
    "        if search_exchange:\n",
    "            results.append(get_historical(symbol, search_exchange, currency, duration=duration, kind=kind))\n",
    "            if suggested_exchange != search_exchange:\n",
    "                results.append(get_historical(symbol, suggested_exchange, currency, duration=duration, kind=kind))\n",
    "            if primary_exchange != suggested_exchange and primary_exchange != search_exchange:\n",
    "                results.append(get_historical(symbol, primary_exchange, currency, duration=duration, kind=kind))\n",
    "        else:\n",
    "            results.append(get_historical(symbol, suggested_exchange, currency, duration=duration, kind=kind))\n",
    "            if primary_exchange != suggested_exchange:\n",
    "                results.append(get_historical(symbol, primary_exchange, currency, duration=duration, kind=kind))\n",
    "        results.append(get_historical(symbol, 'SMART', currency, duration=duration, kind=kind))\n",
    "\n",
    "        # Sort by data length and save the best result\n",
    "        results = sorted(results, key=lambda x: x[1], reverse=True)\n",
    "        if results[0][1]:\n",
    "            save_data(data_path, results[0][0], symbol, results[0][2], currency)\n",
    "            count +=1\n",
    "\n",
    "    print(f'{duration}: {count} scraped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update historical series\n",
    "OVERLAP_BUFFER_DAYS = 5\n",
    "MIN_GAP_SIZE_TO_FILL = 5\n",
    "\n",
    "fill_internal = input('fill internal gaps?(y/n)').lower().strip() == 'y'\n",
    "file_list = os.listdir(data_path)\n",
    "for file_name in tqdm(file_list, total=len(file_list), desc=f\"Updating series\"):\n",
    "    if not file_name.endswith('.csv'):\n",
    "        print(f'skipping {file_name}')\n",
    "        continue\n",
    "\n",
    "    match = re.match(r'^(.*?)-([A-Z0-9]+)-([A-Z]{3})\\.csv$', file_name)\n",
    "    if not match:\n",
    "        print(f\"Skipping malformed filename: {file_name}\")\n",
    "        continue\n",
    "    symbol, exchange, currency = match.groups()\n",
    "\n",
    "    file_path = os.path.join(data_path, file_name)\n",
    "    temp_file_path = file_path + '.tmp'\n",
    "\n",
    "    try:\n",
    "        if os.path.getsize(file_path) == 0:\n",
    "            print(f\"Skipping empty file: {file_name}\")\n",
    "            continue\n",
    "\n",
    "        data_df = pd.read_csv(file_path)\n",
    "        if 'date' not in data_df.columns or data_df.empty:\n",
    "            print(f\"Skipping file with no 'date' column or no data: {file_name}\")\n",
    "            continue\n",
    "        data_df['date'] = pd.to_datetime(data_df['date']).dt.date\n",
    "\n",
    "        if fill_internal:\n",
    "            data_df = fill_internal_gaps(data_df, symbol, exchange, currency)\n",
    "            \n",
    "        last_date = data_df['date'].max()\n",
    "        days_missing = (datetime.now().date() - last_date).days\n",
    "        if days_missing > 1:\n",
    "            days_to_request = days_missing + OVERLAP_BUFFER_DAYS\n",
    "            duration = f'{math.ceil(days_to_request / 365)} Y' if days_to_request >= 365 else f'{days_to_request} D'\n",
    "            new_data, _, _ = get_historical(symbol, exchange, currency, duration=duration, kind=kind)\n",
    "\n",
    "            if new_data:\n",
    "                new_data_df = util.df(new_data)\n",
    "                new_data_df['date'] = pd.to_datetime(new_data_df['date']).dt.date\n",
    "                \n",
    "                updated_data_df = pd.concat([data_df, new_data_df], ignore_index=True)\n",
    "                updated_data_df = updated_data_df.drop_duplicates(subset='date', keep='last')\n",
    "            else:\n",
    "                updated_data_df = data_df\n",
    "        else:\n",
    "            updated_data_df = data_df\n",
    "        \n",
    "        updated_data_df = updated_data_df.sort_values(by='date').reset_index(drop=True)\n",
    "        updated_data_df.to_csv(temp_file_path, index=False)\n",
    "        os.rename(temp_file_path, file_path)\n",
    "\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f\"Error: CSV file is empty. Skipping {file_name}\")\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while processing {file_name}: {e}\")\n",
    "        traceback.print_exc()\n",
    "        if os.path.exists(temp_file_path):\n",
    "            os.remove(temp_file_path)\n",
    "        continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
