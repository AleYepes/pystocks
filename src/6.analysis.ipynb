{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import ast\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from ib_async import *\n",
    "import pandas_datareader.data as web\n",
    "import wbgapi as wb\n",
    "import country_converter as coco\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import gc\n",
    "import argparse\n",
    "\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "from scipy.stats import linregress\n",
    "from scipy.stats import skew\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "from pathlib import Path\n",
    "if 'MAX_STALE_DAYS' not in globals():\n",
    "    os.chdir(Path.cwd().parents[0]) # Set path as if it was in root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTANTS\n",
    "CORRELATION_THRESHOLD = .85\n",
    "\n",
    "# Data Cleaning\n",
    "MAX_STALE_DAYS = 5\n",
    "# Default params for detect_and_nullify_global_outliers\n",
    "Z_THRESHOLD_GLOBAL_DEFAULT = 100.0 # 120\n",
    "OUTLIER_WINDOW_DEFAULT = 5\n",
    "# Params for detect_and_nullify_global_outliers in the main loop\n",
    "Z_THRESHOLD_GLOBAL_LOOP = 50\n",
    "\n",
    "# Walk-Forward Analysis\n",
    "WALK_FORWARD_WINDOW_YEARS = range(3, 5)\n",
    "TRAINING_PERIOD_DAYS = 365\n",
    "# TRAINING_PERIOD_DAYS = 200\n",
    "MOMENTUM_PERIODS_DAYS = {\n",
    "    '1y':  TRAINING_PERIOD_DAYS,\n",
    "    '6mo': TRAINING_PERIOD_DAYS // 2,\n",
    "    '3mo': TRAINING_PERIOD_DAYS // 4,\n",
    "}\n",
    "\n",
    "# Asset Filtering\n",
    "MAX_GAP_LOG = 3.05\n",
    "MAX_PCT_MISSING = 0.3\n",
    "\n",
    "# Factor Construction\n",
    "FACTOR_SCALING_FACTOR = 0.6\n",
    "\n",
    "# Elastic Net Hyperparameters\n",
    "ENET_ALPHAS = np.logspace(-11, -4, 30)\n",
    "ENET_L1_RATIOS = [0.01, 0.05, 0.1, 0.3, 0.5, 0.7, 0.9, 0.95, 1]\n",
    "ENET_CV = 5\n",
    "ENET_TOL = 5e-4\n",
    "\n",
    "# Optimization almost 0\n",
    "ZERO = 1e-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_world_bank_data(all_country_codes, start_date, end_date, indicators):\n",
    "    valid_country_codes = {code for code in all_country_codes if code is not None}\n",
    "    try:\n",
    "        wb_economies = {e['id'] for e in wb.economy.list()}\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"FATAL: Failed to fetch economy list from World Bank API: {e}\")\n",
    "\n",
    "    final_economies = sorted([code for code in valid_country_codes if code in wb_economies])\n",
    "    unrecognized = valid_country_codes - set(final_economies)\n",
    "    if unrecognized:\n",
    "        print(f\"Info: The following economies were not recognized by the World Bank API and will be skipped: {unrecognized}\")\n",
    "    if not final_economies:\n",
    "        raise Exception(\"Error: No valid economies found to query the World Bank API.\")\n",
    "\n",
    "    all_data = []\n",
    "    chunk_size = 40\n",
    "    for i in range(0, len(final_economies), chunk_size):\n",
    "        chunk = final_economies[i:i + chunk_size]\n",
    "        try:\n",
    "            data_chunk = wb.data.DataFrame(list(indicators), chunk, time=range(start_date.year - 5, end_date.year + 1), labels=False)\n",
    "            all_data.append(data_chunk)\n",
    "        except wb.APIError as e:\n",
    "            print(f\"API Error fetching data for chunk {i//chunk_size + 1}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred fetching data for chunk {i//chunk_size + 1}: {e}\")\n",
    "\n",
    "    if not all_data:\n",
    "        raise Exception(\"Error: Failed to retrieve any data from the World Bank.\")\n",
    "\n",
    "    return pd.concat(all_data)\n",
    "\n",
    "\n",
    "def evaluate_literal(val):\n",
    "    try:\n",
    "        return ast.literal_eval(val)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return val\n",
    "    \n",
    "def load(path):\n",
    "    df = pd.read_csv(path)\n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].apply(evaluate_literal)\n",
    "    return df\n",
    "\n",
    "def ensure_series_types(df, price_col):\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.sort_values('date').reset_index(drop=True)\n",
    "    for col in ['volume', price_col]:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    return df\n",
    "\n",
    "def validate_raw_prices(df, price_col):\n",
    "    invalid_price_mask = df[price_col] <= 0\n",
    "    inconsistent_mask = pd.Series(False, index=df.index)\n",
    "    if 'low' in df.columns and 'high' in df.columns:\n",
    "        inconsistent_mask = (df['low'] > df['high'])\n",
    "    local_error_mask = invalid_price_mask | inconsistent_mask\n",
    "    df = df[~local_error_mask].copy()\n",
    "    return df\n",
    "\n",
    "def handle_stale_periods(df, price_col, max_stale_days=MAX_STALE_DAYS):\n",
    "    stale_groups = (df[price_col].diff() != 0).cumsum()\n",
    "    if stale_groups.empty:\n",
    "        return df\n",
    "    period_lengths = df.groupby(stale_groups)[price_col].transform('size')\n",
    "    long_stale_mask = period_lengths > max_stale_days\n",
    "    is_intermediate_stale_row = (stale_groups.duplicated(keep='first') & stale_groups.duplicated(keep='last'))\n",
    "    rows_to_drop_mask = long_stale_mask & is_intermediate_stale_row\n",
    "    df = df[~rows_to_drop_mask].copy()\n",
    "    return df\n",
    "\n",
    "def detect_and_nullify_global_outliers(meta_df, price_col, z_threshold=Z_THRESHOLD_GLOBAL_DEFAULT, window=OUTLIER_WINDOW_DEFAULT):\n",
    "    all_pct_changes = pd.concat(\n",
    "        [row['df']['pct_change'] for _, row in meta_df.iterrows()],\n",
    "        ignore_index=True\n",
    "    ).dropna()\n",
    "    all_pct_changes = all_pct_changes[~np.isinf(all_pct_changes) & (all_pct_changes != 0)]\n",
    "\n",
    "    global_median_return = all_pct_changes.median()\n",
    "    global_mad = (all_pct_changes - global_median_return).abs().median()\n",
    "\n",
    "    for idx, row in meta_df.iterrows():\n",
    "        df = row['df']\n",
    "        df = df.reset_index(drop=True)\n",
    "        if df['pct_change'].isnull().all():\n",
    "            continue\n",
    "        cols_to_null = [price_col, 'volume', 'high', 'low', 'pct_change']\n",
    "        cols_to_null = [c for c in cols_to_null if c in df.columns]\n",
    "\n",
    "        absolute_modified_z = (df['pct_change'] - global_median_return).abs() / global_mad\n",
    "        outlier_mask = absolute_modified_z > z_threshold\n",
    "\n",
    "        if outlier_mask.any():\n",
    "\n",
    "            candidate_indices = df.index[outlier_mask]\n",
    "            for df_idx in candidate_indices:\n",
    "                price_to_check_idx = df_idx - 1\n",
    "                price_to_check = df.loc[price_to_check_idx, price_col]\n",
    "                local_window_start = max(0, price_to_check_idx - window)\n",
    "                local_window = df.loc[local_window_start : price_to_check_idx - 1, price_col].dropna()\n",
    "                local_mean = local_window.mean()\n",
    "                local_std = local_window.std()\n",
    "                if local_std != 0: \n",
    "                    price_z_score = abs(price_to_check - local_mean) / local_std\n",
    "                    if price_z_score > z_threshold / 10:\n",
    "                        df.loc[price_to_check_idx, cols_to_null] = np.nan\n",
    "\n",
    "                price_to_check = df.loc[df_idx, price_col]\n",
    "                local_window_end = min(df_idx + window, df.index[outlier_mask].max())\n",
    "                local_window = df.loc[df_idx + 1: local_window_end, price_col].dropna()\n",
    "                local_mean = local_window.mean()\n",
    "                local_std = local_window.std()\n",
    "                if local_std != 0:\n",
    "                    price_z_score = abs(price_to_check - local_mean) / local_std\n",
    "                    if price_z_score > z_threshold / 10:\n",
    "                        df.loc[df_idx, cols_to_null] = np.nan\n",
    "\n",
    "            df['pct_change'] = df[price_col].pct_change(fill_method=None)\n",
    "            meta_df.at[idx, 'df'] = df\n",
    "\n",
    "def calculate_slope(value1, value2, time1, time2):\n",
    "    return (value1 - value2) / (time1 - time2)\n",
    "\n",
    "def get_return_stats(df, training_cutoff, momentum_cutoffs, risk_free_df):\n",
    "    training_df = df[df.index < training_cutoff]\n",
    "    # training_rf = risk_free_df[risk_free_df.index < training_cutoff]\n",
    "\n",
    "    # excess_returns = training_df['pct_change'] - training_rf['daily_nominal_rate']\n",
    "    # sharpe = excess_returns.mean() / excess_returns.std() if excess_returns.std() != 0 else 0\n",
    "\n",
    "    momentum_3mo = training_df[training_df.index >= momentum_cutoffs['3mo']]['pct_change'].mean()\n",
    "    momentum_6mo = training_df[training_df.index >= momentum_cutoffs['6mo']]['pct_change'].mean()\n",
    "    momentum_1y  = training_df[training_df.index >= momentum_cutoffs['1y']]['pct_change'].mean()\n",
    "\n",
    "    rs_3mo = (1 + training_df[training_df.index >= momentum_cutoffs['3mo']]['pct_change']).prod() - 1\n",
    "    rs_6mo = (1 + training_df[training_df.index >= momentum_cutoffs['6mo']]['pct_change']).prod() - 1\n",
    "    rs_1y  = (1 + training_df[training_df.index >= momentum_cutoffs['1y']]['pct_change']).prod() - 1\n",
    "\n",
    "    return pd.Series([momentum_3mo, \n",
    "                      momentum_6mo, \n",
    "                      momentum_1y, \n",
    "                      rs_3mo, \n",
    "                      rs_6mo, \n",
    "                      rs_1y,], \n",
    "                    #   sharpe], \n",
    "              index=['momentum_3mo', \n",
    "                     'momentum_6mo', \n",
    "                     'momentum_1y', \n",
    "                     'rs_3mo', \n",
    "                     'rs_6mo', \n",
    "                     'rs_1y', ])\n",
    "                    #  'stats_sharpe'])\n",
    "\n",
    "def create_continent_map(standard_names):\n",
    "    continents = cc.convert(names=standard_names, to='continent', not_found=None)\n",
    "    return {name: (cont if cont is not None else 'Other')\n",
    "            for name, cont in zip(standard_names, continents)}\n",
    "\n",
    "def calculate_country_stats(world_bank_data_full, standard_names, end_year, window_size=3):\n",
    "    countries_in_window = [name for name in standard_names.values() if name in world_bank_data_full.index.get_level_values('economy')]\n",
    "    if not countries_in_window:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    data = world_bank_data_full.loc[countries_in_window].dropna(axis=1)\n",
    "    available_years = [int(col) for col in data.columns]\n",
    "\n",
    "    cols_to_keep = [col for col, year in zip(data.columns, available_years) if year <= end_year.year]\n",
    "    data = data[cols_to_keep].copy()\n",
    "    data.dropna(axis=1, inplace=True)\n",
    "\n",
    "    yoy_change = data.diff(axis=1)\n",
    "    first_div = yoy_change.T.rolling(window=window_size).mean().T\n",
    "\n",
    "    # yoy_change_first_div = first_div.diff(axis=1)\n",
    "    # second_div = yoy_change_first_div.T.rolling(window=window_size).mean().T\n",
    "\n",
    "    latest_year_col = data.columns[-1]\n",
    "    latest_first_div_col = first_div.columns[-1]\n",
    "    # latest_second_div_col = second_div.columns[-1]\n",
    "\n",
    "    derivatives = pd.DataFrame(data[latest_year_col])\n",
    "    derivatives.rename(columns={latest_year_col: 'raw_value'}, inplace=True)\n",
    "    derivatives['1st_div'] = first_div[latest_first_div_col] / derivatives['raw_value']\n",
    "    # derivatives['2nd_div'] = second_div[latest_second_div_col] / derivatives['raw_value']\n",
    "    \n",
    "    metric_df_reshaped = derivatives.unstack(level='series')\n",
    "    if isinstance(metric_df_reshaped.columns, pd.MultiIndex):\n",
    "         metric_df_final = metric_df_reshaped.swaplevel(0, 1, axis=1)\n",
    "         metric_df_final.sort_index(axis=1, level=0, inplace=True)\n",
    "    else:\n",
    "         metric_df_final = metric_df_reshaped\n",
    "\n",
    "    return metric_df_final\n",
    "\n",
    "def construct_factor_series(meta_df, returns_df, long_symbols, short_symbols=None, factor_column=None):\n",
    "    long_df = meta_df[meta_df['conId'].isin(long_symbols)].set_index('conId')\n",
    "    long_weights = long_df['profile_cap_usd'].reindex(returns_df.columns).fillna(0)\n",
    "    if factor_column:\n",
    "        factor_weights = (meta_df[factor_column].max() - long_df[factor_column]) / (meta_df[factor_column].max() - meta_df[factor_column].min())\n",
    "        factor_weights = factor_weights.reindex(returns_df.columns).fillna(0)\n",
    "        if factor_weights.sum() != 0:\n",
    "            long_weights *= factor_weights\n",
    "\n",
    "    if long_weights.sum() != 0:\n",
    "        long_weights /= long_weights.sum()\n",
    "    long_returns = returns_df.dot(long_weights)\n",
    "\n",
    "    if short_symbols:\n",
    "        short_df = meta_df[meta_df['conId'].isin(short_symbols)].set_index('conId')\n",
    "        short_weights = short_df['profile_cap_usd'].reindex(returns_df.columns).fillna(0)\n",
    "        if factor_column:\n",
    "            factor_weights = (short_df[factor_column] - meta_df[factor_column].min()) / (meta_df[factor_column].max() - meta_df[factor_column].min())\n",
    "            factor_weights = factor_weights.reindex(returns_df.columns).fillna(0)\n",
    "            if factor_weights.sum() != 0:\n",
    "                short_weights *= factor_weights\n",
    "\n",
    "        if short_weights.sum() != 0:\n",
    "            short_weights /= short_weights.sum()\n",
    "        short_returns = returns_df.dot(short_weights)\n",
    "        \n",
    "        return long_returns - short_returns\n",
    "    else:\n",
    "        return long_returns\n",
    "\n",
    "def construct_factors(filtered_df, pct_changes, portfolio_dfs, risk_free_df, scaling_factor=FACTOR_SCALING_FACTOR):\n",
    "    factors = {}\n",
    "    # Market risk premium\n",
    "    factors['factor_market_premium'] = (portfolio_dfs['equity']['pct_change'] - risk_free_df['daily_nominal_rate'])\n",
    "\n",
    "    # SMB_ETF\n",
    "    small_symbols = filtered_df[filtered_df['marketcap_small'] == 1]['conId'].tolist()\n",
    "    large_symbols = filtered_df[filtered_df['marketcap_large'] == 1]['conId'].tolist()\n",
    "\n",
    "    intersection = set(small_symbols) & set(large_symbols)\n",
    "    small_symbols = [s for s in small_symbols if s not in intersection]\n",
    "    large_symbols = [s for s in large_symbols if s not in intersection]\n",
    "    smb_etf = construct_factor_series(filtered_df, pct_changes, small_symbols, short_symbols=large_symbols)\n",
    "    factors['factor_smb'] = smb_etf\n",
    "\n",
    "    # HML_ETF\n",
    "    value_cols = [col for col in filtered_df.columns if col.startswith('style_') and col.endswith('value')]\n",
    "    growth_cols = [col for col in filtered_df.columns if col.startswith('style_') and col.endswith('growth')]\n",
    "    value_symbols = filtered_df[filtered_df[value_cols].ne(0).any(axis=1)]['conId'].tolist()\n",
    "    growth_symbols = filtered_df[filtered_df[growth_cols].ne(0).any(axis=1)]['conId'].tolist()\n",
    "\n",
    "    intersection = set(value_symbols) & set(growth_symbols)\n",
    "    value_symbols = [s for s in value_symbols if s not in intersection]\n",
    "    growth_symbols = [s for s in growth_symbols if s not in intersection]\n",
    "    hml_etf = construct_factor_series(filtered_df, pct_changes, value_symbols, short_symbols=growth_symbols)\n",
    "    factors['factor_hml'] = hml_etf\n",
    "\n",
    "    # Metadata\n",
    "    excluded = ['style_', 'marketcap_', 'countries_', 'fundamentals_', 'momentum_', 'industries_']\n",
    "    only_long = ['industries_', 'holding_types_']\n",
    "    numerical_cols = [col for col in filtered_df.columns if filtered_df[col].dtype in [np.int64, np.float64] and col not in ['conId']]\n",
    "    for col in numerical_cols:\n",
    "        if not any(col.startswith(prefix) for prefix in excluded):\n",
    "            try:\n",
    "                std = filtered_df[col].std()\n",
    "                mean = filtered_df[col].mean()\n",
    "\n",
    "                upper_boundary = min(filtered_df[col].max(), mean + (scaling_factor * std))\n",
    "                lower_boundary = max(filtered_df[col].min(), mean - (scaling_factor * std))\n",
    "\n",
    "                high_factor_symbols = filtered_df[filtered_df[col] >= upper_boundary]['conId'].tolist()\n",
    "                low_factor_symbols = filtered_df[filtered_df[col] <= lower_boundary]['conId'].tolist()\n",
    "                if col.endswith('variety'):\n",
    "                    factor_series = construct_factor_series(filtered_df, pct_changes, low_factor_symbols, short_symbols=high_factor_symbols, factor_column=col)\n",
    "                elif col.startswith(tuple(only_long)):\n",
    "                    factor_series = construct_factor_series(filtered_df, pct_changes, high_factor_symbols, factor_column=col)\n",
    "                else:\n",
    "                    factor_series = construct_factor_series(filtered_df, pct_changes, high_factor_symbols, short_symbols=low_factor_symbols, factor_column=col)\n",
    "                factors[col] = factor_series\n",
    "\n",
    "            except Exception as e:\n",
    "                print(col)\n",
    "                print(e)\n",
    "                raise\n",
    "\n",
    "    return pd.DataFrame(factors)\n",
    "\n",
    "def prescreen_factors(factors_df, correlation_threshold=CORRELATION_THRESHOLD, drop_map=None):\n",
    "    if factors_df is None or factors_df.empty or factors_df.shape[1] == 0:\n",
    "        raise ValueError(\"factors_df must be a non-empty DataFrame with at least one column.\")\n",
    "    temp_factors_df = factors_df.copy()\n",
    "\n",
    "    corr_matrix = temp_factors_df.corr().abs()\n",
    "    corr_pairs = corr_matrix.where(np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)).stack()\n",
    "    corr_pairs = corr_pairs.sort_values(ascending=False)\n",
    "\n",
    "    if not drop_map:\n",
    "        drop_map = {}\n",
    "    col_order = list(temp_factors_df.columns)\n",
    "    for (col1, col2), corr_val in corr_pairs.items():\n",
    "        if corr_val < correlation_threshold:\n",
    "            break\n",
    "\n",
    "        already_dropped = {c for drops in drop_map.values() for c in drops}\n",
    "        if col1 in already_dropped or col2 in already_dropped:\n",
    "            continue\n",
    "\n",
    "        if col_order.index(col1) < col_order.index(col2):\n",
    "            keeper, to_drop = col1, col2\n",
    "        else:\n",
    "            keeper, to_drop = col2, col1\n",
    "\n",
    "        drop_map.setdefault(keeper, []).append(to_drop)\n",
    "\n",
    "    cols_to_drop = set(col for drops in drop_map.values() for col in drops)\n",
    "    temp_factors_df = temp_factors_df.drop(columns=cols_to_drop)\n",
    "    return temp_factors_df, drop_map\n",
    "\n",
    "def merge_drop_map(drop_map):\n",
    "    cols_to_drop = set(col for drops in drop_map.values() for col in drops)\n",
    "    final_drop_map = {}\n",
    "    for keeper, direct_drops in drop_map.items():\n",
    "        if keeper not in cols_to_drop:\n",
    "            cols_to_check = list(direct_drops) \n",
    "            all_related_drops = set(direct_drops)\n",
    "            while cols_to_check:\n",
    "                col = cols_to_check.pop(0)\n",
    "                if col in drop_map:\n",
    "                    new_drops = [d for d in drop_map[col] if d not in all_related_drops]\n",
    "                    cols_to_check.extend(new_drops)\n",
    "                    all_related_drops.update(new_drops)\n",
    "            \n",
    "            final_drop_map[keeper] = sorted(list(all_related_drops))\n",
    "    \n",
    "    return final_drop_map\n",
    "\n",
    "def run_regressions(distilled_factors):\n",
    "    results = []\n",
    "    for symbol in pct_changes.columns:\n",
    "        etf_excess = pct_changes[symbol] - risk_free_df['daily_nominal_rate']\n",
    "        data = pd.concat([etf_excess.rename('etf_excess'), distilled_factors], axis=1).dropna()\n",
    "\n",
    "        Y = data['etf_excess']\n",
    "        X = sm.add_constant(data.iloc[:, 1:])\n",
    "        model = sm.OLS(Y, X).fit()\n",
    "        result = {\n",
    "            'conId': symbol,\n",
    "            'nobs': model.nobs,\n",
    "            'r_squared': model.rsquared,\n",
    "            'r_squared_adj': model.rsquared_adj,\n",
    "            'f_statistic': model.fvalue,\n",
    "            'f_pvalue': model.f_pvalue,\n",
    "            'aic': model.aic,\n",
    "            'bic': model.bic,\n",
    "            'condition_number': model.condition_number,\n",
    "            'alpha': model.params['const'],\n",
    "            'alpha_pval': model.pvalues['const'],\n",
    "            'alpha_tval': model.tvalues['const'],\n",
    "            'alpha_bse': model.bse['const'],\n",
    "        }\n",
    "        for factor in distilled_factors.columns:\n",
    "            result[f'beta_{factor}'] = model.params[factor]\n",
    "            result[f'pval_beta_{factor}'] = model.pvalues[factor]\n",
    "            result[f'tval_beta_{factor}'] = model.tvalues[factor]\n",
    "            result[f'bse_beta_{factor}'] = model.bse[factor]\n",
    "        results.append(result)\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df\n",
    "\n",
    "def calculate_vif(df):\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"feature\"] = df.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]\n",
    "    return vif_data.sort_values(by='VIF', ascending=False)\n",
    "\n",
    "def run_elastic_net(\n",
    "                    factors_df,\n",
    "                    pct_changes,\n",
    "                    risk_free_df,\n",
    "                    training_cutoff,\n",
    "                    alphas=ENET_ALPHAS,\n",
    "                    l1_ratio=ENET_L1_RATIOS,\n",
    "                    cv=ENET_CV,\n",
    "                    tol=ENET_TOL,\n",
    "                    random_state=42):\n",
    "\n",
    "    data = data = (\n",
    "        factors_df.copy()\n",
    "        .join(pct_changes, how='inner')\n",
    "        .join(risk_free_df[['daily_nominal_rate']], how='inner')\n",
    "        .fillna(0)\n",
    "    )\n",
    "\n",
    "    train = data[data.index < training_cutoff]\n",
    "    test = data[data.index >= training_cutoff]\n",
    "\n",
    "    X_train = train[factors_df.columns].values\n",
    "    X_test = test[factors_df.columns].values\n",
    "    \n",
    "    metrics = []\n",
    "    for etf in tqdm(pct_changes.columns, total=len(pct_changes.columns), desc=\"Elastic Net Regression\"):\n",
    "        Y_train = train[etf].values - train['daily_nominal_rate'].values\n",
    "        Y_test = test[etf].values - test['daily_nominal_rate'].values\n",
    "\n",
    "        pipeline = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('enet', ElasticNetCV(alphas=alphas,\n",
    "                                l1_ratio=l1_ratio,\n",
    "                                cv=cv,\n",
    "                                random_state=random_state,\n",
    "                                max_iter=499999,\n",
    "                                tol=tol,\n",
    "                                fit_intercept=True,\n",
    "                                n_jobs=-1)),\n",
    "        ])\n",
    "\n",
    "        try:\n",
    "            pipeline.fit(X_train, Y_train)\n",
    "        except ValueError as e:\n",
    "            print(f\"Skipping {etf} due to error: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Unscale coefficients and intercept\n",
    "        enet = pipeline.named_steps['enet']\n",
    "        scaler = pipeline.named_steps['scaler']\n",
    "        betas_train = enet.coef_ / scaler.scale_\n",
    "        intercept = enet.intercept_ - np.dot(betas_train, scaler.mean_)\n",
    "\n",
    "        # out-of-sample stats\n",
    "        er_test = pipeline.predict(X_test)\n",
    "\n",
    "        # in-sample stats\n",
    "        er_train = pipeline.predict(X_train)\n",
    "\n",
    "        row = {\n",
    "            'conId': etf,\n",
    "            'jensens_alpha': intercept,\n",
    "            'enet_alpha': enet.alpha_,\n",
    "            'l1_ratio': enet.l1_ratio_,\n",
    "            'n_iter': enet.n_iter_,\n",
    "            'dual_gap': enet.dual_gap_,\n",
    "            'n_nonzero': np.sum(np.abs(betas_train) > 1e-6),\n",
    "            'cv_mse_best': np.min(enet.mse_path_.mean(axis=2)),\n",
    "            'cv_mse_average': np.mean(enet.mse_path_.mean(axis=2)),\n",
    "            'cv_mse_worst': np.max(enet.mse_path_.mean(axis=2)),\n",
    "            'mse_test' : mean_squared_error(Y_test, er_test),\n",
    "            'mse_train' : mean_squared_error(Y_train, er_train),\n",
    "            'r2_test' : r2_score(Y_test, er_test),\n",
    "            'r2_train' : r2_score(Y_train, er_train),\n",
    "        }\n",
    "\n",
    "        # Map back coefficients to factor names\n",
    "        for coef, fname in zip(betas_train, factors_df.columns):\n",
    "            row[f'{fname}_beta'] = coef\n",
    "\n",
    "        metrics.append(row)\n",
    "    \n",
    "    results_df = pd.DataFrame(metrics).set_index('conId')\n",
    "    return results_df\n",
    "\n",
    "def optimize_scalar(series):\n",
    "    def obj(s):\n",
    "        if s <= 0:\n",
    "            return np.inf\n",
    "        return skew(np.log1p(s * series))**2\n",
    "\n",
    "    result = minimize_scalar(obj, bounds=(1e-5, 1e20), method='bounded')\n",
    "    # print(result.x)\n",
    "    return result.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD FILES\n",
    "kind = 'trades'\n",
    "price_col = 'average'\n",
    "root = 'data/'\n",
    "data_path = root + 'daily-trades/series/'\n",
    "verified_path = root + 'daily-trades/verified_files.csv'\n",
    "verified_df = load(verified_path)\n",
    "\n",
    "scraped_path = root + f'preprocessed/justetf_{datetime.now().strftime('%Y-%m')}.csv'\n",
    "scraped_df = load(scraped_path)\n",
    "\n",
    "# Scraped just etf data\n",
    "# accumulating_isins = scraped_df[(scraped_df['distribution_policy'] == 'Accumulating') | (scraped_df['distribution_policy'].isna())]['isin']\n",
    "accumulating_isins = scraped_df[(scraped_df['distribution_policy'] == 'Accumulating')]['isin']\n",
    "# physical_isins = scraped_df[(scraped_df['replication'] == 'Physical') | (scraped_df['replication'].isna())]['isin']\n",
    "physical_isins = scraped_df[(scraped_df['replication'] == 'Physical')]['isin']\n",
    "\n",
    "# Scraped IBKR data\n",
    "fund_path = root + f'preprocessed/fundamentals_{datetime.now().strftime('%Y-%m')}.csv'\n",
    "fund_df = load(fund_path)\n",
    "fund_df['funds_date'] = pd.to_datetime(fund_df['funds_date'])\n",
    "fund_df = fund_df[fund_df['isin'].isin(physical_isins)]\n",
    "accumulating_ids = fund_df[fund_df['isin'].isin(accumulating_isins)]['conId']\n",
    "\n",
    "tradable = fund_df[fund_df['tradable'] == 1]['conId'].to_list()\n",
    "low_real_estate = fund_df[fund_df['industries_RealEstate'] < fund_df['industries_RealEstate'].mean()]['conId'].to_list()\n",
    "gold_conids = fund_df[(fund_df['longName'].str.contains('gold', case=False, na=False)) | (fund_df['longName'].str.contains('auag', case=False, na=False))]['conId'].to_list()\n",
    "\n",
    "fund_df = fund_df.drop(['tradable'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check industry correlation\n",
    "# import matplotlib.pyplot as plt \n",
    "# import seaborn as sns\n",
    "\n",
    "# from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "# from scipy.spatial.distance import squareform\n",
    "\n",
    "# industry_cols = [col for col in fund_df.columns if col.startswith('indust')]\n",
    "# numerical_cols = [col for col in fund_df.columns if fund_df[col].dtype in [np.int64, np.float64] and col not in ['conId'] and not col.startswith('count') and not col.startswith('style')]\n",
    "\n",
    "# ind_corr = fund_df[industry_cols].corr()\n",
    "# dist_matrix = 1 - ind_corr\n",
    "\n",
    "# condensed_dist = squareform(dist_matrix)\n",
    "# linkage_matrix = linkage(condensed_dist, method='ward')\n",
    "\n",
    "# # plt.figure(figsize=(13, 10)) \n",
    "# # sns.heatmap(ind_corr, annot=True, cmap='coolwarm', linewidths=0.5) \n",
    "# # plt.title('Correlation Heatmap')\n",
    "# # plt.show()\n",
    "\n",
    "# plt.figure(figsize=(18, 10))\n",
    "# plt.title('Hierarchical Clustering Dendrogram of IBKR Industries', fontsize=20)\n",
    "# plt.xlabel('Industry', fontsize=16)\n",
    "# plt.ylabel('Distance (1 - Correlation)', fontsize=16)\n",
    "\n",
    "\n",
    "# dn = dendrogram(\n",
    "#     linkage_matrix,\n",
    "#     labels=[col.replace('industries_', '') for col in dist_matrix.columns],\n",
    "#     leaf_rotation=90,\n",
    "# )\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load full historical price series\n",
    "if 'meta' not in globals() or input('Reload CSVs? (y/n)').lower().strip() == 'y':\n",
    "    last_date = (datetime.now() - timedelta(days=365 * 99))\n",
    "    first_date = (datetime.now())\n",
    "    meta = []\n",
    "    file_list = os.listdir(data_path)\n",
    "    for file in tqdm(file_list, total=len(file_list), desc=\"Loading files\"):\n",
    "        if not file.endswith('.csv'):\n",
    "            continue\n",
    "        parts = os.path.splitext(file)[0].split('-')\n",
    "        symbol, exchange, currency = parts[0], parts[1], parts[2]\n",
    "        if not ((verified_df['symbol'] == symbol) & (verified_df['currency'] == currency)).any():\n",
    "            continue\n",
    "        try:\n",
    "            df = load(data_path + file)\n",
    "            df = ensure_series_types(df, price_col)\n",
    "            df = validate_raw_prices(df, price_col)\n",
    "            df = handle_stale_periods(df, price_col)\n",
    "            df['pct_change'] = df[price_col].pct_change()\n",
    "\n",
    "            if df['date'].max() > last_date:\n",
    "                last_date = df['date'].max()\n",
    "            if df['date'].min() < first_date:\n",
    "                first_date = df['date'].min()\n",
    "            \n",
    "            meta.append({\n",
    "                'symbol': symbol,\n",
    "                'currency': currency,\n",
    "                'exchange_api': exchange,\n",
    "                'df': df[['date', price_col, 'volume', 'pct_change']],\n",
    "            })\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"ERROR loading {file}: {e}\")\n",
    "\n",
    "    meta = pd.DataFrame(meta)\n",
    "    detect_and_nullify_global_outliers(meta, price_col=price_col, z_threshold=Z_THRESHOLD_GLOBAL_LOOP, window=OUTLIER_WINDOW_DEFAULT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_indicator_map = {\n",
    "    'NE.IMP.GNFS.ZS': 'imports-goods+serv',\n",
    "    'NE.EXP.GNFS.ZS': 'exports-goods+serv',\n",
    "    'TM.VAL.MRCH.XD.WD': 'import-goods',\n",
    "    'TX.VAL.MRCH.XD.WD': 'export-goods',\n",
    "\n",
    "    'BX.KLT.DINV.WD.GD.ZS': 'foreign-direct-investment',\n",
    "\n",
    "    'NY.GDP.MKTP.CD': 'economic-output-gdp',\n",
    "    'SP.POP.TOTL': 'population',\n",
    "    'NY.GDP.PCAP.CD': 'gdp-pcap',\n",
    "    # 'NY.GDP.DEFL.KD.ZG': 'production_price_inflation',\n",
    "}\n",
    "\n",
    "'''\n",
    "'SH.TRD.VOL': 'share_trade_volume', #1\n",
    "'NY.GDP.DEFL.ZS': 'production_price_inflation',\n",
    "\n",
    "'SERVICES.GOODS.RATIO': 'services_goods_ratio' #1 #2\n",
    "'BX.KLT.DINV.WD.GD.ZS': 'foreign_direct_investment',\n",
    "\n",
    "'SP.POP.TOTL': 'population', #1 #2\n",
    "'NY.GDP.PCAP.CD': 'gdp-pcap',\n",
    "\n",
    "'NY.GDP.MKTP.CD': 'economic_output_gdp', #1 #2\n",
    "'TRADE.SURPLUS': 'trade_surplus',\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_world_bank_data(world_bank_data_full, wb_indicator_map):  \n",
    "    def regress_extrapolate(row, n=3):\n",
    "        y_valid = row.dropna()\n",
    "        x_valid = y_valid.index.astype(int)\n",
    "        if len(y_valid) < n:\n",
    "            return row\n",
    "        \n",
    "        nan_indices = row[row.isna()].index.astype(int)\n",
    "        if len(nan_indices) == 0:\n",
    "            return row\n",
    "        \n",
    "        future_nans = nan_indices[nan_indices > x_valid.max()]\n",
    "        past_nans = nan_indices[nan_indices < x_valid.min()]\n",
    "        if len(future_nans) > 0:\n",
    "            y_recent = y_valid.tail(n)\n",
    "            x_recent = y_recent.index.astype(int)\n",
    "            slope, intercept, _, _, _ = linregress(x_recent, y_recent)\n",
    "            row.loc[future_nans] = slope * future_nans + intercept\n",
    "        if len(past_nans) > 0:\n",
    "            y_early = y_valid.head(n)\n",
    "            x_early = y_early.index.astype(int)\n",
    "            slope, intercept, _, _, _ = linregress(x_early, y_early)\n",
    "            row.loc[past_nans] = slope * past_nans + intercept\n",
    "\n",
    "        return row\n",
    "\n",
    "    processed_data = world_bank_data_full.copy()\n",
    "    processed_data.columns = [int(col[2:]) for col in processed_data.columns]\n",
    "    data_unstacked = processed_data.unstack(level='series')\n",
    "    data_unstacked = data_unstacked.swaplevel(0, 1, axis=1)\n",
    "    data_unstacked.sort_index(axis=1, level=0, inplace=True)\n",
    "\n",
    "    base_import_col = 'NE.IMP.GNFS.ZS'\n",
    "    supp_import_col = 'TM.VAL.MRCH.XD.WD'\n",
    "    base_export_col = 'NE.EXP.GNFS.ZS'\n",
    "    supp_export_col = 'TX.VAL.MRCH.XD.WD'\n",
    "\n",
    "    gdp_col = 'NY.GDP.MKTP.CD'\n",
    "    pop_col = 'SP.POP.TOTL'\n",
    "    gdp_pcap_col = 'NY.GDP.PCAP.CD'\n",
    "\n",
    "    # new_surplus_col = 'TRADE.SURPLUS'\n",
    "    new_share_trade_col = 'SH.TRD.VOL'\n",
    "    # services2goods_col = 'SERVICES.GOODS.RATIO'\n",
    "\n",
    "    # # Fill NaN values in import\n",
    "    # services2goods_multiple = (data_unstacked[base_import_col] / data_unstacked[supp_import_col])\n",
    "    # services2goods_multiple = services2goods_multiple.interpolate(method='akima', axis=1)\n",
    "    # services2goods_multiple = services2goods_multiple.apply(lambda row: regress_extrapolate(row, n=5), axis=1)\n",
    "    # services2goods_multiple = services2goods_multiple.fillna(services2goods_multiple.median())\n",
    "    # import_fill = data_unstacked[supp_import_col].multiply(services2goods_multiple, axis=0)\n",
    "    # data_unstacked[base_import_col] = data_unstacked[base_import_col].fillna(import_fill)\n",
    "    \n",
    "    # # Fill NaN values in export\n",
    "    # services2goods_multiple = (data_unstacked[base_export_col] / data_unstacked[supp_export_col])\n",
    "    # services2goods_multiple = services2goods_multiple.interpolate(method='akima', axis=1)\n",
    "    # services2goods_multiple = services2goods_multiple.apply(lambda row: regress_extrapolate(row, n=5), axis=1)\n",
    "    # services2goods_multiple = services2goods_multiple.fillna(services2goods_multiple.median())\n",
    "    # export_fill = data_unstacked[supp_export_col].multiply(services2goods_multiple, axis=0)\n",
    "    # data_unstacked[base_export_col] = data_unstacked[base_export_col].fillna(export_fill)\n",
    "\n",
    "    # # Create a goods to serv ratio col\n",
    "    # services2goods_multiple.columns = pd.MultiIndex.from_product([[services2goods_col], services2goods_multiple.columns])\n",
    "    # data_unstacked = pd.concat([data_unstacked, services2goods_multiple], axis=1)\n",
    "\n",
    "    # # Recalculate surplus with the now-amended import/export data.\n",
    "    # trade_surplus = data_unstacked[base_export_col] - data_unstacked[base_import_col]\n",
    "    # trade_surplus.columns = pd.MultiIndex.from_product([[new_surplus_col], trade_surplus.columns])\n",
    "    # data_unstacked = pd.concat([data_unstacked, trade_surplus], axis=1)\n",
    "\n",
    "    # Determine each country's contribution to total global trade.\n",
    "    total_trade = data_unstacked[base_export_col] + data_unstacked[base_import_col]\n",
    "    global_trade_by_year = total_trade.sum(axis=0)\n",
    "    share_of_global_trade = total_trade.div(global_trade_by_year, axis=1)\n",
    "    share_of_global_trade.columns = pd.MultiIndex.from_product([[new_share_trade_col], share_of_global_trade.columns])\n",
    "    data_unstacked = pd.concat([data_unstacked, share_of_global_trade], axis=1)\n",
    "\n",
    "    # Verify GDP/capita\n",
    "    population = data_unstacked[pop_col]\n",
    "    calculated_gdp_pcap = data_unstacked[gdp_col].div(population.where(population != 0))\n",
    "    data_unstacked[gdp_pcap_col] = data_unstacked[gdp_pcap_col].fillna(calculated_gdp_pcap)\n",
    "\n",
    "    # Adjust as a share of global gdp\n",
    "    gdp = data_unstacked[gdp_col]\n",
    "    global_gdp_by_year = gdp.sum(axis=0)\n",
    "    share_of_global_gdp = gdp.div(global_gdp_by_year, axis=1)\n",
    "    data_unstacked[gdp_col] = share_of_global_gdp\n",
    "\n",
    "    wb_indicator_map_post = wb_indicator_map.copy()\n",
    "\n",
    "    to_drop = [supp_import_col, supp_export_col, base_import_col, base_export_col]\n",
    "    for col in to_drop:\n",
    "        del wb_indicator_map_post[col]\n",
    "        data_unstacked.drop(columns=[col], level=0, inplace=True, errors='ignore')\n",
    "\n",
    "    wb_indicator_map_post = {**wb_indicator_map_post,\n",
    "                        # **{new_surplus_col: 'trade_surplus',\n",
    "                            **{new_share_trade_col: 'share_trade_volume',\n",
    "                            # services2goods_col: 'services_goods_ratio',\n",
    "                        }}\n",
    "\n",
    "    # Interpolate and Extrapolate\n",
    "    for col in wb_indicator_map_post:\n",
    "        data_unstacked[col] = data_unstacked[col].interpolate(method='akima', axis=1)\n",
    "        data_unstacked[col] = data_unstacked[col].apply(lambda row: regress_extrapolate(row, n=3), axis=1)\n",
    "        data_unstacked[col] = data_unstacked[col].fillna(data_unstacked[col].mean())\n",
    "\n",
    "    stacked = data_unstacked.stack(level=0, future_stack=True)\n",
    "    stacked.index = stacked.index.set_names(['economy', 'series'])\n",
    "    return stacked, wb_indicator_map_post\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download supplementary data\n",
    "if 'risk_free_df_full' not in globals() or input('Redownload supplementary data? (y/n)').lower().strip() == 'y':\n",
    "    # Risk-free series calculation\n",
    "    tickers = {\n",
    "        'US': 'DTB3',\n",
    "        'Canada': 'IR3TIB01CAM156N',\n",
    "        'Germany': 'IR3TIB01DEM156N',\n",
    "        'UK': 'IR3TIB01GBM156N',\n",
    "        'France': 'IR3TIB01FRA156N',\n",
    "    }\n",
    "    bonds = {}\n",
    "    failed = []\n",
    "    for country, ticker in tickers.items():\n",
    "        try:\n",
    "            series = web.DataReader(ticker, 'fred', first_date, last_date)\n",
    "            bonds[country] = series / 100.0\n",
    "        except Exception:\n",
    "            try:\n",
    "                series = web.DataReader(ticker, 'oecd', first_date, last_date)\n",
    "                bonds[country] = series / 100.0\n",
    "            except Exception as oecd_err:\n",
    "                failed.append(country)\n",
    "\n",
    "    # Combine into a single DataFrame\n",
    "    df_bonds = pd.concat(bonds, axis=1)\n",
    "    df_bonds.columns = [c for c in tickers if c not in failed]\n",
    "    df_bonds = df_bonds.interpolate(method='akima').bfill().ffill()\n",
    "\n",
    "    risk_free_df_full = df_bonds.mean(axis=1).rename('nominal_rate')\n",
    "    business_days = pd.date_range(start=first_date, end=last_date, freq='B')\n",
    "    risk_free_df_full = risk_free_df_full.reindex(business_days, copy=False)\n",
    "\n",
    "    risk_free_df_full = pd.DataFrame(risk_free_df_full)\n",
    "    risk_free_df_full['daily_nominal_rate'] = risk_free_df_full['nominal_rate'] / 252\n",
    "\n",
    "    # Get country stats\n",
    "    cc = coco.CountryConverter()\n",
    "\n",
    "    all_country_cols = [col for col in fund_df.columns if col.startswith('countries') and not col.endswith('variety')]\n",
    "    standard_names = {}\n",
    "    for col in all_country_cols:\n",
    "        raw_name = col.replace('countries_', '').replace(' ', '')\n",
    "        standard_name = cc.convert(names=raw_name, to='ISO3', not_found=None)\n",
    "        if standard_name:\n",
    "            standard_names[raw_name] = standard_name\n",
    "\n",
    "    start = max(first_date, datetime(2000, 1, 1))\n",
    "    raw_wb_data = fetch_world_bank_data(standard_names.values(), first_date, last_date, wb_indicator_map.keys())\n",
    "    world_bank_data_full, wb_indicator_map_post = preprocess_world_bank_data(raw_wb_data, wb_indicator_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YEAR_RANGE = 3\n",
    "training_oldest = last_date - timedelta(days=365 * YEAR_RANGE)\n",
    "\n",
    "meta_window = meta.copy()\n",
    "meta_window['df'] = meta['df'].apply(lambda df: df.loc[df['date'].between(training_oldest, last_date)].copy())\n",
    "business_days = pd.date_range(start=training_oldest, end=last_date, freq='B')\n",
    "\n",
    "for idx, row in meta_window.iterrows():\n",
    "    df = row['df']\n",
    "    merged = pd.DataFrame({'date': business_days}).merge(df, on='date', how='left')\n",
    "    present = merged[price_col].notna()\n",
    "    present_idx = np.flatnonzero(present)\n",
    "    gaps = []\n",
    "    length = len(merged)\n",
    "    if present_idx.size > 0:\n",
    "        if present_idx[0] > 0:\n",
    "            gaps.append(present_idx[0])\n",
    "        if present_idx.size > 1:\n",
    "            internal_gaps = np.diff(present_idx) - 1\n",
    "            gaps.extend(gap for gap in internal_gaps if gap > 0)\n",
    "        if present_idx[-1] < length - 1:\n",
    "            gaps.append(length - 1 - present_idx[-1])\n",
    "    else:\n",
    "        gaps = [length]\n",
    "    gaps = np.array(gaps, dtype=int)\n",
    "    gaps = gaps[gaps > 0]\n",
    "    max_gap = float(gaps.max()) if gaps.size > 0 else 0.0\n",
    "    std_gap = float(gaps.std()) if gaps.size > 0 else 0.0\n",
    "    missing = length - present.sum()\n",
    "    pct_missing = missing / length\n",
    "    meta_window.at[idx, 'df'] = merged\n",
    "    meta_window.at[idx, 'max_gap'] = max_gap\n",
    "    meta_window.at[idx, 'missing'] = missing\n",
    "    meta_window.at[idx, 'pct_missing'] = pct_missing\n",
    "meta_window['max_gap_log'] = np.log1p(meta_window['max_gap'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows according to window gap stats\n",
    "condition = ((meta_window['max_gap_log'] < MAX_GAP_LOG) & (meta_window['pct_missing'] < MAX_PCT_MISSING))\n",
    "filtered = meta_window[condition].copy()\n",
    "print(f'{len(filtered)} ETFs included')\n",
    "print(f'{len(meta_window) - len(filtered)} dropped')\n",
    "del meta_window\n",
    "\n",
    "for idx, row in filtered.iterrows():\n",
    "    df = row['df']\n",
    "    df[price_col] = df[price_col].interpolate(method='akima', limit_direction='both')\n",
    "    if df[price_col].isna().any():\n",
    "        df[price_col] = df[price_col].ffill()\n",
    "        df[price_col] = df[price_col].bfill()\n",
    "    df['pct_change'] = df[price_col].pct_change()\n",
    "    filtered.at[idx, 'df'] = df.set_index('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate to one fundamental row per conId + remove uninformative cols\n",
    "training_cutoff = last_date - pd.Timedelta(days=TRAINING_PERIOD_DAYS)\n",
    "\n",
    "before_training_end = fund_df[fund_df['funds_date'] <= training_cutoff]\n",
    "if not before_training_end.empty:\n",
    "    before_training_end = before_training_end.loc[before_training_end.groupby('conId')['funds_date'].idxmax()]\n",
    "else:\n",
    "    before_training_end = pd.DataFrame(columns=fund_df.columns)\n",
    "\n",
    "after_training_end = fund_df[fund_df['funds_date'] > training_cutoff]\n",
    "if not after_training_end.empty:\n",
    "    after_training_end = after_training_end.loc[after_training_end.groupby('conId')['funds_date'].idxmin()]\n",
    "else:\n",
    "    after_training_end = pd.DataFrame(columns=fund_df.columns)\n",
    "\n",
    "\n",
    "if not before_training_end.empty and not after_training_end.empty:\n",
    "    after_training_end = after_training_end[~after_training_end['conId'].isin(before_training_end['conId'])]\n",
    "spliced_fund_df = pd.concat([before_training_end, after_training_end])\n",
    "\n",
    "filtered = pd.merge(filtered, spliced_fund_df, on=['symbol', 'currency'], how='inner').drop(['max_gap', 'missing', 'pct_missing', 'max_gap_log'], axis=1)\n",
    "numerical_cols = [col for col in filtered.columns if filtered[col].dtype in [np.int64, np.float64] and col not in ['conId']]\n",
    "pct_changes = pd.concat(\n",
    "        [row['df']['pct_change'].rename(row['conId']) \n",
    "        for _, row in filtered.iterrows()], axis=1\n",
    "    )\n",
    "\n",
    "# Remove uninformative cols for market portfolios \n",
    "uninformative_cols = [col for col in numerical_cols if filtered[col].nunique(dropna=True) <= 1]\n",
    "filtered = filtered.drop(columns=uninformative_cols)\n",
    "filtered = filtered.dropna(axis=1, how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add rate of change fundamentals\n",
    "rate_fundamentals = [('EPSGrowth-1yr', 'EPSGrowth3yr', 'EPSGrowth5yr'),\n",
    "                    ('ReturnonAssets1Yr', 'ReturnonAssets3Yr'),\n",
    "                    ('ReturnonCapital', 'ReturnonCapital3Yr'),\n",
    "                    ('ReturnonEquity1Yr', 'ReturnonEquity3Yr'),\n",
    "                    ('ReturnonInvestment1Yr', 'ReturnonInvestment3Yr')]\n",
    "\n",
    "for cols in rate_fundamentals:\n",
    "    base_name = cols[0].replace('-1yr', '').replace('1Yr', '')\n",
    "    slope_col = f'fundamentals_{base_name}_slope'\n",
    "    if len(cols) == 3:\n",
    "        col_1yr, col_3yr, col_5yr = cols\n",
    "        filtered[slope_col] = calculate_slope(filtered[f'fundamentals_{col_1yr}'], filtered[f'fundamentals_{col_5yr}'], 1, 5)\n",
    "        slope_1yr_3yr = calculate_slope(filtered[f'fundamentals_{col_1yr}'], filtered[f'fundamentals_{col_3yr}'], 1, 3)\n",
    "        slope_3yr_5yr = calculate_slope(filtered[f'fundamentals_{col_3yr}'], filtered[f'fundamentals_{col_5yr}'], 3, 5)\n",
    "        filtered[f'fundamentals_{base_name}_second_deriv'] = calculate_slope(slope_1yr_3yr, slope_3yr_5yr, 1, 3)\n",
    "    elif len(cols) == 2:\n",
    "        col_1yr, col_3yr = cols\n",
    "        filtered[slope_col] = calculate_slope(filtered[f'fundamentals_{col_1yr}'], filtered[f'fundamentals_{col_3yr}'], 1, 3)\n",
    "numerical_cols = [col for col in filtered.columns if filtered[col].dtype in [np.int64, np.float64] and col not in ['conId']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return stats and split training and tests sets\n",
    "momentum_cutoffs = {\n",
    "    '1y':  training_cutoff - pd.Timedelta(days=MOMENTUM_PERIODS_DAYS['1y']),\n",
    "    '6mo': training_cutoff - pd.Timedelta(days=MOMENTUM_PERIODS_DAYS['6mo']),\n",
    "    '3mo': training_cutoff - pd.Timedelta(days=MOMENTUM_PERIODS_DAYS['3mo']),\n",
    "}\n",
    "risk_free_df = risk_free_df_full.loc[business_days]\n",
    "return_stat_cols = ['momentum_3mo', 'momentum_6mo', 'momentum_1y', 'rs_3mo', 'rs_6mo', 'rs_1y']\n",
    "filtered[return_stat_cols] = filtered['df'].apply(lambda df: get_return_stats(df, training_cutoff, momentum_cutoffs, risk_free_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create holding type portfolios\n",
    "holding_cols = [col for col in filtered.columns if col.startswith('holding_types') and col != 'holding_types_variety'] + ['total']\n",
    "portfolio_dfs = {}\n",
    "\n",
    "for holding_col in holding_cols:\n",
    "    name = holding_col.split('_')[-1]\n",
    "    if holding_col == 'total':\n",
    "        weight = filtered['profile_cap_usd']\n",
    "    else:\n",
    "        weight = (filtered['profile_cap_usd'] * filtered[holding_col])\n",
    "\n",
    "    total_market_cap = (weight).sum()\n",
    "    filtered['weight'] = weight / total_market_cap\n",
    "    \n",
    "    weights = filtered.set_index('conId')['weight']\n",
    "    portfolio_return = pct_changes.dot(weights)\n",
    "    initial_price = 1\n",
    "    portfolio_price = initial_price * (1 + portfolio_return.fillna(0)).cumprod()\n",
    "\n",
    "    portfolio_df = pd.DataFrame({\n",
    "        'date': portfolio_price.index,\n",
    "        price_col: portfolio_price.values,\n",
    "        'pct_change': portfolio_return.values\n",
    "    }).set_index('date')\n",
    "\n",
    "    portfolio_dfs[name] = portfolio_df\n",
    "\n",
    "filtered.drop('weight', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoid dummy trap\n",
    "empty_subcategories = {\n",
    "'holding_types': ['other'],\n",
    "'countries': ['Unidentified'], \n",
    "'currencies': ['<NoCurrency>'],\n",
    "'industries': ['NonClassifiedEquity', 'NotClassified-NonEquity'],\n",
    "'top10': ['OtherAssets', 'AccountsPayable','AccountsReceivable','AccountsReceivable&Pay','AdministrationFees','CustodyFees','ManagementFees','OtherAssetsandLiabilities','OtherAssetslessLiabilities', 'OtherFees','OtherLiabilities','Tax','Tax--ManagementFees'],\n",
    "'debtors': ['OTHER'],\n",
    "'maturity': ['%MaturityOther'],\n",
    "'debt_type': ['%QualityNotAvailable', '%QualityNotRated'],\n",
    "'manual': ['asset_other']\n",
    "}\n",
    "\n",
    "dummy_trap_cols = []\n",
    "for k, lst in empty_subcategories.items():\n",
    "    for i in lst:\n",
    "        if k == 'manual':\n",
    "            dummy_trap_cols.append(i)\n",
    "        else:\n",
    "            dummy_trap_cols.append(f'{k}_{i}')\n",
    "    \n",
    "filtered = filtered.drop(columns=dummy_trap_cols, axis=1, errors='ignore')\n",
    "numerical_cols = [col for col in filtered.columns if filtered[col].dtype in [np.int64, np.float64] and col not in ['conId']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select asset types to work on\n",
    "asset_types = [col for col in filtered.columns if col.startswith('asset_')]\n",
    "asset_conditions = {\n",
    "    asset.replace('asset_', ''): (filtered[asset] == 1)\n",
    "    for asset in asset_types\n",
    "}\n",
    "asset_conditions['other'] = ~pd.concat(asset_conditions.values(), axis=1).any(axis=1)\n",
    "\n",
    "exclude_assets = ['bond', 'cash']\n",
    "include_assets = [asset for asset in asset_conditions.keys() if asset not in exclude_assets]\n",
    "combined_condition = pd.Series(False, index=filtered.index)\n",
    "for asset in include_assets:\n",
    "    combined_condition |= asset_conditions[asset]\n",
    "\n",
    "filtered_df = filtered[combined_condition]\n",
    "cols_to_drop_ending_with_exclude = [\n",
    "    col for col in filtered_df.columns\n",
    "    if any(col.endswith(ea) for ea in exclude_assets)\n",
    "]\n",
    "filtered_df = filtered_df.drop(columns=cols_to_drop_ending_with_exclude)\n",
    "\n",
    "numerical_cols = [col for col in filtered_df.columns if filtered_df[col].dtype in [np.int64, np.float64] and col not in ['conId']]\n",
    "\n",
    "single_value_columns = [col for col in filtered_df.columns if col in numerical_cols and filtered_df[col].nunique() == 1]\n",
    "asset_cols = [col for col in filtered_df if col.startswith('asset')]\n",
    "filtered_df = filtered_df.drop(columns=single_value_columns + asset_cols)\n",
    "numerical_cols = [col for col in filtered_df.columns if filtered_df[col].dtype in [np.int64, np.float64] and col not in ['conId']]\n",
    "\n",
    "pct_changes = pct_changes[filtered_df['conId']]\n",
    "# del filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collapse country columns\n",
    "metric_df = calculate_country_stats(world_bank_data_full, standard_names, last_date, window_size=3)\n",
    "metric_suffixes = {\n",
    "    'raw_value': '_stat',\n",
    "    '1st_div': '_growth_rate',\n",
    "    # '2nd_div': '_acceleration'\n",
    "}\n",
    "for ind_code, ind_name in wb_indicator_map_post.items():\n",
    "    if ind_code in metric_df.columns.get_level_values(0):\n",
    "        for metric_col, suffix in metric_suffixes.items():\n",
    "            new_col_name = f'{ind_name}{suffix}'\n",
    "            filtered_df[new_col_name] = 0.0\n",
    "\n",
    "for std_name, iso_code in standard_names.items():\n",
    "    country_weight_col = f'countries_{std_name}'\n",
    "    if country_weight_col not in filtered_df.columns:\n",
    "        continue\n",
    "\n",
    "    if iso_code in metric_df.index:\n",
    "        for ind_code, ind_name in wb_indicator_map_post.items():\n",
    "            if ind_code in metric_df.columns.get_level_values(0):\n",
    "                for metric_col, suffix in metric_suffixes.items():\n",
    "                    value = metric_df.loc[iso_code, (ind_code, metric_col)]                    \n",
    "                    if pd.isna(value):\n",
    "                        value = 0.0\n",
    "\n",
    "                    target_col = f'{ind_name}{suffix}'\n",
    "                    filtered_df[target_col] += filtered_df[country_weight_col] * value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop single unique value columns\n",
    "numerical_cols = [col for col in filtered_df.columns if filtered_df[col].dtype in [np.int64, np.float64] and col not in ['conId']]\n",
    "single_value_columns = [col for col in numerical_cols if filtered_df[col].nunique() == 1]\n",
    "filtered_df = filtered_df.drop(columns=single_value_columns, errors='ignore')\n",
    "numerical_cols = [col for col in filtered_df.columns if filtered_df[col].dtype in [np.int64, np.float64] and col not in ['conId']]\n",
    "single_value_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collapse fundamental columns\n",
    "fundamental_columns = [col for col in filtered_df.columns if col.startswith('fundamentals')]\n",
    "\n",
    "value_columns_inverted = [\n",
    "    'fundamentals_Price/Book',\n",
    "    'fundamentals_Price/Cash',\n",
    "    'fundamentals_Price/Earnings',\n",
    "    'fundamentals_Price/Sales',\n",
    "]\n",
    "leverage_columns_inverted = [\n",
    "    'fundamentals_LTDebt/Shareholders',\n",
    "    'fundamentals_TotalDebt/TotalCapital',\n",
    "    'fundamentals_TotalDebt/TotalEquity',\n",
    "    'fundamentals_TotalAssets/TotalEquity',\n",
    "]\n",
    "profitability_columns = [\n",
    "    'fundamentals_ReturnonAssets1Yr',\n",
    "    'fundamentals_ReturnonAssets3Yr',\n",
    "    'fundamentals_ReturnonCapital',\n",
    "    'fundamentals_ReturnonCapital3Yr',\n",
    "    'fundamentals_ReturnonEquity1Yr',\n",
    "    'fundamentals_ReturnonEquity3Yr',\n",
    "    'fundamentals_ReturnonInvestment1Yr',\n",
    "    'fundamentals_ReturnonInvestment3Yr',\n",
    "]\n",
    "\n",
    "columns_to_scale = value_columns_inverted + leverage_columns_inverted + profitability_columns + return_stat_cols\n",
    "if any(x in filtered_df.columns for x in columns_to_scale):\n",
    "    scaler = MinMaxScaler()\n",
    "    filtered_df[columns_to_scale] = scaler.fit_transform(filtered_df[columns_to_scale])\n",
    "\n",
    "    filtered_df['factor_value'] = (1 - filtered_df[value_columns_inverted]).sum(axis=1)\n",
    "    filtered_df['factor_leverage'] = (1 - filtered_df[leverage_columns_inverted]).sum(axis=1)\n",
    "    filtered_df['factor_profitability'] = filtered_df[profitability_columns].sum(axis=1)\n",
    "    filtered_df['factor_momentum_relative_strength'] = filtered_df[return_stat_cols].sum(axis=1)\n",
    "\n",
    "    filtered_df = filtered_df.drop(columns=columns_to_scale, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collapse industry columns\n",
    "defensive = ['industries_ConsumerNon-Cyclicals', 'industries_Utilities', 'industries_Healthcare', 'industries_TelecommunicationServices', 'industries_Academic&EducationalServices']\n",
    "cyclical = ['industries_Technology', 'industries_ConsumerCyclicals', 'industries_Industrials', 'industries_Financials', 'industries_RealEstate']\n",
    "commodities = ['industries_BasicMaterials', 'industries_Energy']\n",
    "\n",
    "# defensive = ['industries_ConsumerNon-Cyclicals', 'industries_Financials', 'industries_TelecommunicationServices', 'industries_Healthcare']\n",
    "# cyclical = ['industries_Technology', 'industries_Academic&EducationalServices', 'industries_ConsumerCyclicals']\n",
    "# commodities = ['industries_Energy', 'industries_Utilities', 'industries_Industrials', 'industries_BasicMaterials', 'industries_RealEstate']\n",
    "\n",
    "defensive_cols = [col for col in defensive if col in filtered_df.columns]\n",
    "cyclical_cols = [col for col in cyclical if col in filtered_df.columns]\n",
    "commodities_cols = [col for col in commodities if col in filtered_df.columns]\n",
    "\n",
    "filtered_df['supersector_defensive'] = filtered_df[defensive_cols].sum(axis=1)\n",
    "filtered_df['supersector_cyclical'] = filtered_df[cyclical_cols].sum(axis=1)\n",
    "filtered_df['supersector_commodities'] = filtered_df[commodities_cols].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorganize columns\n",
    "categories = ['share', 'factor', 'holding_types', 'stats', 'momentum', 'profile', 'top10', 'population', 'msci', 'continent', 'countries', 'fundamentals', 'industries', 'supersector', 'currencies', 'debtors', 'maturity', 'debt_type', 'lipper', 'dividends', 'marketcap', 'style', 'domicile', 'asset']\n",
    "\n",
    "numerical_cols = [col for col in filtered_df.columns if filtered_df[col].dtype in [np.int64, np.float64] and col not in ['conId']]\n",
    "non_numerical = [col for col in filtered_df.columns if col not in numerical_cols]\n",
    "\n",
    "sorted_numerical_cols = []\n",
    "remaining_numerical = numerical_cols.copy()\n",
    "\n",
    "for category in categories:\n",
    "    cat_cols = [col for col in remaining_numerical if col.startswith(category)]\n",
    "    \n",
    "    if cat_cols:\n",
    "        col_uniques = {col: filtered_df[col].nunique() for col in cat_cols}\n",
    "        sorted_cat_cols = sorted(col_uniques, key=col_uniques.get, reverse=True)\n",
    "        sorted_numerical_cols.extend(sorted_cat_cols)\n",
    "        remaining_numerical = [col for col in remaining_numerical if col not in cat_cols]\n",
    "\n",
    "sorted_numerical_cols.extend(remaining_numerical)\n",
    "    \n",
    "new_column_order = non_numerical + sorted_numerical_cols\n",
    "filtered_df = filtered_df[new_column_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct factors\n",
    "factors_df = construct_factors(filtered_df, pct_changes, portfolio_dfs, risk_free_df, scaling_factor=FACTOR_SCALING_FACTOR)\n",
    "\n",
    "# custom_drop = [\n",
    "#     'gdp-pcap_stat_beta', \n",
    "#     'gdp-pcap_growth_rate_beta',\n",
    "#     'profile_cap_usd',\n",
    "#     # 'foreign-direct-investment_growth_rate_beta',\n",
    "#     # 'foreign-direct-investment_stat_beta',\n",
    "#     'supersector_cyclical_beta', \n",
    "#     'supersector_defensive',\n",
    "#     'supersector_commodities_beta',\n",
    "#     'population_stat',\n",
    "#     'population_growth_rate_beta',\n",
    "#     'economic-output-gdp_growth_rate_beta',\n",
    "#     'share_trade_volume_growth_rate_beta',\n",
    "#     'factor_momentum_relative_strength_beta',\n",
    "#     'factor_profitability_beta',\n",
    "#     'factor_hml',\n",
    "#     'factor_value'\n",
    "#     ]\n",
    "\n",
    "# custom_drop = [c.split('_beta')[0] for c in custom_drop]\n",
    "# factors_df = factors_df.drop(columns=custom_drop, errors='ignore')\n",
    "# distilled_factors, drop_map = prescreen_factors(factors_df, correlation_threshold=CORRELATION_THRESHOLD)\n",
    "# drop_map = merge_drop_map(drop_map)\n",
    "# if drop_map:\n",
    "#     display(pd.Series(drop_map))\n",
    "\n",
    "\n",
    "# MANUAL FACTORS\n",
    "# distilled_factors = factors_df[['factor_market_premium', 'factor_smb', 'share_trade_volume_stat','population_stat', 'supersector_defensive']].copy()\n",
    "# distilled_factors = factors_df[['factor_market_premium', 'factor_smb', 'share_trade_volume_stat', 'factor_leverage', 'population_stat', 'population_growth_rate']].copy()\n",
    "# distilled_factors = factors_df[['factor_market_premium', 'factor_smb', 'share_trade_volume_stat', 'factor_leverage', 'population_growth_rate', 'foreign-direct-investment_stat']].copy()\n",
    "# distilled_factors = factors_df[['factor_market_premium', 'factor_smb', 'share_trade_volume_stat', 'factor_leverage', 'foreign-direct-investment_stat', 'foreign-direct-investment_growth_rate']].copy()\n",
    "\n",
    "distilled_factors = factors_df[['factor_market_premium', 'factor_smb', 'share_trade_volume_stat', 'factor_leverage', 'foreign-direct-investment_stat']].copy()\n",
    "\n",
    "display(calculate_vif(distilled_factors.dropna(axis=0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ElasticNet regression\n",
    "results_df = run_elastic_net(\n",
    "    factors_df=distilled_factors,\n",
    "    pct_changes=pct_changes,\n",
    "    risk_free_df=risk_free_df,\n",
    "    training_cutoff=training_cutoff,\n",
    "    alphas=ENET_ALPHAS,\n",
    "    l1_ratio=ENET_L1_RATIOS,\n",
    "    cv=ENET_CV,\n",
    "    tol=ENET_TOL,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-regression factor screening\n",
    "from scipy.stats import skew\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "beta_cols = [col for col in results_df if col.endswith('beta')]\n",
    "screening_df = pd.DataFrame(index=results_df.index)\n",
    "\n",
    "screening_df['r2_adj_test'] = results_df['r2_test'].max() - results_df['r2_test']\n",
    "screening_df['r2_adj_test'] = np.log1p(screening_df['r2_adj_test'] * optimize_scalar(screening_df['r2_adj_test']))\n",
    "screening_df['r2_adj_test'] = screening_df['r2_adj_test'].max() - screening_df['r2_adj_test']\n",
    "\n",
    "screening_df['cv_mse_std'] = results_df[['cv_mse_best','cv_mse_average','cv_mse_worst']].std(axis=1)\n",
    "screening_df['cv_mse_std'] = np.log1p(screening_df['cv_mse_std'] * optimize_scalar(screening_df['cv_mse_std']))\n",
    "\n",
    "screening_df['screening_score'] = screening_df['r2_adj_test'] / (1 + screening_df['cv_mse_std'])\n",
    "\n",
    "\n",
    "display_df = pd.DataFrame({\n",
    "    'mean_beta':  results_df[beta_cols].abs().mean(),\n",
    "    'mean_adj_beta': results_df[beta_cols].abs().multiply(screening_df['screening_score'], axis=0).mean(),\n",
    "    'non_zero_percentage': (results_df[beta_cols].abs() > 1e-6).sum() / len(results_df),\n",
    "})\n",
    "display_df.sort_values(by='non_zero_percentage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factor-based ER\n",
    "results_df = results_df[results_df.index.isin(tradable)]\n",
    "results_df = results_df[results_df.index.isin(low_real_estate)]\n",
    "results_df = results_df[results_df.index.isin(accumulating_ids)]\n",
    "# results_df = results_df[~results_df.index.isin(gold_conids)]\n",
    "\n",
    "beta_cols = [col for col in results_df.columns if col.endswith('beta')]\n",
    "asset_betas = results_df[beta_cols]\n",
    "asset_betas.columns = [col.replace('_beta', '') for col in beta_cols]\n",
    "asset_betas = asset_betas[distilled_factors.columns]\n",
    "\n",
    "factor_premia = distilled_factors.mean()\n",
    "# factor_premia[factor_premia > 0] = 0\n",
    "# factor_premia *= -1\n",
    "\n",
    "systematic_returns = asset_betas.dot(factor_premia)\n",
    "factor_based_er = results_df['jensens_alpha'] + systematic_returns\n",
    "\n",
    "screening_df = pd.DataFrame(index=results_df.index)\n",
    "screening_df['expected_return'] = factor_based_er\n",
    "screening_df['expected_return'] -= screening_df['expected_return'].min()\n",
    "\n",
    "screening_df['r2_test'] = results_df['r2_test'].max() - results_df['r2_test']\n",
    "screening_df['r2_test'] = np.log1p(screening_df['r2_test'] * optimize_scalar(screening_df['r2_test']))\n",
    "screening_df['r2_test'] = screening_df['r2_test'].max() - screening_df['r2_test']\n",
    "\n",
    "screening_df['cv_mse_std'] = results_df[['cv_mse_best','cv_mse_average','cv_mse_worst']].std(axis=1)\n",
    "screening_df['cv_mse_std'] = np.log1p(screening_df['cv_mse_std'] * optimize_scalar(screening_df['cv_mse_std']))\n",
    "screening_df['cv_mse_std'] = screening_df['cv_mse_std'].max() - screening_df['cv_mse_std']\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "screening_df[['r2_test', 'cv_mse_std']] = scaler.fit_transform(screening_df[['r2_test', 'cv_mse_std']])\n",
    "screening_df['r2_adjusted_er'] = screening_df['expected_return'] * screening_df['r2_test'] * screening_df['cv_mse_std']\n",
    "screening_df['historical_er'] = pct_changes.mean()\n",
    "\n",
    "mu_utility = screening_df['r2_adjusted_er'] \n",
    "mu_historical = pct_changes.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factor-based COV\n",
    "factor_cov_matrix = distilled_factors.cov()\n",
    "idiosyncratic_variances = results_df['mse_train']\n",
    "D = np.diag(results_df['mse_train'])\n",
    "\n",
    "systematic_cov = asset_betas.values @ factor_cov_matrix.values @ asset_betas.values.T\n",
    "S = pd.DataFrame(\n",
    "    systematic_cov + D,\n",
    "    index=results_df.index,\n",
    "    columns=results_df.index\n",
    ")\n",
    "\n",
    "# Static Risk-free rate \n",
    "rf_rate = risk_free_df['daily_nominal_rate'].iloc[-10:].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization functions\n",
    "import cvxpy as cp\n",
    "from pypfopt import base_optimizer\n",
    "\n",
    "def portfolio_factor_dispersion(portfolio_betas, n_factors):\n",
    "    mean_beta = cp.sum(portfolio_betas) / n_factors\n",
    "    demeaned_betas = portfolio_betas - mean_beta\n",
    "    return cp.norm(demeaned_betas, 2)\n",
    "\n",
    "def print_portfolio_stats(weights, mu, S, asset_betas, printing=True):\n",
    "    readable_weights = {}\n",
    "    df = weights[weights > ZERO].sort_values(ascending=False)\n",
    "    for k,v in df.items():\n",
    "        row = filtered_df[filtered_df['conId'] == k]\n",
    "        symbol = row['symbol'].iloc[0]\n",
    "        if printing: print(f'{symbol}: {round(v*100, 2)}%')\n",
    "        readable_weights[symbol] = k\n",
    "\n",
    "    rf_rate = risk_free_df['daily_nominal_rate'].iloc[-10:].mean()\n",
    "    er, volatility, sharpe_ratio = base_optimizer.portfolio_performance(weights, mu, S, risk_free_rate=rf_rate)\n",
    "    final_portfolio_betas = weights @ asset_betas\n",
    "\n",
    "    if printing:\n",
    "        print(f'beta_std: {final_portfolio_betas.std()}')\n",
    "        print(f'er: {er}')\n",
    "        print(f'volatility: {volatility}')\n",
    "        print(f'sharpe: {sharpe_ratio}\\n')\n",
    "    \n",
    "    return readable_weights\n",
    "\n",
    "def reduce_inputs(conids, mu_utility, mu_hist, S, asset_betas):\n",
    "    mu_post = mu_utility[conids]\n",
    "    mu_hist = mu_hist[conids]\n",
    "    S_post = S[conids].loc[conids]\n",
    "    betas_post = asset_betas.loc[conids]\n",
    "    return mu_post, mu_hist, S_post, betas_post\n",
    "\n",
    "\n",
    "def optimize_convex(mu, S, asset_betas, lambda_dispersion, lambda_risk, upper_bounds, factor_constraints=None, solver='CLARABEL'):\n",
    "    n_assets, n_factors = asset_betas.shape\n",
    "    w = cp.Variable(n_assets)\n",
    "    portfolio_betas = w @ asset_betas.values\n",
    "    expected_return = mu.values @ w\n",
    "    portfolio_risk = cp.quad_form(w, S)\n",
    "    factor_dispersion = portfolio_factor_dispersion(portfolio_betas, n_factors)\n",
    "    \n",
    "    objective = cp.Maximize(\n",
    "        expected_return\n",
    "        - factor_dispersion * lambda_dispersion\n",
    "        - portfolio_risk * lambda_risk\n",
    "    )\n",
    "    \n",
    "    constraints = [cp.sum(w) == 1, w >= 0, w <= upper_bounds]\n",
    "    if factor_constraints:\n",
    "        for factor, limits in factor_constraints.items():\n",
    "            try:\n",
    "                factor_idx = asset_betas.columns.get_loc(factor)\n",
    "                if 'min' in limits:\n",
    "                    constraints.append(portfolio_betas[factor_idx] >= limits['min'])\n",
    "                if 'max' in limits:\n",
    "                    constraints.append(portfolio_betas[factor_idx] <= limits['max'])\n",
    "            except KeyError:\n",
    "                print(f\"Warning: Factor '{factor}' not found in asset_betas.columns. Skipping constraint.\")\n",
    "\n",
    "    problem = cp.Problem(objective, constraints)\n",
    "    problem.solve(solver=solver)\n",
    "\n",
    "    if problem.status not in [\"optimal\", \"optimal_inaccurate\"]:\n",
    "        print(f\"Warning: Optimal solution not found. Status: {problem.status}\")\n",
    "        return None\n",
    "\n",
    "    weights = pd.Series(w.value, index=mu.index)\n",
    "    return weights\n",
    "\n",
    "\n",
    "def optimize_non_convex(mu, S, asset_betas, lambda_dispersion, lambda_risk, upper_bounds, max_assets, factor_constraints=None, solver='SCIP'):\n",
    "    n_assets, n_factors = asset_betas.shape\n",
    "    w = cp.Variable(n_assets)\n",
    "    z = cp.Variable(n_assets, boolean=True)\n",
    "    portfolio_betas = w @ asset_betas.values\n",
    "    expected_return = mu.values @ w\n",
    "    portfolio_risk = cp.quad_form(w, S)\n",
    "    factor_dispersion = portfolio_factor_dispersion(portfolio_betas, n_factors)\n",
    "    \n",
    "    objective = cp.Maximize(\n",
    "        expected_return\n",
    "        - factor_dispersion * lambda_dispersion\n",
    "        - portfolio_risk * lambda_risk\n",
    "    )\n",
    "    \n",
    "    constraints = [\n",
    "        cp.sum(w) == 1,\n",
    "        w >= 0,\n",
    "        cp.sum(z) <= max_assets,\n",
    "        w <= upper_bounds * z,\n",
    "    ]\n",
    "    if factor_constraints:\n",
    "        for factor, limits in factor_constraints.items():\n",
    "            try:\n",
    "                factor_idx = asset_betas.columns.get_loc(factor)\n",
    "                if 'min' in limits:\n",
    "                    constraints.append(portfolio_betas[factor_idx] >= limits['min'])\n",
    "                if 'max' in limits:\n",
    "                    constraints.append(portfolio_betas[factor_idx] <= limits['max'])\n",
    "            except KeyError:\n",
    "                print(f\"Warning: Factor '{factor}' not found in asset_betas.columns. Skipping constraint.\")\n",
    "\n",
    "    problem = cp.Problem(objective, constraints)\n",
    "    problem.solve(solver=solver)#, verbose=True)\n",
    "    if problem.status not in [\"optimal\", \"optimal_inaccurate\"]:\n",
    "        print(f\"Warning: Optimal solution not found. Status: {problem.status}\")\n",
    "\n",
    "    weights = pd.Series(w.value, index=mu.index)\n",
    "    return weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Filtering and discrete optimizations\n",
    "# lambda_risk = 1\n",
    "# lambda_dispersion = 1\n",
    "# upper_bounds = 1\n",
    "# my_constraints = {'factor_market_premium': {'min': asset_betas['factor_market_premium'].mean()}}\n",
    "\n",
    "# weights = optimize_convex(\n",
    "#     mu=mu_utility,\n",
    "#     S=S,\n",
    "#     asset_betas=asset_betas,\n",
    "#     lambda_risk=lambda_risk, \n",
    "#     lambda_dispersion=lambda_dispersion,\n",
    "#     upper_bounds=upper_bounds,\n",
    "#     factor_constraints=my_constraints\n",
    "# )\n",
    "\n",
    "# # Multiple optimizations\n",
    "# first_weights = weights.sort_values(ascending=False).head(200)\n",
    "# mu_post, mu_hist_post, S_post, betas_post = reduce_inputs(first_weights.index.tolist(), mu_utility, mu_historical, S, asset_betas)\n",
    "\n",
    "# # evaluated_results = []\n",
    "# test_range = range(1, 6)\n",
    "# for max_assets in tqdm(test_range, total = len(test_range)):\n",
    "#     post_weights = optimize_non_convex(\n",
    "#         mu=mu_post,\n",
    "#         S=S_post,\n",
    "#         asset_betas=betas_post,\n",
    "#         lambda_dispersion=lambda_dispersion,\n",
    "#         lambda_risk=lambda_risk,\n",
    "#         upper_bounds=upper_bounds,\n",
    "#         max_assets = max_assets,\n",
    "#         factor_constraints=my_constraints\n",
    "#     )\n",
    "\n",
    "#     non_zero_weights = post_weights[post_weights > ZERO]\n",
    "#     num_assets = len(non_zero_weights)\n",
    "#     if num_assets != max_assets:\n",
    "#         break\n",
    "\n",
    "#     er_model, std_model, sharpe_model = base_optimizer.portfolio_performance(post_weights, mu_post, S_post, risk_free_rate=rf_rate)\n",
    "#     er_hist, _, sharpe_hist = base_optimizer.portfolio_performance(post_weights, mu_hist_post, S_post, risk_free_rate=rf_rate)\n",
    "#     readable_weights = print_portfolio_stats(post_weights, mu_post, S_post, betas_post, printing=False)\n",
    "#     final_portfolio_betas = post_weights @ betas_post\n",
    "\n",
    "#     results_row = {\n",
    "#         'sharpe_model': sharpe_model,\n",
    "#         'sharpe_hist': sharpe_hist,\n",
    "#         'er_model': er_model,\n",
    "#         'er_hist': er_hist,\n",
    "#         'volatility': std_model,\n",
    "#         'factor_beta_std': final_portfolio_betas.std(),\n",
    "#         'num_assets': num_assets,\n",
    "#         'readable': readable_weights\n",
    "#     }\n",
    "\n",
    "#     results_row.update(non_zero_weights.to_dict())\n",
    "#     evaluated_results.append(results_row)\n",
    "\n",
    "# pd.DataFrame(evaluated_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluated_ids = fund_df[(fund_df['conId'].isin([col for col in pd.DataFrame(evaluated_results).columns if isinstance(col, int)])) & (fund_df['currency'] == 'EUR')].sort_values(by=['profile_cap_usd'], ascending=[False])['conId'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hand-picked conids\n",
    "china_ids = {\n",
    "    # '36BZ': 288308331,\n",
    "    # 'HSTE': 460091028,\n",
    "    # 'XCS6': 92975937,\n",
    "    # 'L4K3': 358794154,\n",
    "    'DBX9': 45160542,\n",
    "    'UIC2': 478600590,\n",
    "    # 'M9SV': 321415138,\n",
    "    # 'UIC1': 478600587,\n",
    "    # 'CASH1': 151625241,\n",
    "}\n",
    "us_ids = {\n",
    "    # 'IWDA': 100292038,\n",
    "    # 'XDWD': 163606916,\n",
    "    # 'SPPW': 355720826,\n",
    "    # 'RBOT': 254447324,\n",
    "    # 'SMH': 458525706,\n",
    "    'LGUS': 349753789,\n",
    "    # 'CHIP': 354802180,\n",
    "    'LGGL': 349753783,\n",
    "    # 'HSUD': 430252007,\n",
    "    # 'ROBO': 171977881,\n",
    "    # 'WCLD': 386371817,\n",
    "    # 'SKYE': 348172496,\n",
    "    # 'UNCA': 276188942,\n",
    "}\n",
    "eur_ids = {\n",
    "    'MEUD': 125902391,\n",
    "    'CSSX5E': 75961307,\n",
    "    # 'XESC': 59141442,\n",
    "    # 'MSE': 29612106,\n",
    "    # 'EXV1': 89005163,\n",
    "    # 'C6E': 314449634,\n",
    "    # 'EXIE': 617184924,\n",
    "    # 'STW': 173940943,\n",
    "    # 'STK': 173940919,\n",
    "    # 'PRAE': 401624582,\n",
    "    # 'STEC': 555440394,\n",
    "\n",
    "}\n",
    "gold_ids = {\n",
    "    'IS0E': 239379050,\n",
    "    'GDX1': 277212416,\n",
    "    'GDXJ': 277212426,\n",
    "}\n",
    "oil_ids = {\n",
    "    'IS0D': 239379033,\n",
    "    'EXH1': 89005199,\n",
    "    'V0IH': 624398537,\n",
    "}\n",
    "emerging_ids = {\n",
    "    # 'HMEM': 279158242,\n",
    "    'SPYM': 89384980,\n",
    "    'HMAF': 135536495,\n",
    "    'DEM': 181044389, # conflicts with DBX9\n",
    "    'EMQQ': 336523697, # conflicts with UIC2\n",
    "    # 'IFFI': 438873311,\n",
    "}\n",
    "japan_ids = {\n",
    "    'IJPA': 80268543,\n",
    "    'LCUJ': 311572503,\n",
    "    'DXJF': 211920310,\n",
    "}\n",
    "korea_ids = {\n",
    "    'IKRA': 37036642,\n",
    "    'HKOR': 349836421,\n",
    "    'CSKR': 79020239,\n",
    "    'DBX8': 46041699,\n",
    "}\n",
    "# optimal_ids = {}\n",
    "# for d in pd.DataFrame(evaluated_results).readable:\n",
    "#     optimal_ids.update(d)\n",
    "\n",
    "# fund_df[fund_df['conId'].isin(china_ids.values())].sort_values(by=['profile_cap_usd'], ascending=[False])[['conId', 'symbol', 'longName', 'profile_cap_usd', 'isin', 'search_exchange']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # OPTIMIZATION loop hand-picked ids\n",
    "# custom_ids = (\n",
    "#     list(china_ids.values())\n",
    "#     + list(us_ids.values())\n",
    "#     + list(eur_ids.values())\n",
    "#     # + list(gold_ids.values())\n",
    "#     # + list(oil_ids.values())\n",
    "#     + list(emerging_ids.values())\n",
    "#     # + list(japan_ids.values())\n",
    "#     # + list(korea_ids.values())\n",
    "#     # + list(optimal_ids.values())\n",
    "#     # + evaluated_ids\n",
    "#     )\n",
    "\n",
    "# custom_ids = list(set([id for id in custom_ids if id in mu_utility.index]))\n",
    "# mu_post, mu_hist_post, S_post, betas_post = reduce_inputs(custom_ids, mu_utility, mu_historical, S, asset_betas)\n",
    "\n",
    "# lambda_risk = 1\n",
    "# lambda_dispersion = 0\n",
    "# upper_bounds = 1\n",
    "# factor_constraints = {'factor_market_premium': {'min': asset_betas['factor_market_premium'].mean()}}\n",
    "\n",
    "# evaluated_results = []\n",
    "# test_range = range(1, 6)\n",
    "# for max_assets in tqdm(test_range, total = len(test_range)):\n",
    "#     post_weights = optimize_non_convex(\n",
    "#         mu=mu_post,\n",
    "#         S=S_post,\n",
    "#         asset_betas=betas_post,\n",
    "#         lambda_dispersion=lambda_dispersion,\n",
    "#         lambda_risk=lambda_risk,\n",
    "#         upper_bounds=upper_bounds,\n",
    "#         max_assets = max_assets,\n",
    "#         factor_constraints=factor_constraints\n",
    "#     )\n",
    "\n",
    "#     non_zero_weights = post_weights[post_weights > ZERO]\n",
    "#     num_assets = len(non_zero_weights)\n",
    "#     if num_assets != max_assets:\n",
    "#         break\n",
    "\n",
    "#     er_model, std_model, sharpe_model = base_optimizer.portfolio_performance(post_weights, mu_post, S_post, risk_free_rate=rf_rate)\n",
    "#     er_hist, _, sharpe_hist = base_optimizer.portfolio_performance(post_weights, mu_hist_post, S_post, risk_free_rate=rf_rate)\n",
    "#     readable_weights = print_portfolio_stats(post_weights, mu_post, S_post, betas_post, printing=False)\n",
    "#     final_portfolio_betas = post_weights @ betas_post\n",
    "\n",
    "#     # Create a dictionary with the scalar performance metrics\n",
    "#     results_row = {\n",
    "#         'sharpe_model': sharpe_model,\n",
    "#         'sharpe_hist': sharpe_hist,\n",
    "#         'er_model': er_model,\n",
    "#         'er_hist': er_hist,\n",
    "#         'volatility': std_model,\n",
    "#         'factor_beta_std': final_portfolio_betas.std(),\n",
    "#         'num_assets': num_assets,\n",
    "#         'readable': readable_weights\n",
    "#     }\n",
    "\n",
    "#     results_row.update(non_zero_weights.to_dict())\n",
    "#     evaluated_results.append(results_row)\n",
    "\n",
    "# # pd.DataFrame(evaluated_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # OPTIMIZATION loop through auto-selected ids\n",
    "# lambda_risk = 1\n",
    "# lambda_dispersion = 1\n",
    "# upper_bounds = 1\n",
    "# my_constraints = {\n",
    "#     'factor_market_premium': {'min': asset_betas['factor_market_premium'].mean()}\n",
    "# }\n",
    "\n",
    "# weights = optimize_convex(\n",
    "#     mu=mu_utility,\n",
    "#     S=S,\n",
    "#     asset_betas=asset_betas,\n",
    "#     lambda_risk=lambda_risk, \n",
    "#     lambda_dispersion=lambda_dispersion,\n",
    "#     upper_bounds=upper_bounds,\n",
    "#     factor_constraints=my_constraints\n",
    "# )\n",
    "\n",
    "# # Multiple optimizations\n",
    "# first_weights = weights.sort_values(ascending=False).head(500)\n",
    "# mu_post, mu_hist_post, S_post, betas_post = reduce_inputs(first_weights.index.tolist(), mu_utility, mu_historical, S, asset_betas)\n",
    "\n",
    "# evaluated_results = []\n",
    "# test_range = range(2, 6)\n",
    "# for max_assets in tqdm(test_range, total = len(test_range)):\n",
    "#     post_weights = optimize_non_convex(\n",
    "#         mu=mu_post,\n",
    "#         S=S_post,\n",
    "#         asset_betas=betas_post,\n",
    "#         lambda_dispersion=lambda_dispersion,\n",
    "#         lambda_risk=lambda_risk,\n",
    "#         upper_bounds=upper_bounds,\n",
    "#         max_assets = max_assets,\n",
    "#         factor_constraints=my_constraints\n",
    "#     )\n",
    "#     readable_weights = print_portfolio_stats(post_weights, mu_post, S_post, betas_post)\n",
    "\n",
    "#     num_assets = len(post_weights[post_weights > ZERO])\n",
    "#     assert num_assets <= max_assets\n",
    "\n",
    "#     er_model, std_model, sharpe_model = base_optimizer.portfolio_performance(post_weights, mu_post, S_post, risk_free_rate=rf_rate)\n",
    "#     er_hist, _, sharpe_hist = base_optimizer.portfolio_performance(post_weights, mu_hist_post, S_post, risk_free_rate=rf_rate)\n",
    "#     final_portfolio_betas = post_weights @ betas_post\n",
    "\n",
    "#     evaluated_results.append({\n",
    "#         'sharpe_model': sharpe_model,\n",
    "#         'sharpe_hist': sharpe_hist,\n",
    "#         'er_model': er_model,\n",
    "#         'er_hist': er_hist,\n",
    "#         'volatility': std_model,\n",
    "#         'factor_beta_std': final_portfolio_betas.std(),\n",
    "#         'num_assets': num_assets,\n",
    "#         'weights': post_weights[post_weights > ZERO],\n",
    "#         'readable': readable_weights\n",
    "#     })\n",
    "\n",
    "# pd.DataFrame(evaluated_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIMIZATION ONCE hand-picked ids\n",
    "custom_ids = (\n",
    "    list(china_ids.values())\n",
    "    # + list(us_ids.values())\n",
    "    + list(eur_ids.values())\n",
    "    # + list(gold_ids.values())\n",
    "    # + list(oil_ids.values())\n",
    "    + list(emerging_ids.values())\n",
    "    # + list(optimal_ids.values())\n",
    "    )\n",
    "\n",
    "\n",
    "# custom_ids = [125902391, 45160542, 478600590, 75961307]\n",
    "\n",
    "custom_ids = list(set([id for id in custom_ids if id in mu_utility.index]))\n",
    "custom_mu, custom_mu_hist, custom_S, custom_asset_betas = reduce_inputs(custom_ids, mu_utility, mu_historical, S, asset_betas)\n",
    "\n",
    "lambda_risk = 1\n",
    "lambda_dispersion = 1\n",
    "upper_bounds = 1\n",
    "\n",
    "custom_weights = optimize_convex(\n",
    "    mu=custom_mu,\n",
    "    S=custom_S,\n",
    "    asset_betas=custom_asset_betas,\n",
    "    lambda_risk=lambda_risk, \n",
    "    lambda_dispersion=lambda_dispersion,\n",
    "    upper_bounds=upper_bounds,\n",
    "    factor_constraints=my_constraints\n",
    ")\n",
    "custom_weights = optimize_non_convex(\n",
    "    mu=custom_mu,\n",
    "    S=custom_S,\n",
    "    asset_betas=custom_asset_betas,\n",
    "    lambda_dispersion=lambda_dispersion,\n",
    "    lambda_risk=lambda_risk,\n",
    "    upper_bounds=upper_bounds,\n",
    "    max_assets = 2,\n",
    "    factor_constraints=factor_constraints\n",
    ")\n",
    "_ = print_portfolio_stats(custom_weights, custom_mu, custom_S, custom_asset_betas)\n",
    "_ = print_portfolio_stats(custom_weights, custom_mu_hist, custom_S, custom_asset_betas)\n",
    "# pd.DataFrame(evaluated_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save rebalancing weights\n",
    "final_conids = custom_weights[custom_weights > ZERO]\n",
    "funds_dates = fund_df[fund_df['conId'].isin(final_conids.index)].set_index('conId')['funds_date']\n",
    "final_df = pd.DataFrame([{\n",
    "    'save_date': datetime.now().strftime('%Y-%m-%d'),\n",
    "    'weights': final_conids.to_dict(),\n",
    "    'training_oldest': training_oldest.strftime('%Y-%m-%d'),\n",
    "    'last_date': last_date.strftime('%Y-%m-%d'),\n",
    "    'mean_funds_date': funds_dates.mean().strftime('%Y-%m-%d'),\n",
    "    'fund_date': funds_dates.dt.strftime('%Y-%m-%d').to_dict(),\n",
    "    'factors': display_df.index.to_list(),\n",
    "    'lambda_risk': lambda_risk,\n",
    "    'lambda_dispersion': lambda_dispersion,\n",
    "    'factor_constraints': factor_constraints,\n",
    " }])\n",
    "try:\n",
    "    temp_df = pd.read_csv('data/final_weights.csv')\n",
    "    final_df = pd.concat([temp_df, final_df]).drop_duplicates(subset='save_date').dropna(axis=1, how='all')\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "print('saving...')\n",
    "final_df.to_csv('data/final_weights.csv', index=False)\n",
    "\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fund_df[(fund_df['longName'].str.contains('spdr europe heal', case=False, na=False) \n",
    "#          & fund_df['conId'].isin(tradable) \n",
    "#          & fund_df['currency'].isin(['EUR']))].sort_values(by=['profile_cap_usd'], ascending=[False])\n",
    "\n",
    "fund_df[fund_df['conId'].isin(custom_ids)].sort_values(by=['profile_cap_usd'], ascending=[False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "symbol = 'SPYM'\n",
    "\n",
    "x = filtered_df[filtered_df['symbol'] == symbol]['df'].iloc[-1].index\n",
    "y = filtered_df[filtered_df['symbol'] == symbol]['df'].iloc[-1].average\n",
    "\n",
    "plt.plot(x,y)\n",
    "plt.show()\n",
    "filtered_df[filtered_df['symbol'] == symbol]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
