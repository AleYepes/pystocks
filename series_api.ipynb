{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ib_async import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "import math\n",
    "import re\n",
    "\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kind = 'midpoint'\n",
    "# kind = 'trades'\n",
    "# kind = 'indices'\n",
    "\n",
    "if kind == 'midpoint':\n",
    "    root = 'data/daily-midpoint/'\n",
    "elif kind == 'trades':\n",
    "    root = 'data/daily-trades/'\n",
    "elif kind == 'indices':\n",
    "    root = 'data/indices/'\n",
    "\n",
    "data_path = root + 'series/'\n",
    "verified_path = root + 'verified_files.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to ibkr\n",
    "util.startLoop()\n",
    "\n",
    "ib = IB()\n",
    "ib.connect('127.0.0.1', 7497, clientId=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_historical(symbol, exchange, currency, duration='40 Y', kind=None):\n",
    "    contract = Stock(symbol, exchange, currency)\n",
    "    if kind == 'midpoint':\n",
    "        data = ib.reqHistoricalData(\n",
    "            contract, \n",
    "            endDateTime='',\n",
    "            durationStr=duration,\n",
    "            barSizeSetting='1 day', \n",
    "            whatToShow='MIDPOINT', \n",
    "            useRTH=True,\n",
    "        )\n",
    "    elif kind == 'trades' or kind == 'indices':\n",
    "        data = ib.reqHistoricalData(\n",
    "            contract, \n",
    "            endDateTime='',\n",
    "            durationStr=duration,\n",
    "            barSizeSetting='1 day', \n",
    "            whatToShow='TRADES', \n",
    "            useRTH=True,\n",
    "        )\n",
    "    length = len(data) - 1 if data and exchange == 'SMART' else len(data)\n",
    "    return data, length, exchange\n",
    "\n",
    "def save_data(data_path, data, symbol, exchange, currency):\n",
    "    if data:\n",
    "        data_df = util.df(data)\n",
    "        data_df['date'] = pd.to_datetime(data_df['date'])\n",
    "        data_df = data_df.sort_values(by='date').reset_index(drop=True)\n",
    "        data_df.to_csv(f'{data_path}{symbol}-{exchange}-{currency}.csv', index=False)\n",
    "        # print(f'{symbol} saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get missing historical series\n",
    "if kind == 'indices':\n",
    "    raise Exception('Incorrect kind for this. Needs to be trades or midpoint)')\n",
    "\n",
    "years = ['40 Y', '20 Y', '10 Y', '5 Y', '3 Y', '2 Y', '1 Y']\n",
    "# years = ['1 Y']\n",
    "for duration in years:\n",
    "    contracts_df = pd.read_csv('data/contract_elaborated.csv')\n",
    "    \n",
    "    contracts_df['search_exchange'] = contracts_df['search_exchange'].str.extract(r'\\((.*?)\\)').fillna('')\n",
    "    file_list = os.listdir(data_path)\n",
    "    file_list = [name.split('-')[0] for name in file_list]\n",
    "\n",
    "    missing_symbols = contracts_df[~contracts_df['symbol'].isin(file_list)].copy()\n",
    "    count = 0\n",
    "    for _, row in tqdm(missing_symbols.iterrows(), total=len(missing_symbols), desc=f\"Getting {duration} series\"):\n",
    "        symbol = row['symbol']\n",
    "        search_exchange = row['search_exchange']\n",
    "        suggested_exchange = row['exchange']\n",
    "        primary_exchange = row['primaryExchange']\n",
    "        currency = row['currency']\n",
    "        \n",
    "        results = []\n",
    "        if search_exchange:\n",
    "            results.append(get_historical(symbol, search_exchange, currency, duration=duration, kind=kind))\n",
    "            if suggested_exchange != search_exchange:\n",
    "                results.append(get_historical(symbol, suggested_exchange, currency, duration=duration, kind=kind))\n",
    "            if primary_exchange != suggested_exchange and primary_exchange != search_exchange:\n",
    "                results.append(get_historical(symbol, primary_exchange, currency, duration=duration, kind=kind))\n",
    "        else:\n",
    "            results.append(get_historical(symbol, suggested_exchange, currency, duration=duration, kind=kind))\n",
    "            if primary_exchange != suggested_exchange:\n",
    "                results.append(get_historical(symbol, primary_exchange, currency, duration=duration, kind=kind))\n",
    "        results.append(get_historical(symbol, 'SMART', currency, duration=duration, kind=kind))\n",
    "        results = sorted(results, key=lambda x: x[1], reverse=True)\n",
    "        if results[0][1]:\n",
    "            save_data(data_path, results[0][0], symbol, results[0][2], currency)\n",
    "            count +=1\n",
    "\n",
    "    print(f'{duration}: {count} scraped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update historical series\n",
    "file_list = os.listdir(data_path)\n",
    "\n",
    "for file_name in tqdm(file_list, total=len(file_list), desc=f\"Updating {data_path}\"):\n",
    "    symbol, exchange, currency = file_name.replace('.csv', '').split('-')\n",
    "    \n",
    "    file_path = os.path.join(data_path, file_name)\n",
    "    data_df = pd.read_csv(file_path)\n",
    "    data_df['date'] = pd.to_datetime(data_df['date'])\n",
    "    last_date = data_df['date'].max()\n",
    "    time_missing = (datetime.now() - last_date).days\n",
    "    if time_missing > 364:\n",
    "        time_missing = math.ceil(time_missing / 364)\n",
    "        duration = f'{time_missing} Y'\n",
    "    else:\n",
    "        duration = f'{time_missing} D'\n",
    "    \n",
    "    if time_missing:\n",
    "        new_data,_,_ = get_historical(symbol, exchange, currency, duration=duration, kind=kind)\n",
    "        if new_data:\n",
    "            new_data_df = util.df(new_data)\n",
    "            new_data_df['date'] = pd.to_datetime(new_data_df['date'])\n",
    "            updated_data_df = pd.concat([new_data_df, data_df]).drop_duplicates(subset='date').sort_values(by='date').reset_index(drop=True)\n",
    "            updated_data_df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Prep indices\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare indices\n",
    "def melt(data_df):\n",
    "    value_columns = ['open', 'close']\n",
    "    id_columns = [col for col in data_df.columns.to_list() if col not in value_columns]\n",
    "    melted_df = data_df.melt(id_vars=id_columns, value_vars=value_columns, var_name='kind', value_name='value')\n",
    "    return melted_df.sort_values(by=['date', 'kind'], ascending=[True, False]).reset_index(drop=True)\n",
    "\n",
    "# Load indices and merge them all into one df\n",
    "indices = {}\n",
    "file_list = os.listdir('data/indices/series/')\n",
    "for file in file_list:\n",
    "    symbol = os.path.splitext(file)[0].split('-')[0]\n",
    "    indices[symbol] = pd.read_csv('data/indices/series/' + file)\n",
    "\n",
    "# Melt indices, filters, and calc pct_change. ASSUMES that indices are sorted chronologically\n",
    "training_start_date = pd.to_datetime('2020-02-01')\n",
    "month_ago = datetime.today() - timedelta(days=31)\n",
    "\n",
    "day_gap = 6 # SET ACCEPTABLE DAY GAP\n",
    "\n",
    "melted_indices, index_returns = [], {}\n",
    "for symbol, df in tqdm(indices.items(), total=len(indices), desc=f'Melting and filtering {kind} indices'):\n",
    "    df = melt(df)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "    latest_date = df['date'].iloc[-1]\n",
    "    earliest_date = df['date'].iloc[0]\n",
    "    dates = df['date'].unique()\n",
    "    date_gaps = dates[1:] - dates[:-1]\n",
    "    df['symbol'] = symbol\n",
    "    df['pct_change'] = df['value'].pct_change()\n",
    "    index_returns[symbol] = df['pct_change'].mean()\n",
    "    melted_indices.append(df)\n",
    "print(f'Loaded {len(melted_indices)} out of {len(file_list)} series ({round(len(melted_indices)/len(file_list)*100, 4)}%)')\n",
    "\n",
    "# Concatenate and pivot data\n",
    "index_df = pd.concat(melted_indices, ignore_index=True)\n",
    "index_df = index_df.pivot(index=['date', 'kind'], columns='symbol', values='pct_change')\n",
    "index_df = index_df.sort_values(by=['date', 'kind'], ascending=[True, False]).reset_index()#.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define verified files\n",
    "try:\n",
    "    with open(verified_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        verified_files = [line.strip() for line in lines]\n",
    "    file_list = os.listdir(data_path)\n",
    "except FileNotFoundError:\n",
    "    file_list = os.listdir(data_path)\n",
    "    contracts_df = pd.read_csv('data/contract_elaborated.csv')\n",
    "\n",
    "    verified_files = []\n",
    "    for file_name in tqdm(file_list, total=len(file_list), desc=f\"Verifying {kind} series with elaborated data\"):\n",
    "        symbol, exchange, currency = file_name.replace('.csv', '').split('-')\n",
    "        try:\n",
    "            contract_details = ib.reqContractDetails(Stock(symbol, exchange, currency))\n",
    "            id = contract_details[0].secIdList[0].value\n",
    "\n",
    "            if contracts_df[contracts_df['symbol'] == symbol]['isin'].iloc[0] == id:\n",
    "                \n",
    "                instrument_name = contracts_df[contracts_df['symbol'] == symbol]['longName'].iloc[0]\n",
    "                instrument_name = instrument_name.replace('-', '').replace('+', '')\n",
    "                for word in instrument_name.split():\n",
    "                    if re.fullmatch(r'\\d+X', word):\n",
    "                        if int(word[:-1]) > 1:\n",
    "                            continue\n",
    "                        if word.startswith(('LV', 'LEV')):\n",
    "                            print(f'    {instrument_name}')\n",
    "                            \n",
    "                verified_files.append(file_name.split('-')[0])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    with open(verified_path, 'w') as f:\n",
    "        for item in verified_files:\n",
    "            f.write(str(item) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
